{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"GPT-3\"\n",
        "format: \n",
        "    revealjs:\n",
        "        toc: true\n",
        "        toc-depth: 1\n",
        "        toc-title: Plan\n",
        "# format:\n",
        "#     revealjs:\n",
        "#         width: 1920\n",
        "#         height: 1080\n",
        "---"
      ],
      "id": "c2f57361"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# What does GPT-3 do?\n",
        "\n",
        "##\n",
        "\n",
        "Do you like poetry?\n",
        "\n",
        ". . .\n",
        "\n",
        "A rose is a rose is [a rose]{.fragment}\n",
        "\n",
        ". . .\n",
        "\n",
        "Gertrude Stein\n",
        "\n",
        ". . .\n",
        "\n",
        "Brexit means brexit means [Brexit]{.fragment}\n",
        "\n",
        ". . .\n",
        "\n",
        "John Crace\n",
        "\n",
        ". . .\n",
        "\n",
        "Elementary my dear [Watson]{.fragment}\n",
        "\n",
        ". . .\n",
        "\n",
        "P.G. Woodehouse\n",
        "\n",
        ". . .\n",
        "\n",
        "There is an easy way for the government to end the strike without withdrawing the pension reform, \n",
        "\n",
        ". . .\n",
        "\n",
        "\n",
        "## Complete Text\n",
        "\n",
        "Generative language models perform *text completion*\n",
        "\n",
        "They generate plausible[^plausible] text following a prompt.\n",
        "\n",
        "The type of answer, will depend on the kind of prompt.\n",
        "\n",
        "[^plausible]: here, plausible, means that it is more likely to be a correct text written by a human, rather than otherwise\n",
        "\n",
        "## Some Examples\n",
        "\n",
        "\n",
        "##\n",
        "\n",
        "To use GPT-3 profficiently, you have to experiment with the prompt.\n",
        "\n",
        "- try the [Playground mode](https://platform.openai.com/playground)\n",
        "\n",
        "It is the same as learning how to do google queries\n",
        "\n",
        "- altavista: `+noir +film -\"pinot noir\"`\n",
        "- nowadays: ???\n",
        "\n",
        "\n",
        "# How does it do it?\n",
        "\n",
        "## Language Models and Cryptography\n",
        "\n",
        "![](assets/code-secret.webp)\n",
        "\n",
        "The Caesar code\n",
        "\n",
        "## {auto-animate=\"true\"}\n",
        "\n",
        "![Zodiac 408 Cipher](assets/zodiac_408.png)\n",
        "\n",
        "\n",
        "## {auto-animate=\"true\"}\n",
        "\n",
        "\n",
        "::: {#fig-elephants layout-ncol=2}\n",
        "\n",
        "![Zodiac 408 Cipher](assets/zodiac_408.png){width=20%}\n",
        "\n",
        "![Key for Zodiac 408](assets/408_solution.jpg){width=40%}\n",
        "\n",
        "Solved in a week by Bettye and Donald Harden.\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "##\n",
        "\n",
        "Later in 2001, in a prison, somewhere in California \n",
        "\n",
        "![](assets/prison.png)\n",
        "\n",
        ". . .\n",
        "\n",
        "Solved by Stanford's [Persi Diaconis](https://math.uchicago.edu/~shmuel/Network-course-readings/MCMCRev.pdf\n",
        ") and his students using Monte Carlo Markov Chains\n",
        "\n",
        "## Monte Carlo Markov Chains\n",
        "\n",
        "\n",
        "Take a letter $x_n$, what is the probability of the next letter being $x_{n+1}$?\n",
        "\n",
        "$$\\pi_{X,Y} = P(x_{n+1}=Y, x_{n}=X)$$\n",
        "\n",
        "for $X=\\{a, b, .... , z\\} , Y=\\{a,b,c, ... z\\}$\n",
        "\n",
        "TODO: new slide\n",
        "\n",
        "The language model can be trained using english language.\n",
        "\n",
        "And used to determine whether a given cipher-key is consistent with english language.\n",
        "\n",
        "It yields a very efficient algorithm to decode any caesar code (with very small sample)\n",
        "\n",
        "## MCMC to generate text\n",
        "\n",
        "MCMCs can also be used to generate text:\n",
        "\n",
        "- take initial prompt: `I think therefore I`\n",
        "    - last letter is I\n",
        "    - most plausible character afterwards is ` `\n",
        "    - most plausible character afterwards is `I`\n",
        "- Result: `I think therefore I I I I I I`\n",
        "\n",
        "Not good but promising (ðŸ¤·)\n",
        "\n",
        "Remedies:\n",
        "- augment memory\n",
        "- change basic unit (use phonems or words)\n",
        "\n",
        "## \n",
        "\n",
        "Some example using MCMC\n",
        "\n",
        "\n",
        "He ha â€˜s killâ€™d me Mother , Run away I pray you Oh this is Counter you false Danish Dogges .\n",
        "\n",
        "##\n",
        "\n",
        "Augmenting memory:\n",
        "\n",
        "- if you want to compute the most frequent letter (among `26`) after `50` letters, you need to take into account `5.6061847e+70` combinations !\n",
        "    - impossible to store, let alone do the training\n",
        "\n",
        "- but some combinations are useless:\n",
        "    - `wjai dfni`\n",
        "    - `Despite the constant negative press covfefe` [ðŸ¤”]{.fragment}\n",
        "\n",
        "##\n",
        "\n",
        "Neural network can reduce dimensionality\n",
        "\n",
        "\n",
        "\n",
        "## Reccurrent Neural Networks\n",
        "\n",
        "\n",
        "::: {#fignn layout-ncol=2}\n",
        "\n",
        "![A glowing artificial neural network floating midair, with a white background.](assets/glowing_nn.png){width=20%}\n",
        "\n",
        "![Key for Zodiac 408](assets/Recurrent_neural_network_unfold.svg.png)\n",
        "\n",
        "Solved in a week by Bettye and Donald Harden.\n",
        "\n",
        ":::\n",
        "\n",
        "Neural networks make it possible to increase the state-space to represent\n",
        "\n",
        "$$\\forall X, P(x_n=X| x_{n-1}, ..., x_{n-k})$$\n",
        "\n",
        "they reduce endogenously the dimensionality.\n",
        "\n",
        "\n",
        "## Recurrent Neural Networks\n",
        "\n",
        "In 2015\n",
        "\n",
        "\n",
        "<iframe width=\"780\" height=\"500\" src=\"http://karpathy.github.io/2015/05/21/rnn-effectiveness/\" title=\"The unreasonable Effectiveness of Recurrent Neural Networks\"></iframe>\n",
        "\n",
        "- __Neural Network__ reduce dimensionality of data discovering structure\n",
        "\n",
        "\n",
        "## Long Short Term Memory\n",
        "\n",
        "![](assets/Long_Short-Term_Memory.svg)\n",
        "\n",
        "- 2000->2019 : Emergence of Long Short Term Memory models\n",
        "    - speech recognition\n",
        "    \n",
        "    - LSTM behind \"Google Translate\", \"Alexa\"\n",
        "\n",
        "## Transformer\n",
        "\n",
        "::: columns\n",
        "\n",
        ":::: column\n",
        "\n",
        "1. Position Encodings\n",
        "    - model is not sequential anymore\n",
        "    - tries to learn sequence\n",
        "2. Attention\n",
        "3. Self-Attention\n",
        "\n",
        "::::\n",
        "\n",
        ":::: column\n",
        "\n",
        "![](assets/transformers.png)\n",
        "\n",
        "::::\n",
        "\n",
        ":::\n",
        "\n",
        "## Quick summary\n",
        "\n",
        "- Language models\n",
        "    - frequency tables\n",
        "    - monte carlo markov chains\n",
        "    - long-short-term memory (>2000)\n",
        "    - transformers (>2018)\n",
        "\n",
        "- Since 2010 main breakthrough came through the development of deep-learning techniques (software/hardware)\n",
        "- Recently, models have improving tremndously\n",
        "\n",
        "\n",
        "## GPT3\n",
        "\n",
        "- 2018: GPT1\n",
        "    - had to be fine-tuned to a particular problem\n",
        "    - transfer learning (few shots learning)\n",
        "- GPT2:\n",
        "    - multitask\n",
        "    - no mandatory fine tuning\n",
        "- GPT3: \n",
        "    - bigger: 175 billions parameters\n",
        "- GPT4: \n",
        "    - even bigger: 1000 billions parameters\n",
        "    - on your harddrive: 1Tb\n",
        "It is a generative model\n",
        "\n",
        "## Corpus\n",
        "\n",
        "GPT 3 wat trained on\n",
        "\n",
        "- CommonCrawl\n",
        "- WebText\n",
        "- Wikipedia\n",
        "- many books\n",
        "\n",
        "\n",
        "45 TB of data\n",
        "- cured into TODO\n",
        "\n",
        "Dataset (mostly) ends in 2021.\n",
        "\n",
        "## The Relation between GPT-3 and ChatGPT\n",
        "\n",
        "It is trivial to make a chatbot using GPT3.\n",
        "\n",
        "ChatGPT is an interface on top of GPT3.5 plus some fine-tuning.\n",
        "\n",
        "Technical paper is not out, but we know the following:\n",
        "\n",
        "- it is *fine-tuned* by interacting with humans\n",
        "- it uses *reinforcement learning* to optimize learning\n",
        "\n",
        "\n",
        "# How can we use GPT3\n",
        "\n",
        "## Generate Text\n",
        "\n",
        "Would you use GPT3 to:\n",
        "\n",
        "- write an paper?\n",
        "- quickly respond to an email\n",
        "- write the boring details of a paper\n",
        "- get some ideas?\n",
        "- help you structure a talk?\n",
        "\n",
        "There are several concerns...\n",
        "\n",
        "## The quality of the generated text\n",
        "\n",
        "- GPT-3 has the tendancy to *hallucinate* facts.\n",
        "\n",
        "    - like kids who don't distinguish facts and play\n",
        "\n",
        "- This is being worked on:\n",
        "\n",
        "    - GPT4: *Can you teach old dogs new tricks?* [yes]{.fragment}\n",
        "\n",
        "- Still a problem for research\n",
        "\n",
        "- Possibly solved by mixing AI and traditional computing\n",
        "\n",
        "## Ethical Concerns\n",
        "\n",
        "We must avoid at all cost *plagiarism* and credit our sources.\n",
        "\n",
        "We need personal ethics:\n",
        "\n",
        "- give sources\n",
        "- mention use of GPT3 we we use it\n",
        "\n",
        "## Replace Many NLP Algorithms\n",
        "\n",
        "Beyond generating text, most Natural Language Processing tasks can be now done with GPT-3:\n",
        "\n",
        "- Named entity recognition\n",
        "- Classification\n",
        "    - sentiment analysis\n",
        "    - multimodel sentiment analysis\n",
        "- Entity linking\n",
        "- Summarization\n",
        "- ...\n",
        "\n",
        "## Named Entity recognition\n",
        "\n",
        "Prompt:[^cf]\n",
        "\n",
        "```\n",
        "[Text]: Helena Smith founded Core.ai 2 years ago. She is now the CEO and CTO of the company and is building a team of highly skilled developers in machine learning and natural language processing.\n",
        "[Position]: CEO and CTO\n",
        "###\n",
        "[Text]: Tech Robotics is a robot automation company specialized in AI driven robotization. Its Chief Technology Officer, Max Smith, says a new wave of improvements should be expected for next year.\n",
        "[Position]: Chief Technology Officer\n",
        "###\n",
        "[Text]: FranÃ§ois is a Go developer. He mostly works as a freelancer but is open to any kind of job offering!\n",
        "[Position]: Go developer\n",
        "###\n",
        "[Text]: Maxime is a data scientist at Auto Dataset, and he's been working there for 1 year.\n",
        "[Position]:\n",
        "```\n",
        ". . .\n",
        "\n",
        "```Response: Data Scientist```\n",
        "\n",
        "[^cf]: https://towardsdatascience.com/advanced-ner-with-gpt-3-and-gpt-j-ce43dc6cdb9c\n",
        "\n",
        "\n",
        "## Same approach with fine tuning"
      ],
      "id": "050a30f2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model.train(\"\"\"\n",
        "\"[Text]: Helena Smith founded Core.ai 2 years ago. She is now the CEO and CTO of the company and is building a team of highly skilled developers in machine learning and natural language processing.\n",
        "[Position]: CEO and CTO\n",
        "\"\"\")"
      ],
      "id": "84efec29",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model.train(\"\"\"\n",
        "[Text]: Tech Robotics is a robot automation company specialized in AI driven robotization. Its Chief Technology Officer, Max Smith, says a new wave of improvements should be expected for next year.\n",
        "[Position]: Chief Technology Officer\n",
        "\"\"\")"
      ],
      "id": "18cacc16",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Are we on the way to General Artificial Intelligence?\n",
        "\n",
        "## Artificial Intelligence vs Artificial Humanity"
      ],
      "id": "c9f981aa"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
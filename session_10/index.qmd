---
title: Some Important Points
subtitle: Data-Based Economics
author: Year 2022-2023
format:
  # html: default
  revealjs:
    toc: true
    toc-depth: 1
    toc-title: Plan
    navigation: grid
    width: 1920
    height: 1080
    output-file: slides.html

---


## Bias vs Variance

::: incremental

- A model is fitted (trained / regressed) on a given amount of data
- A model can be more or less flexible
  - have more or less independent parameters (aka degrees of freedom)
  - ex: $y = a + b x$ (2) vs $y = a + b x_1 + c x_1^2 + e x_2 + f x_3$ (5)
- More flexible models fit the training data better...
- ...but tend to perform worse for predictions
- This is known as:
  - The Bias (bad fit) vs Variance (bad prediction) tradeoff
  - The no free lunch theorem

:::

---

## Overfitting

![](graphs/overfitting.png)

---

## Explanation vs Prediction

::: incremental

- The goal of __machine learning__ consists in making the best __predictions__:
  - use enough data to maximize the fit...
  - ... but control the number of independent parameters to prevent overfitting
    - ex: LASSO regression has lots of parameters, but tries to keep most of them zero
  - ultimately quality of prediction is evaluated on a test set, independent from the training set
- In __econometrics__ we can perform
  - predictions: sames issues as ML
  - explanatory analysis: focus on the effect of one (or a few) explanatory variables
    - this does not necessary require strong predictive power

:::

---

## Read Regression Results

```text
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.252
Model:                            OLS   Adj. R-squared:                  0.245
Method:                 Least Squares   F-statistic:                     33.08
Date:                Tue, 30 Mar 2021   Prob (F-statistic):           1.01e-07
Time:                        02:34:12   Log-Likelihood:                -111.39
No. Observations:                 100   AIC:                             226.8
Df Residuals:                      98   BIC:                             232.0
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
==============================================================================
Intercept     -0.1750      0.162     -1.082      0.282      -0.496       0.146
x              0.1377      0.024      5.751      0.000       0.090       0.185
==============================================================================
Omnibus:                        2.673   Durbin-Watson:                   1.118
Prob(Omnibus):                  0.263   Jarque-Bera (JB):                2.654
Skew:                           0.352   Prob(JB):                        0.265
Kurtosis:                       2.626   Cond. No.                         14.9
==============================================================================
```

- Understand p-value: chances that a given statistics might have been obtained, under the H0 hypothesis

::: r-stack

:::: {.fragment .current-visible}

- Check:
  - `R2`:  provides an indication of predictive power. Does not prevent overfitting.
  - `adj. R2`: predictive power corrected for excessive degrees of freedom
  - global significance (p-value of Fisher test): chances we would have obtained this R2 if all real coefficients were actually 0 (H0 hypothesis)

::::
:::: fragment

- Coefficient:
  - `p-value` probability that coefficient might have been greater than observed, if it was actually 0.
  - if p-value is smaller than 5%: the coefficient is significant at a 5% level
  - confidence intervals (5%): if the true coefficient was out of this interval, observed value would be very implausible
     -  higher confidence levels -> bigger intervals 

::::

:::

---

## Read A Regression Table

<img src=graphs/regression_table.png width=80%>



## Can there be too many variables?


1. Overfitting

  - bad predictions

2. Colinearity
  - can bias a coefficient of interest
  - not a problem for prediction
  - exact colinearity makes traditional OLS fail

To choose the right amount of variables find a combination which maximizes adjusted R2 or an information criterium


## Colinearity

-  $x$ is colinear with $y$ if $cor(x,y)$ very close to 1
-  more generally $x$ is colinear with $y_1, ... y_n$ if $x$ can be deduced linearly from $y_1...y_n$

  - there exists $\lambda_1, ... \lambda_n$ such that $x = \lambda_1 x_1 + ... + \lambda_n x_n$
  - example: hours of sleep / hours awake (sleep=24-awake)
-  perfect colinearity is a problem: coefficients are not well defined
  
  - $\text{productivity} = 0.1 + 0.5 \text{sleep} + 0.5 \text{awake}$ or $\text{productivity} = -11.9 + 1 \text{sleep} + 1 \text{awake}$  ?
-  best regressions have regressors that:

  - explain independent variable
  - are independent from each other (as much as possible)

---

## Ommitted Variable

What if you don't have enough variables?

$y = a + bx$

- R2 can be low. It's ok for explanatory analysis.
- as long as residuals are normally distributed
  - check graphically to be sure
  - (more advanced): there are statistical tests

---

## omitted variable

::: columns

:::: column

<div class="r-stack">

<img class="fragment current-visible" src=graphs/reg_1_full.svg  width=100% data-fragment-index=1>
<img class="fragment current-visible" src=graphs/reg_2_full.svg  width=100% data-fragment-index=2>
<img class="fragment current-visible" src=graphs/reg_3_mixed.svg width=100% data-fragment-index=3>
<img class="fragment current-visible" src=graphs/reg_4_mixed.svg width=100% data-fragment-index=4>
<img class="fragment" src=graphs/reg_5_mixed.svg width=100% data-fragment-index=5>

</div>

::::

:::: column

::::: incremental

- Suppose we want to know the effect of $x$ on $y$.
- We run the regression $y = a + b x$
  - we find $y = 0.21 + \color{red}{0.15} x$
- We then realize we have access to a categorical variable $gender \in {male, female}$
- We then add the $\delta$ dummy variable to the regression: $y = a + bx + c \delta$
  - we find $ y = -0.04 + \color{red}{0.2} x - 0.98 \delta$
- Note that adding the indicator
  - improved the fit  ($R^2$ is 0.623 instead of 0.306)
  - corrected for the <strong>omitted variable bias</strong> (true value of b is actually 0.2)
  - provided an estimate for the effect of variable gender

:::::

::::

:::

---

## Unobserved Heterogeneity

<div class="container">
<div class="col">

<div class="r-stack">

<img class="fragment current-visible" src=graphs/panel_1.svg  width=100% data-fragment-index=1>
<img class="fragment current-visible" src=graphs/panel_2.svg  width=100% data-fragment-index=2>
<img class="fragment current-visible" src=graphs/panel_3.svg  width=100% data-fragment-index=4>
<img class="fragment current-visible" src=graphs/panel_3.svg  width=100% data-fragment-index=3>
<img class="fragment" src=graphs/panel_4.svg  width=100% data-fragment-index=5>

</div>

| Individual | Gender | Shirt color | Education | ... |
| ---------- | ------ | ----------- | --------- | --- |
| 1          | M      | ?           | ?         | ?   |
| 2          | F      | ?           | ?         | ?   |
| 3          | M      | ?           | ?         | ?   |
| ...        | ...    | ...         | ...       | ... |


</div>

<div class="col">

- <!-- .element: class="fragment" data-fragment-index="1" -->Suppose we want to know the effect of $x$ on $y$.
- <!-- .element: class="fragment" data-fragment-index="2" -->We run the regression $y = a + b x$
  - we find $y = 1.09 + \color{red}{0.24} x$
- <!-- .element: class="fragment" data-fragment-index="3" -->But now, the data has a special panel structure.
  - we have a categorical variable corresponding to each individual
  - it is linked to several omitted variables, most of which we don't observe
-  <!-- .element: class="fragment"  data-fragment-index="4" --> <b>Fixed Effects</b>: use an indicator for each individual
  - we need to drop the individual specific dummies that we observe (i.e. gender)
- <!-- .element: class="fragment" data-fragment-index="5" -->Run the regression $y_{i,n} = a_i + b x_{i,n} $
  - we find $ y_{i,n} = a_i + \color{red}{0.18} x_{i,n} $
- <!-- .element: class="fragment" -->Note that adding the indicator
  - <!-- .element: class="fragment" data-fragment-index="6" -->improved the fit  ($R^2$ is 0.631 instead of 0.278)
  - <!-- .element: class="fragment" data-fragment-index="7" -->corrected for the <strong>unobserved heterogeneity bias</strong> (true value of b is actually 0.2)

</div>

</div>

---

## Fixed Effects

- Essentially: the intuition behind correcting for unobserved heterogeneity is the same as the one behind ommitted variable bias.
- Fixed Effects are essentially dummies specific to some categories of observations
- In panel data, there are two categories: individual index and time
  - individual (<span style="color: red"> Entity Effects</span>)
  - time (<span style="color: blue">Time Effects</span>)
- We index regressions by them:
  $$y_{it} = \color{red}{a_i} + \color{blue}{a_t} + b x_{i t} + ... $$
- Fixed effects are not very useful for prediction:
  - one can predict $y_{it}$ only for $i,t$ already in the database
  - impossible to predict new individuals/new dates
- Remedy: random fixed effects

---

## Endogeneity

- Consider the regression model $y = a + b x + \epsilon$
- When $\epsilon$ is correlated with $x$ we have an __endogeneity__ problem.
  - we can check in the regression results whether the residuals ares correlated with $y$ or $x$
- Endogeneity can have several sources: omitted variable, measurement error, simultaneity
  - it creates a bias in the estimate of $a$ and $b$
- We say we *control* for endogeneity by adding some variables
- A special case of endogeneity is a __confounding factor__ a variable $z$ which causes at the same time $x$ and $y$

---

## Instrument

$$y = a + b x + \epsilon$$

- Recall: endogeneity issue when $\epsilon$ is correlated with $x$
- Instrument: a way to keep only the variability of $x$ that is independent from $\epsilon$ 
  - it needs to be correlated with $x$
  - not with all components of $\epsilon$
- An instrument can be used to solve endogeneity issues
- It can also establish the causality from $x$ to $y$:
  - since it is independent from $\epsilon$, all its effect on $y$ goes through $x$

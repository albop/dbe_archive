[
  {
    "objectID": "10_recap.html",
    "href": "10_recap.html",
    "title": "Data-Based Economics",
    "section": "",
    "text": "A model is fitted (trained / regressed) on a given amount of data\nA model can be more or less flexible\n\nhave more or less independent parameters (aka degrees of freedom)\nex: \\(y = a + b x\\) (2) vs \\(y = a + b x_1 + c x_1^2 + e x_2 + f x_3\\) (5)\n\nMore flexible models fit the training data better…\n…but tend to perform worse for predictions\nThis is known as:\n\nThe Bias (fit) vs Variance (prediction) tradeoff\nThe no free lunch theorem\n\n\n\n\n\n\n\n\n\n\n\n\nThe goal of machine learning consists in making the best predictions:\n\nuse enough data to maximize the fit…\n… but control the number of independent parameters to prevent overfitting\n\nex: LASSO regression has lots of parameters, but tries to keep most of them zero\n\nultimately quality of prediction is evaluated on a test set, independent from the training set\n\nIn econometrics we can perform\n\npredictions: sames issues as ML\nexplanatory analysis: focus on the effect of one (or a few) explanatory variables\n\nthis does not necessary require strong predictive power\n\n\n\n\n\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.252\nModel:                            OLS   Adj. R-squared:                  0.245\nMethod:                 Least Squares   F-statistic:                     33.08\nDate:                Tue, 30 Mar 2021   Prob (F-statistic):           1.01e-07\nTime:                        02:34:12   Log-Likelihood:                -111.39\nNo. Observations:                 100   AIC:                             226.8\nDf Residuals:                      98   BIC:                             232.0\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n==============================================================================\nIntercept     -0.1750      0.162     -1.082      0.282      -0.496       0.146\nx              0.1377      0.024      5.751      0.000       0.090       0.185\n==============================================================================\nOmnibus:                        2.673   Durbin-Watson:                   1.118\nProb(Omnibus):                  0.263   Jarque-Bera (JB):                2.654\nSkew:                           0.352   Prob(JB):                        0.265\nKurtosis:                       2.626   Cond. No.                         14.9\n==============================================================================\n\n\nUnderstand p-value: chances that a given statistics might have been obtained, under the H0 hypothesis\n\nCheck:\n\n\nglobal significance (Fisher test): chances would have obtained this R2 if all real coefficients were actually 0 (H0 hypothesis)\n\nR2: provides an indication of predictive power. Does not prevent overfitting.\n\nadj. R2: predictive power corrected for excessive degrees of freedom\n\ncoefficient:\n\np-value probability that coefficient might have been greater than observed, if it was actually 0.\nif p-value is smaller than 5%: the coefficient is significant at a 5% level\nconfidence intervals (5%): if the true coefficient was out of this interval, observed value would be very implausible\n\nhigher confidence levels -&gt; bigger intervals\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverfitting\n\n\nbad predictions\n\n\nColinearity\n\n\ncan bias a coefficient of interest\nnot a problem for prediction\nexact colinearity makes traditional OLS fail\n\nTo choose the right amount of variables find a combination which maximizes adjusted R2 or an information criterium\n\n\n\n\n\n\n\\(x\\) is colinear with \\(y\\) if \\(cor(x,y)\\) very close to 1\n\nmore generally \\(x\\) is colinear with \\(y_1, ... y_n\\) if \\(x\\) can be deduced linearly from \\(y_1...y_n\\)\nthere exists \\(\\lambda_1, ... \\lambda_n\\) such that \\(x = \\lambda_1 x_1 + ... + \\lambda_n x_n\\)\nexample: hours of sleep / hours awake (sleep=24-awake)\n\nperfect colinearity is a problem: coefficients are not well defined\n\\(\\text{productivity} = 0.1 + 0.5 \\text{sleep} + 0.5 \\text{awake}\\) or \\(\\text{productivity} = -11.9 + 1 \\text{sleep} + 1 \\text{awake}\\) ?\n\nbest regressions have regressors that:\nexplain independent variable\nare independent from each other (as much as possible)\n\n\n\n\n\nWhat if you don’t have enough variables?\n\\(y = a + bx\\)\n\nR2 can be low. It’s ok for explanatory analysis.\nas long as residuals are normally distributed\n\ncheck graphically to be sure\n(more advanced): there are statistical tests\n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\nSuppose we want to know the effect of \\(x\\) on \\(y\\).\n\nWe run the regression \\(y = a + b x\\)\n\nwe find \\(y = 0.21 + \\color{red}{0.15} x\\)\n\n\nWe then realize we have access to a categorical variable \\(gender \\in {male, female}\\)\n\nWe then add the \\(\\delta\\) dummy variable to the regression: \\(y = a + bx + c \\delta\\)\n\nwe find $ y = -0.04 + x - 0.98 $\n\n\nNote that adding the indicator\n\n\nimproved the fit (\\(R^2\\) is 0.623 instead of 0.306)\n\ncorrected for the ommited variable bias (true value of b is actually 0.2)\n\nprovided an estimate for the effect of variable gender\n\n\n\n\n\n\n\n\n\n\n\n    \n\n\n\n\nIndividual\nGender\nShirt color\nEducation\n…\n\n\n\n\n1\nM\n?\n?\n?\n\n\n2\nF\n?\n?\n?\n\n\n3\nM\n?\n?\n?\n\n\n…\n…\n…\n…\n…\n\n\n\n\n\n\n\nSuppose we want to know the effect of \\(x\\) on \\(y\\).\n\nWe run the regression \\(y = a + b x\\)\n\nwe find \\(y = 1.09 + \\color{red}{0.24} x\\)\n\n\nBut now, the data has a special panel structure.\n\nwe have a categorical variable corresponding to each individual\nit is linked to several ommited variables, most of which we don’t observe\n\n\nFixed Effects: use an indicator for each individual\nwe need to drop the individual specific dummies that we observe (i.e. gender)\n\nRun the regression $y_{i,n} = a_i + b x_{i,n} $\n\nwe find $ y_{i,n} = a_i + x_{i,n} $\n\n\nNote that adding the indicator\n\n\nimproved the fit (\\(R^2\\) is 0.631 instead of 0.278)\n\ncorrected for the unobserved heterogeneity bias (true value of b is actually 0.2)\n\n\n\n\n\n\n\n\n\nEssentially: the intuition behing correcting for unobserved heterogeneity is the same as the one behind ommitted variable bias.\nFixed Effects are essentially dummies specific to some categories of observations\nIn panel data, there are two categories: individual index and time\n\nindividual ( Entity Effects)\ntime (Time Effects)\n\nWe index regressions by them: \\[y_{it} = \\color{red}{a_i} + \\color{blue}{a_t} + b x_{i t} + ... \\]\nFixed effects are not very useful for prediction:\n\none can predict \\(y_{it}\\) only for \\(i,t\\) already in the database\nimpossible to predict new individuals/new dates\n\nRemedy: random fixed effects\n\n\n\n\n\n\nConsider the regression model \\(y = a + b x + \\epsilon\\)\nWhen \\(\\epsilon\\) is correlated with \\(x\\) we have an endogeneity problem.\n\nwe can check in the regression results whether the residuals ares correlated with \\(y\\) or \\(x\\)\n\nEndogeneity can have several sources: ommited variable, measurement error, simultaneity\n\nit creates a bias in the estimate of \\(a\\) and \\(b\\)\n\nWe say we control for endogeneity by adding some variables\nA special case of endogeneity is a confounding factor a variable \\(z\\) which causes at the same time \\(x\\) and \\(y\\)\n\n\n\n\n\n\\[y = a + b x + \\epsilon\\]\n\nRecall: endogeneity issue when \\(\\epsilon\\) is correlated with \\(x\\)\nInstrument: a way to keep only the variability of \\(x\\) that is independent from \\(\\epsilon\\)\n\nit needs to be correlated with \\(x\\)\nnot with all components of \\(\\epsilon\\)\n\nAn instrument can be used to solve endogeneity issues\nIt can also establish the causality from \\(x\\) to \\(y\\):\n\nsince it is independent from \\(\\epsilon\\), all its effect on \\(y\\) goes through \\(x\\)"
  },
  {
    "objectID": "10_recap.html#important-points",
    "href": "10_recap.html#important-points",
    "title": "Data-Based Economics",
    "section": "",
    "text": "A model is fitted (trained / regressed) on a given amount of data\nA model can be more or less flexible\n\nhave more or less independent parameters (aka degrees of freedom)\nex: \\(y = a + b x\\) (2) vs \\(y = a + b x_1 + c x_1^2 + e x_2 + f x_3\\) (5)\n\nMore flexible models fit the training data better…\n…but tend to perform worse for predictions\nThis is known as:\n\nThe Bias (fit) vs Variance (prediction) tradeoff\nThe no free lunch theorem\n\n\n\n\n\n\n\n\n\n\n\n\nThe goal of machine learning consists in making the best predictions:\n\nuse enough data to maximize the fit…\n… but control the number of independent parameters to prevent overfitting\n\nex: LASSO regression has lots of parameters, but tries to keep most of them zero\n\nultimately quality of prediction is evaluated on a test set, independent from the training set\n\nIn econometrics we can perform\n\npredictions: sames issues as ML\nexplanatory analysis: focus on the effect of one (or a few) explanatory variables\n\nthis does not necessary require strong predictive power\n\n\n\n\n\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.252\nModel:                            OLS   Adj. R-squared:                  0.245\nMethod:                 Least Squares   F-statistic:                     33.08\nDate:                Tue, 30 Mar 2021   Prob (F-statistic):           1.01e-07\nTime:                        02:34:12   Log-Likelihood:                -111.39\nNo. Observations:                 100   AIC:                             226.8\nDf Residuals:                      98   BIC:                             232.0\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n==============================================================================\nIntercept     -0.1750      0.162     -1.082      0.282      -0.496       0.146\nx              0.1377      0.024      5.751      0.000       0.090       0.185\n==============================================================================\nOmnibus:                        2.673   Durbin-Watson:                   1.118\nProb(Omnibus):                  0.263   Jarque-Bera (JB):                2.654\nSkew:                           0.352   Prob(JB):                        0.265\nKurtosis:                       2.626   Cond. No.                         14.9\n==============================================================================\n\n\nUnderstand p-value: chances that a given statistics might have been obtained, under the H0 hypothesis\n\nCheck:\n\n\nglobal significance (Fisher test): chances would have obtained this R2 if all real coefficients were actually 0 (H0 hypothesis)\n\nR2: provides an indication of predictive power. Does not prevent overfitting.\n\nadj. R2: predictive power corrected for excessive degrees of freedom\n\ncoefficient:\n\np-value probability that coefficient might have been greater than observed, if it was actually 0.\nif p-value is smaller than 5%: the coefficient is significant at a 5% level\nconfidence intervals (5%): if the true coefficient was out of this interval, observed value would be very implausible\n\nhigher confidence levels -&gt; bigger intervals\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverfitting\n\n\nbad predictions\n\n\nColinearity\n\n\ncan bias a coefficient of interest\nnot a problem for prediction\nexact colinearity makes traditional OLS fail\n\nTo choose the right amount of variables find a combination which maximizes adjusted R2 or an information criterium\n\n\n\n\n\n\n\\(x\\) is colinear with \\(y\\) if \\(cor(x,y)\\) very close to 1\n\nmore generally \\(x\\) is colinear with \\(y_1, ... y_n\\) if \\(x\\) can be deduced linearly from \\(y_1...y_n\\)\nthere exists \\(\\lambda_1, ... \\lambda_n\\) such that \\(x = \\lambda_1 x_1 + ... + \\lambda_n x_n\\)\nexample: hours of sleep / hours awake (sleep=24-awake)\n\nperfect colinearity is a problem: coefficients are not well defined\n\\(\\text{productivity} = 0.1 + 0.5 \\text{sleep} + 0.5 \\text{awake}\\) or \\(\\text{productivity} = -11.9 + 1 \\text{sleep} + 1 \\text{awake}\\) ?\n\nbest regressions have regressors that:\nexplain independent variable\nare independent from each other (as much as possible)\n\n\n\n\n\nWhat if you don’t have enough variables?\n\\(y = a + bx\\)\n\nR2 can be low. It’s ok for explanatory analysis.\nas long as residuals are normally distributed\n\ncheck graphically to be sure\n(more advanced): there are statistical tests\n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\nSuppose we want to know the effect of \\(x\\) on \\(y\\).\n\nWe run the regression \\(y = a + b x\\)\n\nwe find \\(y = 0.21 + \\color{red}{0.15} x\\)\n\n\nWe then realize we have access to a categorical variable \\(gender \\in {male, female}\\)\n\nWe then add the \\(\\delta\\) dummy variable to the regression: \\(y = a + bx + c \\delta\\)\n\nwe find $ y = -0.04 + x - 0.98 $\n\n\nNote that adding the indicator\n\n\nimproved the fit (\\(R^2\\) is 0.623 instead of 0.306)\n\ncorrected for the ommited variable bias (true value of b is actually 0.2)\n\nprovided an estimate for the effect of variable gender\n\n\n\n\n\n\n\n\n\n\n\n    \n\n\n\n\nIndividual\nGender\nShirt color\nEducation\n…\n\n\n\n\n1\nM\n?\n?\n?\n\n\n2\nF\n?\n?\n?\n\n\n3\nM\n?\n?\n?\n\n\n…\n…\n…\n…\n…\n\n\n\n\n\n\n\nSuppose we want to know the effect of \\(x\\) on \\(y\\).\n\nWe run the regression \\(y = a + b x\\)\n\nwe find \\(y = 1.09 + \\color{red}{0.24} x\\)\n\n\nBut now, the data has a special panel structure.\n\nwe have a categorical variable corresponding to each individual\nit is linked to several ommited variables, most of which we don’t observe\n\n\nFixed Effects: use an indicator for each individual\nwe need to drop the individual specific dummies that we observe (i.e. gender)\n\nRun the regression $y_{i,n} = a_i + b x_{i,n} $\n\nwe find $ y_{i,n} = a_i + x_{i,n} $\n\n\nNote that adding the indicator\n\n\nimproved the fit (\\(R^2\\) is 0.631 instead of 0.278)\n\ncorrected for the unobserved heterogeneity bias (true value of b is actually 0.2)\n\n\n\n\n\n\n\n\n\nEssentially: the intuition behing correcting for unobserved heterogeneity is the same as the one behind ommitted variable bias.\nFixed Effects are essentially dummies specific to some categories of observations\nIn panel data, there are two categories: individual index and time\n\nindividual ( Entity Effects)\ntime (Time Effects)\n\nWe index regressions by them: \\[y_{it} = \\color{red}{a_i} + \\color{blue}{a_t} + b x_{i t} + ... \\]\nFixed effects are not very useful for prediction:\n\none can predict \\(y_{it}\\) only for \\(i,t\\) already in the database\nimpossible to predict new individuals/new dates\n\nRemedy: random fixed effects\n\n\n\n\n\n\nConsider the regression model \\(y = a + b x + \\epsilon\\)\nWhen \\(\\epsilon\\) is correlated with \\(x\\) we have an endogeneity problem.\n\nwe can check in the regression results whether the residuals ares correlated with \\(y\\) or \\(x\\)\n\nEndogeneity can have several sources: ommited variable, measurement error, simultaneity\n\nit creates a bias in the estimate of \\(a\\) and \\(b\\)\n\nWe say we control for endogeneity by adding some variables\nA special case of endogeneity is a confounding factor a variable \\(z\\) which causes at the same time \\(x\\) and \\(y\\)\n\n\n\n\n\n\\[y = a + b x + \\epsilon\\]\n\nRecall: endogeneity issue when \\(\\epsilon\\) is correlated with \\(x\\)\nInstrument: a way to keep only the variability of \\(x\\) that is independent from \\(\\epsilon\\)\n\nit needs to be correlated with \\(x\\)\nnot with all components of \\(\\epsilon\\)\n\nAn instrument can be used to solve endogeneity issues\nIt can also establish the causality from \\(x\\) to \\(y\\):\n\nsince it is independent from \\(\\epsilon\\), all its effect on \\(y\\) goes through \\(x\\)"
  },
  {
    "objectID": "ai/slides.html#do-you-like-poetry",
    "href": "ai/slides.html#do-you-like-poetry",
    "title": "Short Intro to Language Models",
    "section": "Do you like poetry?",
    "text": "Do you like poetry?\n\n\nA rose is a rose is a rose\n\n\n\nGertrude Stein\n\n\n\nBrexit means Brexit means Brexit\n\n\n\nJohn Crace\n\n\n\nElementary my dear Watson\n\n\n\nP.G. Woodehouse"
  },
  {
    "objectID": "ai/slides.html#section",
    "href": "ai/slides.html#section",
    "title": "Short Intro to Language Models",
    "section": "",
    "text": "There is an easy way for the government to end the strike without withdrawing the pension reform,"
  },
  {
    "objectID": "ai/slides.html#complete-text",
    "href": "ai/slides.html#complete-text",
    "title": "Short Intro to Language Models",
    "section": "Complete Text",
    "text": "Complete Text\nGenerative language models perform text completion\nThey generate plausible1 text following a prompt.\nThe type of answer, will depend on the kind of prompt.\nhere, plausible, means that it is more likely to be a correct text written by a human, rather than otherwise"
  },
  {
    "objectID": "ai/slides.html#gpt3-playground",
    "href": "ai/slides.html#gpt3-playground",
    "title": "Short Intro to Language Models",
    "section": "GPT3 Playground",
    "text": "GPT3 Playground\nTo use GPT-3 profficiently, you have to experiment with the prompt.\n\ntry the Playground mode\n\nIt is the same as learning how to do google queries\n\naltavista: +noir +film -\"pinot noir\"\nnowadays: ???"
  },
  {
    "objectID": "ai/slides.html#some-examples",
    "href": "ai/slides.html#some-examples",
    "title": "Short Intro to Language Models",
    "section": "Some Examples",
    "text": "Some Examples\nBy providing enough context, it is possible to perform amazing tasks\nLook at the demos"
  },
  {
    "objectID": "ai/slides.html#language-models-and-cryptography",
    "href": "ai/slides.html#language-models-and-cryptography",
    "title": "Short Intro to Language Models",
    "section": "Language Models and Cryptography",
    "text": "Language Models and Cryptography\n\nThe Caesar code"
  },
  {
    "objectID": "ai/slides.html#section-1",
    "href": "ai/slides.html#section-1",
    "title": "Short Intro to Language Models",
    "section": "",
    "text": "Zodiac 408 Cipher"
  },
  {
    "objectID": "ai/slides.html#section-2",
    "href": "ai/slides.html#section-2",
    "title": "Short Intro to Language Models",
    "section": "",
    "text": "Zodiac 408 Cipher\n\n\n\n\n\n\n\nKey for Zodiac 408\n\n\n\n\nFigure 1: Solved in a week by Bettye and Donald Harden using frequency tables."
  },
  {
    "objectID": "ai/slides.html#section-3",
    "href": "ai/slides.html#section-3",
    "title": "Short Intro to Language Models",
    "section": "",
    "text": "Later in 2001, in a prison, somewhere in California\n\n\nSolved by Stanford’s Persi Diaconis and his students using Monte Carlo Markov Chains"
  },
  {
    "objectID": "ai/slides.html#monte-carlo-markov-chains",
    "href": "ai/slides.html#monte-carlo-markov-chains",
    "title": "Short Intro to Language Models",
    "section": "Monte Carlo Markov Chains",
    "text": "Monte Carlo Markov Chains\nTake a letter \\(x_n\\), what is the probability of the next letter being \\(x_{n+1}\\)?\n\\[\\pi_{X,Y} = P(x_{n+1}=Y, x_{n}=X)\\]\nfor \\(X=\\{a, b, .... , z\\} , Y=\\{a,b,c, ... z\\}\\)\nThe language model can be trained using dataset of english language.\nAnd used to determine whether a given cipher-key is consistent with english language.\nIt yields a very efficient algorithm to decode any caesar code (with very small sample)"
  },
  {
    "objectID": "ai/slides.html#mcmc-to-generate-text",
    "href": "ai/slides.html#mcmc-to-generate-text",
    "title": "Short Intro to Language Models",
    "section": "MCMC to generate text",
    "text": "MCMC to generate text\nMCMCs can also be used to generate text:\n\ntake initial prompt: I think therefore I\n\nlast letter is I\nmost plausible character afterwards is \nmost plausible character afterwards is I\n\nResult: I think therefore I I I I I I\n\nNot good but promising (🤷)"
  },
  {
    "objectID": "ai/slides.html#mcmc-to-generate-text-1",
    "href": "ai/slides.html#mcmc-to-generate-text-1",
    "title": "Short Intro to Language Models",
    "section": "MCMC to generate text",
    "text": "MCMC to generate text\nGoing further\n\naugment memory\n\nfore I&gt; ???\n\nchange basic unit (use phonems or words)\n\nAn example using MCMC\n\nusing words and 3 states He ha ‘s kill’d me Mother , Run away I pray you Oh this is Counter you false Danish Dogges ."
  },
  {
    "objectID": "ai/slides.html#big-mcmc",
    "href": "ai/slides.html#big-mcmc",
    "title": "Short Intro to Language Models",
    "section": "Big MCMC",
    "text": "Big MCMC\nCan we augment memory?\n\nif you want to compute the most frequent letter (among 26) after 50 letters, you need to take into account 5.6061847e+70 combinations !\n\nimpossible to store, let alone do the training\n\nbut some combinations are useless:\n\nwjai dfni\nDespite the constant negative press covfefe 🤔"
  },
  {
    "objectID": "ai/slides.html#reccurrent-neural-networks",
    "href": "ai/slides.html#reccurrent-neural-networks",
    "title": "Short Intro to Language Models",
    "section": "Reccurrent Neural Networks",
    "text": "Reccurrent Neural Networks\n\n\n\n\n\n\n\nRecurrent Neural Network\n\n\n\n\nNeural networks make it possible to increase the state-space to represent\n\\[\\forall X, P(x_n=X| x_{n-1}, ..., x_{n-k})\\]\nthey reduce endogenously the dimensionality."
  },
  {
    "objectID": "ai/slides.html#recurrent-neural-networks",
    "href": "ai/slides.html#recurrent-neural-networks",
    "title": "Short Intro to Language Models",
    "section": "Recurrent Neural Networks",
    "text": "Recurrent Neural Networks\nIn 2015\n\n\n\nNeural Network reduce dimensionality of data discovering structure"
  },
  {
    "objectID": "ai/slides.html#long-short-term-memory",
    "href": "ai/slides.html#long-short-term-memory",
    "title": "Short Intro to Language Models",
    "section": "Long Short Term Memory",
    "text": "Long Short Term Memory\n\n\n2000-&gt;2019 : Emergence of Long Short Term Memory models\n\nspeech recognition\nLSTM behind “Google Translate”, “Alexa”, …"
  },
  {
    "objectID": "ai/slides.html#the-latest-transformer",
    "href": "ai/slides.html#the-latest-transformer",
    "title": "Short Intro to Language Models",
    "section": "The Latest: Transformer",
    "text": "The Latest: Transformer\n\n\n\nPosition Encodings\n\nmodel is not sequential anymore\ntries to learn sequence\n\nAttention\nSelf-Attention\n\n\n\n\n\nExplanations here or here"
  },
  {
    "objectID": "ai/slides.html#quick-summary",
    "href": "ai/slides.html#quick-summary",
    "title": "Short Intro to Language Models",
    "section": "Quick summary",
    "text": "Quick summary\n\nLanguage models\n\nfrequency tables\nmonte carlo markov chains\nlong-short-term memory (&gt;2000)\ntransformers (&gt;2018)\n\n\n\n\nSince 2010 main breakthrough came through the development of deep-learning techniques (software/hardware)\nRecently, models/algorithms have improved tremendously"
  },
  {
    "objectID": "ai/slides.html#gpt3",
    "href": "ai/slides.html#gpt3",
    "title": "Short Intro to Language Models",
    "section": "GPT3",
    "text": "GPT3\nGenerative Pre-trained Transformer\n\nGPT1 (1018)\n\n0.1 billion parameters\nhad to be fine-tuned to a particular problem\ntransfer learning (few shots learning)\n\nGPT2:\n\nmultitask\nno mandatory fine tuning\n\nGPT3:\n\nbigger: 175 billions parameters\n\nGPT4:\n\neven bigger: 1000 billions parameters\non your harddrive: 1Tb"
  },
  {
    "objectID": "ai/slides.html#corpus",
    "href": "ai/slides.html#corpus",
    "title": "Short Intro to Language Models",
    "section": "Corpus",
    "text": "Corpus\n\n\nGPT 3 was trained on\n\nCommonCrawl\nWebText\nWikipedia\nmany books\n\n45 TB of data\n\ncured into ???\n\nDataset (mostly) ends in 2021."
  },
  {
    "objectID": "ai/slides.html#chat-gpt",
    "href": "ai/slides.html#chat-gpt",
    "title": "Short Intro to Language Models",
    "section": "Chat GPT",
    "text": "Chat GPT\nIt is trivial to make a chatbot using GPT3.\nChatGPT is an interface on top of GPT3.5 (now GPT4) plus some fine-tuning.\n\nit keeps the context\nhas a consistent “personality”"
  },
  {
    "objectID": "ai/slides.html#the-relation-between-gpt-3-and-chatgpt",
    "href": "ai/slides.html#the-relation-between-gpt-3-and-chatgpt",
    "title": "Short Intro to Language Models",
    "section": "The Relation between GPT-3 and ChatGPT",
    "text": "The Relation between GPT-3 and ChatGPT\nTechnical paper is not out, but we know the following:\n\nit is fine-tuned by interacting with humans\nit uses reinforcement learning to optimize learning\nnow based on GPT4\n\nImprovment goals:\n\nbetter alignment\nless making up of facts"
  },
  {
    "objectID": "ai/slides.html#generate-text",
    "href": "ai/slides.html#generate-text",
    "title": "Short Intro to Language Models",
    "section": "Generate Text",
    "text": "Generate Text\n\n\nWould you use GPT3 to:\n\n\nwrite a paper?\nquickly respond to an email\nwrite the boring details of a paper\nget some ideas?\nhelp you structure a talk?\n\n\nThere are several concerns…"
  },
  {
    "objectID": "ai/slides.html#the-quality-of-the-generated-text",
    "href": "ai/slides.html#the-quality-of-the-generated-text",
    "title": "Short Intro to Language Models",
    "section": "The quality of the generated text",
    "text": "The quality of the generated text\n\n\nGPT-3 has the tendancy to hallucinate facts.\n\nlike kids who don’t distinguish facts and play\n\nThis is being worked on:\n\nGPT4: Can you teach old dogs new tricks? yes 🐕\n\nStill a problem for research\nPossibly solved by mixing AI and traditional computing"
  },
  {
    "objectID": "ai/slides.html#ethical-concerns",
    "href": "ai/slides.html#ethical-concerns",
    "title": "Short Intro to Language Models",
    "section": "Ethical Concerns",
    "text": "Ethical Concerns\nWe must avoid at all cost plagiarism and credit our sources.\nWe need personal ethics:\n\ngive sources\nmention use of GPT3 we we use it"
  },
  {
    "objectID": "ai/slides.html#replace-many-nlp-algorithms",
    "href": "ai/slides.html#replace-many-nlp-algorithms",
    "title": "Short Intro to Language Models",
    "section": "Replace Many NLP Algorithms",
    "text": "Replace Many NLP Algorithms\nBeyond generating text, most Natural Language Processing tasks can be now done with GPT-3:\n\nNamed entity recognition\nClassification\n\nsentiment analysis\nmultimodel sentiment analysis\n\nEntity linking\nSummarization\nMany more"
  },
  {
    "objectID": "ai/slides.html#named-entity-recognition",
    "href": "ai/slides.html#named-entity-recognition",
    "title": "Short Intro to Language Models",
    "section": "Named Entity recognition",
    "text": "Named Entity recognition\nPrompt:1\n[Text]: Helena Smith founded Core.ai 2 years ago. She is now the CEO and CTO of the company and is building a team of highly skilled developers in machine learning and natural language processing.\n[Position]: CEO and CTO\n###\n[Text]: Tech Robotics is a robot automation company specialized in AI driven robotization. Its Chief Technology Officer, Max Smith, says a new wave of improvements should be expected for next year.\n[Position]: Chief Technology Officer\n###\n[Text]: François is a Go developer. He mostly works as a freelancer but is open to any kind of job offering!\n[Position]: Go developer\n###\n[Text]: Maxime is a data scientist at Auto Dataset, and he's been working there for 1 year.\n[Position]:\n\nResponse: Data Scientist\n\nhttps://towardsdatascience.com/advanced-ner-with-gpt-3-and-gpt-j-ce43dc6cdb9c"
  },
  {
    "objectID": "ai/slides.html#fine-tuning-1",
    "href": "ai/slides.html#fine-tuning-1",
    "title": "Short Intro to Language Models",
    "section": "Fine-tuning (1)",
    "text": "Fine-tuning (1)\n\nTo get better result, or to proceed larger amounts of data, pretrain the model\n\nmodel.train(\"\"\"\n\"[Text]: Helena Smith founded Core.ai 2 years ago. She is now the CEO and CTO of the company and is building a team of highly skilled developers in machine learning and natural language processing.\n[Position]: CEO and CTO\n\"\"\")\nmodel.train(\"\"\"\n[Text]: Tech Robotics is a robot automation company specialized in AI driven robotization. Its Chief Technology Officer, Max Smith, says a new wave of improvements should be expected for next year.\n[Position]: Chief Technology Officer\n\"\"\")"
  },
  {
    "objectID": "ai/slides.html#fine-tuning-2",
    "href": "ai/slides.html#fine-tuning-2",
    "title": "Short Intro to Language Models",
    "section": "Fine-tuning (2)",
    "text": "Fine-tuning (2)\n\nQuery the model:\n\nmodel.eval(\"\"\"\n[Text]: Maxime is a data scientist at Auto Dataset, and he's been working there for 1 year.\n[Position]:\n\"\"\")\n…\n[Text]: Maxime is a data scientist at Auto Dataset, and he's been working there for 1 year.\n[Position]: Data Scientist```"
  },
  {
    "objectID": "ai/slides.html#sentiment-analysis",
    "href": "ai/slides.html#sentiment-analysis",
    "title": "Short Intro to Language Models",
    "section": "Sentiment Analysis",
    "text": "Sentiment Analysis\nZero-shot learning:\nDecide whether a Tweet's sentiment is positive, neutral, or negative.\n\nTweet: \"I loved the new Batman movie!\"\nSentiment:\nOutput: Positive"
  },
  {
    "objectID": "ai/slides.html#are-we-on-the-way-to-general-artificial-intelligence",
    "href": "ai/slides.html#are-we-on-the-way-to-general-artificial-intelligence",
    "title": "Short Intro to Language Models",
    "section": "Are we on the way to General Artificial Intelligence?",
    "text": "Are we on the way to General Artificial Intelligence?\n\nA Benevolent General Purpose Artificial Intelligence. Dall-E-2\nMy two cents:\n\n\n\nartificial intelligence is already there (we are fooling ourselves…)\nare we ready for artificial humanity?"
  },
  {
    "objectID": "ai/index.html",
    "href": "ai/index.html",
    "title": "Short Intro to Language Models",
    "section": "",
    "text": ". . .\n\nA rose is a rose is a rose\n\n. . .\nGertrude Stein\n. . .\n\nBrexit means Brexit means Brexit\n\n. . .\nJohn Crace\n. . .\n\nElementary my dear Watson\n\n. . .\nP.G. Woodehouse\n. . .\n\n\n\n\nThere is an easy way for the government to end the strike without withdrawing the pension reform,\n\n. . .\n\n\n\nGenerative language models perform text completion\nThey generate plausible1 text following a prompt.\nThe type of answer, will depend on the kind of prompt.\n\n\n\nTo use GPT-3 profficiently, you have to experiment with the prompt.\n\ntry the Playground mode\n\nIt is the same as learning how to do google queries\n\naltavista: +noir +film -\"pinot noir\"\nnowadays: ???\n\n\n\n\nBy providing enough context, it is possible to perform amazing tasks\nLook at the demos"
  },
  {
    "objectID": "ai/index.html#do-you-like-poetry",
    "href": "ai/index.html#do-you-like-poetry",
    "title": "Short Intro to Language Models",
    "section": "",
    "text": ". . .\n\nA rose is a rose is a rose\n\n. . .\nGertrude Stein\n. . .\n\nBrexit means Brexit means Brexit\n\n. . .\nJohn Crace\n. . .\n\nElementary my dear Watson\n\n. . .\nP.G. Woodehouse\n. . ."
  },
  {
    "objectID": "ai/index.html#section",
    "href": "ai/index.html#section",
    "title": "Short Intro to Language Models",
    "section": "",
    "text": "There is an easy way for the government to end the strike without withdrawing the pension reform,\n\n. . ."
  },
  {
    "objectID": "ai/index.html#complete-text",
    "href": "ai/index.html#complete-text",
    "title": "Short Intro to Language Models",
    "section": "",
    "text": "Generative language models perform text completion\nThey generate plausible1 text following a prompt.\nThe type of answer, will depend on the kind of prompt."
  },
  {
    "objectID": "ai/index.html#gpt3-playground",
    "href": "ai/index.html#gpt3-playground",
    "title": "Short Intro to Language Models",
    "section": "",
    "text": "To use GPT-3 profficiently, you have to experiment with the prompt.\n\ntry the Playground mode\n\nIt is the same as learning how to do google queries\n\naltavista: +noir +film -\"pinot noir\"\nnowadays: ???"
  },
  {
    "objectID": "ai/index.html#some-examples",
    "href": "ai/index.html#some-examples",
    "title": "Short Intro to Language Models",
    "section": "",
    "text": "By providing enough context, it is possible to perform amazing tasks\nLook at the demos"
  },
  {
    "objectID": "ai/index.html#language-models-and-cryptography",
    "href": "ai/index.html#language-models-and-cryptography",
    "title": "Short Intro to Language Models",
    "section": "Language Models and Cryptography",
    "text": "Language Models and Cryptography\n\nThe Caesar code"
  },
  {
    "objectID": "ai/index.html#section-1",
    "href": "ai/index.html#section-1",
    "title": "Short Intro to Language Models",
    "section": "",
    "text": "Zodiac 408 Cipher"
  },
  {
    "objectID": "ai/index.html#section-2",
    "href": "ai/index.html#section-2",
    "title": "Short Intro to Language Models",
    "section": "",
    "text": "Zodiac 408 Cipher\n\n\n\n\n\n\n\nKey for Zodiac 408\n\n\n\n\nFigure 1: Solved in a week by Bettye and Donald Harden using frequency tables."
  },
  {
    "objectID": "ai/index.html#section-3",
    "href": "ai/index.html#section-3",
    "title": "Short Intro to Language Models",
    "section": "",
    "text": "Later in 2001, in a prison, somewhere in California\n\n. . .\nSolved by Stanford’s Persi Diaconis and his students using Monte Carlo Markov Chains"
  },
  {
    "objectID": "ai/index.html#monte-carlo-markov-chains",
    "href": "ai/index.html#monte-carlo-markov-chains",
    "title": "Short Intro to Language Models",
    "section": "Monte Carlo Markov Chains",
    "text": "Monte Carlo Markov Chains\nTake a letter \\(x_n\\), what is the probability of the next letter being \\(x_{n+1}\\)?\n\\[\\pi_{X,Y} = P(x_{n+1}=Y, x_{n}=X)\\]\nfor \\(X=\\{a, b, .... , z\\} , Y=\\{a,b,c, ... z\\}\\)\nThe language model can be trained using dataset of english language.\nAnd used to determine whether a given cipher-key is consistent with english language.\nIt yields a very efficient algorithm to decode any caesar code (with very small sample)"
  },
  {
    "objectID": "ai/index.html#mcmc-to-generate-text",
    "href": "ai/index.html#mcmc-to-generate-text",
    "title": "Short Intro to Language Models",
    "section": "MCMC to generate text",
    "text": "MCMC to generate text\nMCMCs can also be used to generate text:\n\ntake initial prompt: I think therefore I\n\nlast letter is I\nmost plausible character afterwards is \nmost plausible character afterwards is I\n\nResult: I think therefore I I I I I I\n\nNot good but promising (🤷)"
  },
  {
    "objectID": "ai/index.html#mcmc-to-generate-text-1",
    "href": "ai/index.html#mcmc-to-generate-text-1",
    "title": "Short Intro to Language Models",
    "section": "MCMC to generate text",
    "text": "MCMC to generate text\nGoing further\n\naugment memory\n\nfore I&gt; ???\n\nchange basic unit (use phonems or words)\n\nAn example using MCMC\n\nusing words and 3 states He ha ‘s kill’d me Mother , Run away I pray you Oh this is Counter you false Danish Dogges ."
  },
  {
    "objectID": "ai/index.html#big-mcmc",
    "href": "ai/index.html#big-mcmc",
    "title": "Short Intro to Language Models",
    "section": "Big MCMC",
    "text": "Big MCMC\nCan we augment memory?\n\nif you want to compute the most frequent letter (among 26) after 50 letters, you need to take into account 5.6061847e+70 combinations !\n\nimpossible to store, let alone do the training\n\nbut some combinations are useless:\n\nwjai dfni\nDespite the constant negative press covfefe 🤔"
  },
  {
    "objectID": "ai/index.html#reccurrent-neural-networks",
    "href": "ai/index.html#reccurrent-neural-networks",
    "title": "Short Intro to Language Models",
    "section": "Reccurrent Neural Networks",
    "text": "Reccurrent Neural Networks\n\n\n\n\n\n\n\nRecurrent Neural Network\n\n\n\n\nNeural networks make it possible to increase the state-space to represent\n\\[\\forall X, P(x_n=X| x_{n-1}, ..., x_{n-k})\\]\nthey reduce endogenously the dimensionality."
  },
  {
    "objectID": "ai/index.html#recurrent-neural-networks",
    "href": "ai/index.html#recurrent-neural-networks",
    "title": "Short Intro to Language Models",
    "section": "Recurrent Neural Networks",
    "text": "Recurrent Neural Networks\nIn 2015\n\n\n\nNeural Network reduce dimensionality of data discovering structure"
  },
  {
    "objectID": "ai/index.html#long-short-term-memory",
    "href": "ai/index.html#long-short-term-memory",
    "title": "Short Intro to Language Models",
    "section": "Long Short Term Memory",
    "text": "Long Short Term Memory\n\n\n2000-&gt;2019 : Emergence of Long Short Term Memory models\n\nspeech recognition\nLSTM behind “Google Translate”, “Alexa”, …"
  },
  {
    "objectID": "ai/index.html#the-latest-transformer",
    "href": "ai/index.html#the-latest-transformer",
    "title": "Short Intro to Language Models",
    "section": "The Latest: Transformer",
    "text": "The Latest: Transformer\n\n\n\nPosition Encodings\n\nmodel is not sequential anymore\ntries to learn sequence\n\nAttention\nSelf-Attention\n\n\n\n\n\nExplanations here or here"
  },
  {
    "objectID": "ai/index.html#quick-summary",
    "href": "ai/index.html#quick-summary",
    "title": "Short Intro to Language Models",
    "section": "Quick summary",
    "text": "Quick summary\n\nLanguage models\n\nfrequency tables\nmonte carlo markov chains\nlong-short-term memory (&gt;2000)\ntransformers (&gt;2018)\n\n\n. . .\n\nSince 2010 main breakthrough came through the development of deep-learning techniques (software/hardware)\nRecently, models/algorithms have improved tremendously"
  },
  {
    "objectID": "ai/index.html#gpt3",
    "href": "ai/index.html#gpt3",
    "title": "Short Intro to Language Models",
    "section": "GPT3",
    "text": "GPT3\nGenerative Pre-trained Transformer\n\nGPT1 (1018)\n\n0.1 billion parameters\nhad to be fine-tuned to a particular problem\ntransfer learning (few shots learning)\n\nGPT2:\n\nmultitask\nno mandatory fine tuning\n\nGPT3:\n\nbigger: 175 billions parameters\n\nGPT4:\n\neven bigger: 1000 billions parameters\non your harddrive: 1Tb"
  },
  {
    "objectID": "ai/index.html#corpus",
    "href": "ai/index.html#corpus",
    "title": "Short Intro to Language Models",
    "section": "Corpus",
    "text": "Corpus\n\n\nGPT 3 was trained on\n\nCommonCrawl\nWebText\nWikipedia\nmany books\n\n45 TB of data\n\ncured into ???\n\nDataset (mostly) ends in 2021."
  },
  {
    "objectID": "ai/index.html#chat-gpt",
    "href": "ai/index.html#chat-gpt",
    "title": "Short Intro to Language Models",
    "section": "Chat GPT",
    "text": "Chat GPT\nIt is trivial to make a chatbot using GPT3.\nChatGPT is an interface on top of GPT3.5 (now GPT4) plus some fine-tuning.\n\nit keeps the context\nhas a consistent “personality”"
  },
  {
    "objectID": "ai/index.html#the-relation-between-gpt-3-and-chatgpt",
    "href": "ai/index.html#the-relation-between-gpt-3-and-chatgpt",
    "title": "Short Intro to Language Models",
    "section": "The Relation between GPT-3 and ChatGPT",
    "text": "The Relation between GPT-3 and ChatGPT\nTechnical paper is not out, but we know the following:\n\nit is fine-tuned by interacting with humans\nit uses reinforcement learning to optimize learning\nnow based on GPT4\n\nImprovment goals:\n\nbetter alignment\nless making up of facts"
  },
  {
    "objectID": "ai/index.html#generate-text",
    "href": "ai/index.html#generate-text",
    "title": "Short Intro to Language Models",
    "section": "Generate Text",
    "text": "Generate Text\n\n\nWould you use GPT3 to:\n\n\nwrite a paper?\nquickly respond to an email\nwrite the boring details of a paper\nget some ideas?\nhelp you structure a talk?\n\n\nThere are several concerns…"
  },
  {
    "objectID": "ai/index.html#the-quality-of-the-generated-text",
    "href": "ai/index.html#the-quality-of-the-generated-text",
    "title": "Short Intro to Language Models",
    "section": "The quality of the generated text",
    "text": "The quality of the generated text\n\n\nGPT-3 has the tendancy to hallucinate facts.\n\nlike kids who don’t distinguish facts and play\n\nThis is being worked on:\n\nGPT4: Can you teach old dogs new tricks? yes 🐕\n\nStill a problem for research\nPossibly solved by mixing AI and traditional computing"
  },
  {
    "objectID": "ai/index.html#ethical-concerns",
    "href": "ai/index.html#ethical-concerns",
    "title": "Short Intro to Language Models",
    "section": "Ethical Concerns",
    "text": "Ethical Concerns\nWe must avoid at all cost plagiarism and credit our sources.\nWe need personal ethics:\n\ngive sources\nmention use of GPT3 we we use it"
  },
  {
    "objectID": "ai/index.html#replace-many-nlp-algorithms",
    "href": "ai/index.html#replace-many-nlp-algorithms",
    "title": "Short Intro to Language Models",
    "section": "Replace Many NLP Algorithms",
    "text": "Replace Many NLP Algorithms\nBeyond generating text, most Natural Language Processing tasks can be now done with GPT-3:\n\nNamed entity recognition\nClassification\n\nsentiment analysis\nmultimodel sentiment analysis\n\nEntity linking\nSummarization\nMany more"
  },
  {
    "objectID": "ai/index.html#named-entity-recognition",
    "href": "ai/index.html#named-entity-recognition",
    "title": "Short Intro to Language Models",
    "section": "Named Entity recognition",
    "text": "Named Entity recognition\nPrompt:2\n[Text]: Helena Smith founded Core.ai 2 years ago. She is now the CEO and CTO of the company and is building a team of highly skilled developers in machine learning and natural language processing.\n[Position]: CEO and CTO\n###\n[Text]: Tech Robotics is a robot automation company specialized in AI driven robotization. Its Chief Technology Officer, Max Smith, says a new wave of improvements should be expected for next year.\n[Position]: Chief Technology Officer\n###\n[Text]: François is a Go developer. He mostly works as a freelancer but is open to any kind of job offering!\n[Position]: Go developer\n###\n[Text]: Maxime is a data scientist at Auto Dataset, and he's been working there for 1 year.\n[Position]:\n. . .\nResponse: Data Scientist"
  },
  {
    "objectID": "ai/index.html#fine-tuning-1",
    "href": "ai/index.html#fine-tuning-1",
    "title": "Short Intro to Language Models",
    "section": "Fine-tuning (1)",
    "text": "Fine-tuning (1)\n\nTo get better result, or to proceed larger amounts of data, pretrain the model\n\n```{python}\nmodel.train(\"\"\"\n\"[Text]: Helena Smith founded Core.ai 2 years ago. She is now the CEO and CTO of the company and is building a team of highly skilled developers in machine learning and natural language processing.\n[Position]: CEO and CTO\n\"\"\")\n```\n```{python}\nmodel.train(\"\"\"\n[Text]: Tech Robotics is a robot automation company specialized in AI driven robotization. Its Chief Technology Officer, Max Smith, says a new wave of improvements should be expected for next year.\n[Position]: Chief Technology Officer\n\"\"\")\n```"
  },
  {
    "objectID": "ai/index.html#fine-tuning-2",
    "href": "ai/index.html#fine-tuning-2",
    "title": "Short Intro to Language Models",
    "section": "Fine-tuning (2)",
    "text": "Fine-tuning (2)\n\nQuery the model:\n\n```{python}\nmodel.eval(\"\"\"\n[Text]: Maxime is a data scientist at Auto Dataset, and he's been working there for 1 year.\n[Position]:\n\"\"\")\n```\n…\n[Text]: Maxime is a data scientist at Auto Dataset, and he's been working there for 1 year.\n[Position]: Data Scientist```"
  },
  {
    "objectID": "ai/index.html#sentiment-analysis",
    "href": "ai/index.html#sentiment-analysis",
    "title": "Short Intro to Language Models",
    "section": "Sentiment Analysis",
    "text": "Sentiment Analysis\nZero-shot learning:\nDecide whether a Tweet's sentiment is positive, neutral, or negative.\n\nTweet: \"I loved the new Batman movie!\"\nSentiment:\nOutput: Positive"
  },
  {
    "objectID": "ai/index.html#are-we-on-the-way-to-general-artificial-intelligence",
    "href": "ai/index.html#are-we-on-the-way-to-general-artificial-intelligence",
    "title": "Short Intro to Language Models",
    "section": "Are we on the way to General Artificial Intelligence?",
    "text": "Are we on the way to General Artificial Intelligence?\n\n\n\nA Benevolent General Purpose Artificial Intelligence. Dall-E-2\n\n\n. . .\nMy two cents:\n. . .\n\nartificial intelligence is already there (we are fooling ourselves…)\nare we ready for artificial humanity?"
  },
  {
    "objectID": "ai/index.html#footnotes",
    "href": "ai/index.html#footnotes",
    "title": "Short Intro to Language Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhere, plausible, means that it is more likely to be a correct text written by a human, rather than otherwise↩︎\nhttps://towardsdatascience.com/advanced-ner-with-gpt-3-and-gpt-j-ce43dc6cdb9c↩︎"
  },
  {
    "objectID": "coursework/coursework.html",
    "href": "coursework/coursework.html",
    "title": "Data-Based Economics",
    "section": "",
    "text": "A visualization\n\nChoose a simple graph from https://ourworldindata.org/ and try to reproduce it as well as you can. You can use the data that is usually attached with the graphs.\n\n\nFeldstein-Horioka puzzle\n\nIn a famous paper, Domestic saving and international capital flows (Economic Journal, 1980), Martin Feldstein and Charles Horioka, exposed the following puzzle: - if international capital markets were"
  },
  {
    "objectID": "coursework/ideas.html",
    "href": "coursework/ideas.html",
    "title": "Data-Based Economics",
    "section": "",
    "text": "Feldstein-Horioka Puzzle (Économétrie Appliquée)\n\nCorrélation S/Y and I/Y\nPays de l’OCDE\nCollecte des données Régression simple\n\nRègle de Taylor: évaluation empirique sur 6 plus gros pays de l’Union Européenne\n\n\ndatabase: PIB (nominal)\nIPC\ntaux d’intérêt à court terme\n\n\nCAPM"
  },
  {
    "objectID": "coursework/instructions.html",
    "href": "coursework/instructions.html",
    "title": "Coursework",
    "section": "",
    "text": "You are allowed to work in small groups with up to 3 people. The work must be returned on April X (X to be determined on April 5). It should consist in a jupyter notebook, with all the text and code toghether with the attached datafile(s). The notebook must make for enjoyable reading without any prior knowledge of the paper.\nYour goal is to replicate the main result from one of the two following famous papers:\n\nAre Emily and Greg More Employable Than Lakisha and Jamal? A Field Experiment on Labor Market Discrimination, by Marianne Bertrand and Sendhil Mullainathan, Americal Economic Review, 2004\nMalleable Lies: Communication and Cooperation in a High StakesTV Game, by Uyanga Turmunkh, Martijn J. van den Assem, Dennie van Dolder, Management Science, October 2019\n\nIf you are interested in replicating another paper, discuss it with the professor before.\nThe instructions below apply to any paper.\n\nread the paper\nidentify the main hypothesis, and the empirical strategy\nlocate replication files (they are available from the publisher’s website)\nimport the data / describe the data\ntry to reproduce the main result(s) from the paper\n\nintepret the statistics and comment on what you get\nit’s ok if the figures are not exactly the same1\n\nmake the finishing touches:\n\nwork on the notebook to make it a nice reading\nensure the notebook can be run on any computer (in particuler, it should not reference computer specific file paths)"
  },
  {
    "objectID": "coursework/instructions.html#footnotes",
    "href": "coursework/instructions.html#footnotes",
    "title": "Coursework",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn the Bertrand and Mullainathan paper, they use probit regressions rather than logistic regression. They are essentially very similar (they make different distributional assumptions). I would advise to start with logistic regressions as in the course, and possibly compare the results you get with probit regressions.↩︎"
  },
  {
    "objectID": "coursework/test/Introduction.html",
    "href": "coursework/test/Introduction.html",
    "title": "Modern Scientific Programming",
    "section": "",
    "text": "PSE, 2019\nPablo Winant"
  },
  {
    "objectID": "coursework/test/Introduction.html#about-myself",
    "href": "coursework/test/Introduction.html#about-myself",
    "title": "Modern Scientific Programming",
    "section": "About Myself",
    "text": "About Myself\n\n(computational) economist\nworked at IMF and Bank of England\n\nDSGE modeling\nvarious technological projects\n\ninvolved in several opensource projects\n\nquantecon (Tom Sargent and John Stachurski)\nARK (Chris Carrol)\ndolo/dolark/bttt\n…\n\nresearch\n\nmacro stuff (international finance, inequalities, optimal policies)\nmachine learning (especially neural networks)"
  },
  {
    "objectID": "coursework/test/Introduction.html#what-is-modern-scientific-programming",
    "href": "coursework/test/Introduction.html#what-is-modern-scientific-programming",
    "title": "Modern Scientific Programming",
    "section": "What is Modern Scientific Programming?",
    "text": "What is Modern Scientific Programming?\n\nscientific programming\n\nalgorithm\nspeed, memory efficiency, accuracy\n= numerical methods (judd) + Fortran\n\nmodern?\n\nversioned\nopensource, free\nreproducible\nmultilanguage\nmultiplatform"
  },
  {
    "objectID": "coursework/test/Introduction.html#versioned-open-source",
    "href": "coursework/test/Introduction.html#versioned-open-source",
    "title": "Modern Scientific Programming",
    "section": "Versioned, open source",
    "text": "Versioned, open source\n\ngit is a decentralized, versioning system\ngithub adds online collaborative tools\nit has fostered, the development of many softwares, libraries (linux, python, Julia, R, etc.)\nindustry and community have converged on the same tools\n\n\ngraph about adoption of software since the introduction of Github"
  },
  {
    "objectID": "coursework/test/Introduction.html#reproducible-and-interactive",
    "href": "coursework/test/Introduction.html#reproducible-and-interactive",
    "title": "Modern Scientific Programming",
    "section": "Reproducible and interactive",
    "text": "Reproducible and interactive\n\nUse open data\nBeing able to reproduce the exact environment used for data processing and computations\nProvide interactive views of the results"
  },
  {
    "objectID": "coursework/test/Introduction.html#multilanguage",
    "href": "coursework/test/Introduction.html#multilanguage",
    "title": "Modern Scientific Programming",
    "section": "Multilanguage",
    "text": "Multilanguage\nSeveral programming languages, with different strengths - C/Fortran: low-level speed - Python: easy, readable, many scientific libraries, glue language - Julia: low-level speed, high-level features\ncan interact…\nLots of specialized language are useful too (for research) - html/javascript - vega/ggog - yaml/json - database (sql) - tensorflow (deep learning)"
  },
  {
    "objectID": "coursework/test/Introduction.html#multiplatforms",
    "href": "coursework/test/Introduction.html#multiplatforms",
    "title": "Modern Scientific Programming",
    "section": "Multiplatforms",
    "text": "Multiplatforms\nCalculations can (must be run) on a variety of different platforms:\n\nlaptop\nserver\nvirtual machine / container\nfarm of computers\nspecialized hardware (e.g. GPU, TPU)"
  },
  {
    "objectID": "coursework/test/Introduction.html#why-should-you-learn-to-program",
    "href": "coursework/test/Introduction.html#why-should-you-learn-to-program",
    "title": "Modern Scientific Programming",
    "section": "Why should you learn to program ?",
    "text": "Why should you learn to program ?\n\nEconometricians, modellers, data scientists, spend a lot of time writing code\n\nand do it inefficiently…\n\nProgramming efficiently requires awareness of\n\ncertain basic concepts: (types, control flow, functions, objects)\nsome tools (programming language, code versioning, command line)\nwhich are never taught…\n\nAnd yet they are very easy to learn\n\nanyone can become an expert !"
  },
  {
    "objectID": "coursework/test/Introduction.html#now-is-the-right-time",
    "href": "coursework/test/Introduction.html#now-is-the-right-time",
    "title": "Modern Scientific Programming",
    "section": "Now is the right time !",
    "text": "Now is the right time !\n\nA lot of demand everywhere for skilled programmers.\nMany faculties are introducing formal programming courses (for economists)\n\nMIT, NYU, Penn, …\nSummer Bootcamps\n\nNew applications are even more code-intensive than before:\n\ndata science\nmachine learning\nmodeling"
  },
  {
    "objectID": "coursework/test/Introduction.html#do-it-in-the-open-2",
    "href": "coursework/test/Introduction.html#do-it-in-the-open-2",
    "title": "Modern Scientific Programming",
    "section": "Do it in the open ! (2)",
    "text": "Do it in the open ! (2)\n\nMany excellent online resources:\n\nSoftware Carpentry\nQuantEcon from Tom Sargent and John Stachurski\n\nOpensource community is very welcoming:\n\nask on mailing lists or online chats (Julia users, quantecon, dynare, …)\nopen issues (for instance against Dolo (https://github.com/EconForge/Dolo.jl/issues)[https://github.com/EconForge/Dolo.jl/issues]\nparticipating is also a great occasion to learn"
  },
  {
    "objectID": "coursework/test/Introduction.html#mental-break-even-toddlers-can-learn-to-program",
    "href": "coursework/test/Introduction.html#mental-break-even-toddlers-can-learn-to-program",
    "title": "Modern Scientific Programming",
    "section": "Mental Break: even toddlers can learn to program",
    "text": "Mental Break: even toddlers can learn to program\nhttps://www.youtube.com/watch?v=4gN9nPB7LIw\nhttps://www.kickstarter.com/projects/primotoys/cubetto-hands-on-coding-for-girls-and-boys-aged-3"
  },
  {
    "objectID": "coursework/test/Introduction.html#for-next-time",
    "href": "coursework/test/Introduction.html#for-next-time",
    "title": "Modern Scientific Programming",
    "section": "For next time",
    "text": "For next time\n\nRegister on zulip if not done already.\nInstall Anaconda Python 3.7 (user-wide so that packages can be installed without admin rights"
  },
  {
    "objectID": "folder/Untitled.html",
    "href": "folder/Untitled.html",
    "title": "Data-Based Economics",
    "section": "",
    "text": "import pandas as pd\n\n\ntxt = \"\"\"year;country;measure\n2018;\"france\";950.0\n2019;\"france\";960.0\n2020;\"france\";1000.0\n2018;\"usa\";2500.0\n2019;\"usa\";2150.0\n2020;\"usa\";2300.0\n\"\"\"\nopen('annoying_dummy_file.csv','w').write(txt) # we wri\n\n136\n\n\n\ndf = pd.read_csv(\"annoying_dummy_file.csv\",sep=\";\")\n\n\ndf\n\n\n\n\n\n\n\n\nyear\ncountry\nmeasure\n\n\n\n\n0\n2018\nfrance\n950.0\n\n\n1\n2019\nfrance\n960.0\n\n\n2\n2020\nfrance\n1000.0\n\n\n3\n2018\nusa\n2500.0\n\n\n4\n2019\nusa\n2150.0\n\n\n5\n2020\nusa\n2300.0\n\n\n\n\n\n\n\n\n!pip install vega_datasets\n\nCollecting vega_datasets\n  Downloading vega_datasets-0.9.0-py3-none-any.whl (210 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 210.8/210.8 kB 5.5 MB/s eta 0:00:00[31m8.6 MB/s eta 0:00:01\nRequirement already satisfied: pandas in /home/pablo/.local/opt/mambaforge/lib/python3.10/site-packages (from vega_datasets) (1.4.4)\nRequirement already satisfied: python-dateutil&gt;=2.8.1 in /home/pablo/.local/opt/mambaforge/lib/python3.10/site-packages (from pandas-&gt;vega_datasets) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /home/pablo/.local/opt/mambaforge/lib/python3.10/site-packages (from pandas-&gt;vega_datasets) (2022.2.1)\nRequirement already satisfied: numpy&gt;=1.21.0 in /home/pablo/.local/opt/mambaforge/lib/python3.10/site-packages (from pandas-&gt;vega_datasets) (1.22.4)\nRequirement already satisfied: six&gt;=1.5 in /home/pablo/.local/opt/mambaforge/lib/python3.10/site-packages (from python-dateutil&gt;=2.8.1-&gt;pandas-&gt;vega_datasets) (1.16.0)\nInstalling collected packages: vega_datasets\nSuccessfully installed vega_datasets-0.9.0\n\n\n\nimport vega_datasets\ndf = vega_datasets.data('iris')\ndf\n\n\n\n\n\n\n\n\nsepalLength\nsepalWidth\npetalLength\npetalWidth\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n...\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\nvirginica\n\n\n146\n6.3\n2.5\n5.0\n1.9\nvirginica\n\n\n147\n6.5\n3.0\n5.2\n2.0\nvirginica\n\n\n148\n6.2\n3.4\n5.4\n2.3\nvirginica\n\n\n149\n5.9\n3.0\n5.1\n1.8\nvirginica\n\n\n\n\n150 rows × 5 columns\n\n\n\n\n!pip install dbnomics\n\nCollecting dbnomics\n  Downloading DBnomics-1.2.3-py3-none-any.whl (20 kB)\nRequirement already satisfied: requests&gt;=2.18.4 in /home/pablo/.local/opt/mambaforge/lib/python3.10/site-packages (from dbnomics) (2.28.1)\nRequirement already satisfied: pandas&gt;=0.21 in /home/pablo/.local/opt/mambaforge/lib/python3.10/site-packages (from dbnomics) (1.4.4)\nRequirement already satisfied: python-dateutil&gt;=2.8.1 in /home/pablo/.local/opt/mambaforge/lib/python3.10/site-packages (from pandas&gt;=0.21-&gt;dbnomics) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /home/pablo/.local/opt/mambaforge/lib/python3.10/site-packages (from pandas&gt;=0.21-&gt;dbnomics) (2022.2.1)\nRequirement already satisfied: numpy&gt;=1.21.0 in /home/pablo/.local/opt/mambaforge/lib/python3.10/site-packages (from pandas&gt;=0.21-&gt;dbnomics) (1.22.4)\nRequirement already satisfied: charset-normalizer&lt;3,&gt;=2 in /home/pablo/.local/opt/mambaforge/lib/python3.10/site-packages (from requests&gt;=2.18.4-&gt;dbnomics) (2.1.1)\nRequirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /home/pablo/.local/opt/mambaforge/lib/python3.10/site-packages (from requests&gt;=2.18.4-&gt;dbnomics) (1.26.11)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /home/pablo/.local/opt/mambaforge/lib/python3.10/site-packages (from requests&gt;=2.18.4-&gt;dbnomics) (3.3)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /home/pablo/.local/opt/mambaforge/lib/python3.10/site-packages (from requests&gt;=2.18.4-&gt;dbnomics) (2022.9.24)\nRequirement already satisfied: six&gt;=1.5 in /home/pablo/.local/opt/mambaforge/lib/python3.10/site-packages (from python-dateutil&gt;=2.8.1-&gt;pandas&gt;=0.21-&gt;dbnomics) (1.16.0)\nInstalling collected packages: dbnomics\nSuccessfully installed dbnomics-1.2.3\n\n\n\nimport dbnomics\n\n\ndf = dbnomics.fetch_series(\"OECD/MEI/FRA.CPALTT01.CTGY.M\")\n\n\ndf['value'].plot()\n\n&lt;AxesSubplot: &gt;"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data-Based Economics (2022-2023)",
    "section": "",
    "text": "Session\nDate\nContent\nTutorial\nHomework\n\n\n\n\n1\n18/01\nintro\npython essentials\npush-ups (1)\n\n\n2\n25/01\ndataframes, descriptive statistics\npython essentials (2)\npush-ups (2)\n\n\n3\n01/02\nlinear regression\ndata exploration, visualization\npush-ups (3)\n\n\n4\n08/02\nmultiple regression\n\n\n\n\n5\n15/02\nmultiple regression\n\n\n\n\n6\n22/02\ninstrumental variables\n\ncoursework (1)\n\n\n7\n01/02\nmachine learning introduction (1)\n\n\n\n\n8\n21/03\nmachine learning introduction (2)\n\n\n\n\n9\n28/03\ntext analysis\n\n\n\n\n10\n04/04\nfinal exam"
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Data-Based Economics (2022-2023)",
    "section": "",
    "text": "Session\nDate\nContent\nTutorial\nHomework\n\n\n\n\n1\n18/01\nintro\npython essentials\npush-ups (1)\n\n\n2\n25/01\ndataframes, descriptive statistics\npython essentials (2)\npush-ups (2)\n\n\n3\n01/02\nlinear regression\ndata exploration, visualization\npush-ups (3)\n\n\n4\n08/02\nmultiple regression\n\n\n\n\n5\n15/02\nmultiple regression\n\n\n\n\n6\n22/02\ninstrumental variables\n\ncoursework (1)\n\n\n7\n01/02\nmachine learning introduction (1)\n\n\n\n\n8\n21/03\nmachine learning introduction (2)\n\n\n\n\n9\n28/03\ntext analysis\n\n\n\n\n10\n04/04\nfinal exam"
  },
  {
    "objectID": "index.html#about-pushups",
    "href": "index.html#about-pushups",
    "title": "Data-Based Economics (2022-2023)",
    "section": "About pushups",
    "text": "About pushups\n\nPushups\n\nindividual\nto do after every course, before next session\nyou are encouraged to collaborate\nsend via Nuvolos platform"
  },
  {
    "objectID": "index.html#evaluation-final-examination",
    "href": "index.html#evaluation-final-examination",
    "title": "Data-Based Economics (2022-2023)",
    "section": "Evaluation & Final Examination",
    "text": "Evaluation & Final Examination\n\nData Projects (x2) (50%)\n\ngroupwork (goups smaller than 4)\n\nimport some data\nperform/replicate some econometric work\npresent results with nice plots\n\n\nFinal Exam (50%)\n\nindividual\n90 min during last session\ntest general knowledge of econometrics / machine learning\nthere will be some programming tasks\n\n\n\nWork Environment\n\nWe will use Nuvolos\n\nonline datascience platform with Jupyterlab\n\nYou should have received an invitation by then."
  },
  {
    "objectID": "session_1/Exercises.html",
    "href": "session_1/Exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "Exercise 1\nWhat do you think the value of z is after running the code below?\n\nz = 3\nz = z + 4\nprint(\"z is\", z)\n\nz is 7\n\n\n(back to text)\nExercise 2\nRead about out what the len function does (by writing len?).\nWhat will it produce if we give it the variable x?\nCheck whether you were right by running the code len(x).\n\nlen?\n\n\nSignature: len(obj, /)\nDocstring: Return the number of items in a container.\nType:      builtin_function_or_method\n\n\n\n\n\nx = \"abe\"\nlen(x)\n\n3\n\n\n\nx = [1,2,3,4]\nlen(x)\n\n4\n\n\n(back to text)\nExercise 3\nWe can use our introspection skills to investigate a package’s contents.\nIn the cell below, use tab completion to find a function from the time module that will display the local time.\nUse time.FUNC_NAME? (where FUNC_NAME is replaced with the function you found) to see information about that function and then call the function. (Hint: look for something to do with the word local).\n\nimport time\ntime.localtime()\n\ntime.struct_time(tm_year=2021, tm_mon=1, tm_mday=19, tm_hour=9, tm_min=20, tm_sec=20, tm_wday=1, tm_yday=19, tm_isdst=0)\n\n\n\nimport time\n# your code here -- notice the comment!\ntime.time() / 3600\n\n447512.31643098034\n\n\n(back to text)\nExercise 4\nTry running import time as t in the cell below, then see if you can call the function you identified above.\nDoes it work?\n\ntime.localtime()\n\ntime.struct_time(tm_year=2021, tm_mon=1, tm_mday=19, tm_hour=9, tm_min=21, tm_sec=48, tm_wday=1, tm_yday=19, tm_isdst=0)\n\n\n\nimport time as t\n\n\nt.localtime()\n\ntime.struct_time(tm_year=2021, tm_mon=1, tm_mday=19, tm_hour=9, tm_min=22, tm_sec=4, tm_wday=1, tm_yday=19, tm_isdst=0)\n\n\n(back to text)\nExercise 5\nCreate the following variables:\n\nD: A floating point number with the value 10,000\n\nr: A floating point number with value 0.025\n\nT: An integer with value 30\n\nWe will use them in a later exercise.\n\n# D = float(10000) // to comment a line : Ctrl+/\nD = 10000.0\nr = 0.025\nT = 30\n\n(back to text)\nExercise 6\nRemember the variables we created earlier?\nLet’s compute the present discounted value of a payment ($ D $) made in $ T $ years assuming an interest rate of 2.5%. Save this value to a new variable called PDV and print your output.\nHint: The formula is\n\\[\n\\text{PDV} = \\frac{D}{(1 + r)^T}\n\\]\n\nPDV = D / (1+r)**T\nPDV\n\n4767.426851809713\n\n\n(back to text)\nExercise 7\nVerify the “trick” where the percent difference ($ \\() between two numbers close to 1 can be well approximated by the difference between the log of the two numbers (\\) (x) - (y) $).\nUse the numbers x and y below. (Hint: you will want to use the math.log function)\n\nx = 1.05\ny = 1.02\nA = (x-y)/x\nimport math\nB = math.log(x) - math.log(y)\nprint(f\"A: {A} | B: {B}\")\n\nA: 0.028571428571428595 | B: 0.02898753687325232\n\n\n\nfrom math import log\nlog(x)\n\n0.04879016416943205\n\n\n(back to text)\nExercise 8\nThe code below is invalid Python code\n\nx = \"What's wrong with this string\"\n\n\nx = 'What\\'s wrong with this string'\nx\n\n\"What's wrong with this string\"\n\n\nCan you fix it?\nHint: Try creating a code cell below and testing things out until you find a solution.\n(back to text)\nExercise 9\nUsing the variables x and y, how could you create the sentence Hello World?\nHint: Think about how to represent a space as a string.\n(back to text)\n\nx = \"Hello\"\ny = \"World\"\n\n\nx + \" \" + y\n\n'Hello World'\n\n\n\nf\"{x} {y}\" \n\n'Hello World'\n\n\nExercise 10\nOne of our favorite (and most frequently used) string methods is replace.\nIt substitutes all occurrences of a particular pattern with a different pattern.\nFor the variable test below, use the replace method to change the c to a d.\nHint: Type test.replace? to get some help for how to use the method replace.\n\ntest = \"abcabcabc\"\n\n\ntest.replace?\n\n\nSignature: test.replace(old, new, count=-1, /)\nDocstring:\nReturn a copy with all occurrences of substring old replaced by new.\n  count\n    Maximum number of occurrences to replace.\n    -1 (the default value) means replace all occurrences.\nIf the optional argument count is given, only the first count occurrences are\nreplaced.\nType:      builtin_function_or_method\n\n\n\n\n\ntest.replace(\"c\", \"d\")\n\n'abdabdabd'\n\n\n\ntest[2] = 'd'\n\nTypeError: 'str' object does not support item assignment\n\n\n\ntest2 = test.replace(\"c\", \"d\")\n\n\ntest\n\n'abdabdabd'\n\n\n\ntest2\n\n'abdabdabd'\n\n\n(back to text)\nExercise 11\nSuppose you are working with price data and encounter the value \"\\$6.50\".\nWe recognize this as being a number representing the quantity “six dollars and fifty cents.”\nHowever, Python interprets the value as the string \"\\$6.50\". (Quiz: why is this a problem? Think about the examples above.)\nIn this exercise, your task is to convert the variable price below into a number.\nHint: Once the string is in a suitable format, you can call write float(clean_price) to make it a number.\nprice = \"$6.50\"\n(back to text)\nExercise 12\nLookup a country in World Bank database, and format a string showing the growth rate of GDP over the last 2 years.\n(back to text)\nExercise 13\nInstead of hard-coding the values above, try to use the country, GDP and year variables you previously defined.\n(back to text)\nExercise 14\nCreate a new string and use formatting to produce each of the following statements\n\n“The 1st quarter revenue was 110M”\n\n“The 2nd quarter revenue was 95M”\n\n“The 3rd quarter revenue was 100M”\n\n“The 4th quarter revenue was 130M”\n\n(back to text)\nExercise 15\nWithout typing the commands, determine whether the following statements are true or false.\nOnce you have evaluated whether the command is True or False, run the code in Python.\nx = 2\ny = 2\nz = 4\n\n# Statement 1\nx &gt; z\n\n# Statement 1\nx == y\n\n# Statement 3\n(x &lt; y) and (x &gt; y)\n\n# Statement 4\n(x &lt; y) or (x &gt; y)\n\n# Statement 5\n(x &lt;= y) and (x &gt;= y)\n\n# Statement 6\nTrue and ((x &lt; z) or (x &lt; y))\n\n# code here!\n\n(back to text)\nExercise 16\nFor each of the code cells below, think carefully about what you expect to be returned before evaluating the cell.\nThen evaluate the cell to check your intuitions.\nNOTE: For now, do not worry about what the [ and ] mean – they allow us to create lists which we will learn about in an upcoming lecture.\n\nall([True, True, True])\n\n\nall([False, True, False])\n\n\nall([False, False, False])\n\n\nany([True, True, True])\n\n\nany([False, True, False])\n\n\nany([False, False, False])\n\n(back to text)"
  },
  {
    "objectID": "session_1/slides.html#so-what-will-we-do",
    "href": "session_1/slides.html#so-what-will-we-do",
    "title": "Introduction",
    "section": "So what will we do ?",
    "text": "So what will we do ?\n\n\nProgramming\nEconometrics / Machine Learning\nTalk about economics"
  },
  {
    "objectID": "session_1/slides.html#econometrics",
    "href": "session_1/slides.html#econometrics",
    "title": "Introduction",
    "section": "Econometrics",
    "text": "Econometrics"
  },
  {
    "objectID": "session_1/slides.html#programming",
    "href": "session_1/slides.html#programming",
    "title": "Introduction",
    "section": "Programming",
    "text": "Programming"
  },
  {
    "objectID": "session_1/slides.html#why-should-you-learn-programming",
    "href": "session_1/slides.html#why-should-you-learn-programming",
    "title": "Introduction",
    "section": "Why Should you learn programming ?",
    "text": "Why Should you learn programming ?\n\n\nResearchers (econometricians or data scientists) spend 80% of their time writing code.\n\nPresentation (plots, interactive apps) is key and relies on\n\n… programming\n\n\nInteraction with code becomes unavoidable in business environment\nfixing the website\nquerying the database\n…\n\nWorth investing a bit of time to learn it\n\nyou can easily become an expert\n\n\nPlus it’s fun"
  },
  {
    "objectID": "session_1/slides.html#programming-1",
    "href": "session_1/slides.html#programming-1",
    "title": "Introduction",
    "section": "Programming",
    "text": "Programming"
  },
  {
    "objectID": "session_1/slides.html#how-good-should-you-program",
    "href": "session_1/slides.html#how-good-should-you-program",
    "title": "Introduction",
    "section": "How good should you program ?",
    "text": "How good should you program ?\n\n\n\n&lt;div class=\"fragment current-visible\" data-fragment-index=1&gt;   &lt;img src=\"anxious.jpg\"&gt; &lt;/div&gt;\n&lt;div class=\"fragment current-visible\" data-fragment-index=2&gt;   &lt;img src=\"furious.webp\"&gt;  &lt;/div&gt;\n&lt;div class=\"fragment current-visible\" data-fragment-index=3&gt;  &lt;img src=\"relieved.jpg\"&gt; &lt;/div&gt;\n&lt;div class=\"fragment current-visible\" data-fragment-index=4&gt;  &lt;img src=\"googleit.avif\"&gt;  &lt;/div&gt;\n&lt;div class=\"fragment current-visible\" data-fragment-index=5&gt;  &lt;img src=\"happy.jpeg\" width=600&gt;  &lt;/div&gt;\n\n\n\n\n\nWe will “assume” everybody as some prior experience with Python\n\nEven though some of you have possibly never touched it\n\nWe’ll do some catchup today\n\nAnd count on you to find the resources to learn what you need when you need it\n\nOf course you can always ask questions"
  },
  {
    "objectID": "session_1/slides.html#additional-resources",
    "href": "session_1/slides.html#additional-resources",
    "title": "Introduction",
    "section": "Additional resources",
    "text": "Additional resources\nPlenty of online resources to learn python/econometrics/machine learning\n\nlearnpython sponsored by datacamp\nquantecon: designed for economists, good examples of projects\nPython Data Science Handbook: by Jake Van der Plas, very complete. Online free version.\nIntroduction to Econometrics with R, in R but very clear (beginner and advanced versions)"
  },
  {
    "objectID": "session_1/slides.html#quantecon",
    "href": "session_1/slides.html#quantecon",
    "title": "Introduction",
    "section": "Quantecon",
    "text": "Quantecon\n\n\n\n\n&lt;div class=\"col\" &gt;\n&lt;img src=\"tom_sargent.jpg\" &gt;&lt;br&gt;\nTom Sargent\n&lt;/div&gt;\n&lt;div class=\"col\"&gt;\n&lt;img src=\"john_stachurski.jpg\" width=50%&gt;&lt;br&gt;\nJohn Stachurski\n&lt;/div&gt;\n\n\n\nTom Sargent @ ESCP &lt;br&gt;\n&lt;img  src=\"t3m_escp.jpg\"&gt;\n\n\n\n\nQuantecon: free online lectures to learn python programming and (advanced) economics\n\nnow with a section on datascience\nit is excellent!\nwe will use some of it today"
  },
  {
    "objectID": "session_1/index.html#so-what-will-we-do",
    "href": "session_1/index.html#so-what-will-we-do",
    "title": "Introduction",
    "section": "So what will we do ?",
    "text": "So what will we do ?\n\n\nProgramming\nEconometrics / Machine Learning\nTalk about economics\n\n\n\nData-based economics (1)\n\nMost economists use data all the time\n\nto illustrate facts\nto test theories\n\n\n\n\n\nData-based economics\nIn practice these are the tasks an economist needs to perform:\n\n\nimport data\n\nclean the data\n\ndeal with heterogenous sources, missing data, abnormal observerations\nsuper time consuming\nwe’ll make this part easy for you\n\n\ndescribe the data (statistics), visualize it\n\ninterpret it using a model\n\npresent results"
  },
  {
    "objectID": "session_1/index.html#econometrics",
    "href": "session_1/index.html#econometrics",
    "title": "Introduction",
    "section": "Econometrics",
    "text": "Econometrics\n\n\nEconometricks\n\nAn art invented by economists: \\[\\underbrace{y}\\_{\\text{dependent variable}} = a \\underbrace{x}\\_{\\text{explanatory variable}} + b\\]\n\nMain challenge:\n\ngiven dataset \\((x_i, y_i)\\)\nfind \\(a\\) while controlling for \\(b\\)\nunderstand robustness of results\npredict new values of \\(y\\) for new values of \\(x\\)\n\n\nExample 1:\n\nHow happy are you?\nWhat is \\(x\\) ? What is \\(y\\) ?\n\n\n\n\n\nEconometricks (2)\n\\[\\underbrace{y}\\_{\\text{dependent variable}} = a \\underbrace{x}\\_{\\text{explanatory variable}} + b\\]\n\n\nExample 2:\n\nyoung men who go to war receive in average lower wages when they return than men who didn’t go to war\n\n… is it because they skipped college?\n\n… or did they choose to go to war because they were less skilled for college?\n\nfind a way to extract causality\n\n-&gt; instrumental variables\n\n\nThis was worth a Nobel Prize! (D. Card, J. Angrist, G.W. Imbens)\n\n\n\n\nEconometricks (3)\n\\[\\underbrace{y}\\_{\\text{dependent variable}} = a \\underbrace{x}\\_{\\text{explanatory variable}} + b\\]\n\n\nExample 3:\n\nI want to establish a link between growth and trade openness\n\nbut my country has only 10 years of historical data… (within dimension)\n\n… let’s take 50 countries at the same time (between dimension)\n\nfind a way to analyze both dimensions at the same time\n\n-&gt; panel data\n\n\n\n\n\nBig Data Era and Machine Learning\n\nData has become very abundant\n\nLarge amounts of data of all kinds\n\nstructured (tables, …)\nunstructured (text, images, …)\n\n\nMachine learning:\n\na set of powerful algorithms…\n… so powerful some call it artificial intelligence\n\nthey learn by processing data\n\n… to extract information and relations in large data sets\n…\n\n\nComparison with econometrics\n\nML has it own, partially redundant, jargon\nmuch harder to understand causality, standard deviation (i.e. precision)\n\n\n\n\n\nMachine Learning\n\\[\\underbrace{y}\\_{\\text{predicted variable}} = f( \\underbrace{x}\\_{\\text{feature}} , a)\\]\n\nChallenge:\n\ngiven dataset \\((x_i, y_i)\\)\nfind \\(a\\), that is find a nonlinear relationship between \\(a\\) and \\(b\\)\npredict new values of \\(y\\) given new values of \\(x\\)\n\nWhat are the difference with econometrics?\n\n\n\n\nBig Data Era and Machine Learning (1)\n\nSentiment analysis: predict population’s optimism by analyzing tweets.\nCheck sentiment viz\n\n\n\nBig Data Era and Machine Learning (2)\n\nDeep learning: artificial neural nets"
  },
  {
    "objectID": "session_1/index.html#programming",
    "href": "session_1/index.html#programming",
    "title": "Introduction",
    "section": "Programming",
    "text": "Programming\n\n\nWhy program in Python?\n\nWhy learn and use Python?\nAnd not \n\nR\nSPSS\nStata\nMatlab\nC\nJavascript\nSQL\n…\n\n\n\n\n\nBecause\nPython is:\n\n\n&lt;img src=\"python_kids.jpg\" width=800&gt;&lt;br&gt;Easy\n\n\n&lt;img src=\"download_python.png\" width=1200&gt;&lt;br&gt;Free\n\n\n\nPopular (TIOBE Index) - lively community - lots of online ressources - libraries for virtually anything\n\n\n   \n\nThe lingua Franca of Machine learning\n\nAll major machine learning softwares are written or interface with Python"
  },
  {
    "objectID": "session_1/index.html#why-should-you-learn-programming",
    "href": "session_1/index.html#why-should-you-learn-programming",
    "title": "Introduction",
    "section": "Why Should you learn programming ?",
    "text": "Why Should you learn programming ?\n\n\nResearchers (econometricians or data scientists) spend 80% of their time writing code.\n\nPresentation (plots, interactive apps) is key and relies on\n\n… programming\n\n\nInteraction with code becomes unavoidable in business environment\nfixing the website\nquerying the database\n…\n\nWorth investing a bit of time to learn it\n\nyou can easily become an expert\n\n\nPlus it’s fun"
  },
  {
    "objectID": "session_1/index.html#programming-1",
    "href": "session_1/index.html#programming-1",
    "title": "Introduction",
    "section": "Programming",
    "text": "Programming"
  },
  {
    "objectID": "session_1/index.html#how-good-should-you-program",
    "href": "session_1/index.html#how-good-should-you-program",
    "title": "Introduction",
    "section": "How good should you program ?",
    "text": "How good should you program ?\n\n\n\n&lt;div class=\"fragment current-visible\" data-fragment-index=1&gt;   &lt;img src=\"anxious.jpg\"&gt; &lt;/div&gt;\n&lt;div class=\"fragment current-visible\" data-fragment-index=2&gt;   &lt;img src=\"furious.webp\"&gt;  &lt;/div&gt;\n&lt;div class=\"fragment current-visible\" data-fragment-index=3&gt;  &lt;img src=\"relieved.jpg\"&gt; &lt;/div&gt;\n&lt;div class=\"fragment current-visible\" data-fragment-index=4&gt;  &lt;img src=\"googleit.avif\"&gt;  &lt;/div&gt;\n&lt;div class=\"fragment current-visible\" data-fragment-index=5&gt;  &lt;img src=\"happy.jpeg\" width=600&gt;  &lt;/div&gt;\n\n\n\n\n\nWe will “assume” everybody as some prior experience with Python\n\nEven though some of you have possibly never touched it\n\nWe’ll do some catchup today\n\nAnd count on you to find the resources to learn what you need when you need it\n\nOf course you can always ask questions"
  },
  {
    "objectID": "session_1/index.html#additional-resources",
    "href": "session_1/index.html#additional-resources",
    "title": "Introduction",
    "section": "Additional resources",
    "text": "Additional resources\nPlenty of online resources to learn python/econometrics/machine learning\n\nlearnpython sponsored by datacamp\nquantecon: designed for economists, good examples of projects\nPython Data Science Handbook: by Jake Van der Plas, very complete. Online free version.\nIntroduction to Econometrics with R, in R but very clear (beginner and advanced versions)"
  },
  {
    "objectID": "session_1/index.html#quantecon",
    "href": "session_1/index.html#quantecon",
    "title": "Introduction",
    "section": "Quantecon",
    "text": "Quantecon\n\n\n\n\n&lt;div class=\"col\" &gt;\n&lt;img src=\"tom_sargent.jpg\" &gt;&lt;br&gt;\nTom Sargent\n&lt;/div&gt;\n&lt;div class=\"col\"&gt;\n&lt;img src=\"john_stachurski.jpg\" width=50%&gt;&lt;br&gt;\nJohn Stachurski\n&lt;/div&gt;\n\n\n\nTom Sargent @ ESCP &lt;br&gt;\n&lt;img  src=\"t3m_escp.jpg\"&gt;\n\n\n\n\nQuantecon: free online lectures to learn python programming and (advanced) economics\n\nnow with a section on datascience\nit is excellent!\nwe will use some of it today"
  },
  {
    "objectID": "session_1/Python_basics.html#set-up",
    "href": "session_1/Python_basics.html#set-up",
    "title": "Introduction to Python",
    "section": "Set-up",
    "text": "Set-up\nTo run this tutorial you need a version of Python greater than 3.6.\nIf you use your own computer, I recommend downloading Anaconda Python https://www.anaconda.com/distribution/. Anaconda is a package manager, which allows to install many python libraries as well as a few useful binary dependencies which are complicated to install otherwise. When installing anaconda make sure you install it in user space, so that you can later install and update libraries without admin rights.\n\nJupyter Notebook\nJupyter Notebook have become the standard for literate programming (i.e. mix code and program). They need a special server to be run. You can run one locally (Jupyter Lab) or use an online one (Colab). If you use VSCode and open a notebook it will launch a server for you.\nOnce a notebook is open, you can execute a cell and move to the next one by pressing shift+Enter. Results are printed below each cell.\n\n\nIntepreter / Python script\nType python in a console or terminal. You get an interactive prompt, where you can evaluate any expression. If it is installed, you can use a nicer, interactive prompt: ipython.\nIt is possible to run python programs in a non-interactive way from the command line. Just type python followed by the file (ending in .py) that you want to run.\nPython scripts can be edited using any text editor. If you don’t have strong views, I recommend Visual Studio Code."
  },
  {
    "objectID": "session_1/Python_basics.html#basic-types",
    "href": "session_1/Python_basics.html#basic-types",
    "title": "Introduction to Python",
    "section": "Basic Types",
    "text": "Basic Types\n\nnumbers\n\n# integers and floats\n\n\n1\n\n\n1.0\n\n\n# conversion with int() and float()\n\n\n# floor: int()\n\n\n# no difference between various types of floats (16 bits, 32 bits, 64 bits, ...)\n\n\n# usual operations + - *\nprint( 2 + 3 )\nprint( 9 - 6 )\nprint( 3 / 2 )\nprint(2304310958 * 41324)\n\n\n# divisions / and //\nprint(3 / 4)\nprint(13 // 4)\n\n\n# exponentiation ** (not ^!)\n# (1.04)^10\n(1.04)**10\n\n\n# comparison operators: &gt;, &lt;, &gt;=, &lt;=, ==\n\nprint((1.0453432)*(0.96)  &gt; 1.001 )\n\nprint(1.001 &gt;= 1.001)\n\n\n# comparison operators can be chained:\nprint(0.2&lt;0.4&lt;0.5)\nprint(0.5&lt;=0.4&lt;=0.5) # equivalent to ((0.5&lt;=0.4) and(0.4&lt;=0.5))\n\n\n\nspecial types: boolean and None\nThere are only two booleans: True and False (note uppercase). None is a dummy type, which is used when no other type fits.\n\nprint( False )\nTrue\n\n\n(True, False, None)\n\n\nNone\n\nDouble equal sign tests for equality. Result should always be a boolean.\n\nTrue==False\n\nLogical operators are not, and and or:\n\n(True or False)\n\n\nnot (True or False)\n\n\n(1.3**1.04 &gt; 1.9) | (1000**1143&gt;1001**1142)\n\nOperators or and and can be replaced by | and & respectively. They are non-greedy, that is terms are not evaluated if the result of the comparison is already known.\n\nFalse and (print(\"Hello\"))\n\n\nprint( (print(\"Hello\")) and False )\n\n\n\nstrings\n\ndefinition\nStrings are defined by enclosing characters either by ' (single quotes) or \" (double quote). Single quotes strings can contain double quotes strings and vice-versa.\n\n\"name\"\n\n\n'name'\n\n\n'I say \"hello\"'\n\n\n\"You can 'quote' me\"\n\nStrings spanning over sever lines can be defined with triple quotes (single or double).\n\ns = \"\"\"¿Qué es la vida? Un frenesí.\n¿Qué es la vida? Una ilusión,\nuna sombra, una ficción,\ny el mayor bien es pequeño;\nque toda la vida es sueño,\ny los sueños, sueños son.\n\"\"\"\n\nIt is also possible to use the newline character \\n.\n\n\"La vida es sueño,\\ny los sueños, sueños son.\"\n\n\nprint(\"La vida es sueño,\\ny los sueños, sueños son.\")\n\n\n\ncharacter sets\nStrings can contain any unicode character:\n\ns = \"🎻⽻༽\"\n\nRefresher: ASCII vs unicode\nASCII (or ASCII-US) is an old standard which codes a character with 7 bits (or 8 bits for extended ASCII). This allows to code 128 different characters (256 for ex-ASCII).\nOnly a subset of these characters can be printed regularly.\n\nchr(44)\n\n\n# ASCII: \nfor i in range(32,127):\n    print( chr(i), end=' ')\n\nThe other characters include delete, newline and carriage return among others.\n\ns = 'This is\\na\\nmultiline string.' # note the newline character '\\n'\n\n\n# print(s)\nlen(s)\n\nSome antiquated platforms still use newline + carriage return at the end of each line. This is absolutely not required and causes incompatibilities.\n\ns2 = 'This is\\n\\ra\\n\\rmultiline string.' # note the newline character '\\n' and carriager return '\\r'\n\n\nprint(s2)\nprint(len(s2))\n\nUnicode contains a repertoire of over 137,000 characters with all ASCII characters as subcases\nTo type: copy/paste, ctrl+shift+hexadecimal, latex + tab\n\n# example\nÆ\nγ\n\nÜ\n\nVariable names aka identifiers can contain unicode characters with some restrictions: - they cannot start with a digit - they can’t contain special variables (‘!,#,@,%,$’ and other unicode specials ???) - they can contain underscore\n\n\noperations\nconcatenation\n\n'abc' + 'def'\n\n\n'abc'*3\n\n\n'abc' + 'abc' + 'abc'\n\n\n\nsubstrings\n\n# strings can be accessed as arrays (0 based indexing)\ns = \"a b c\"\ns[0]\n\n\n# slice notation (  [min,max[ )\ns = \"a b c d\"\ns[2:5] # 0-based; 2 included, 5 excluded\n\n\n# substrings are easy to check\n\"a\" in s\n\n\n\"b c\" in \"a b c d\"\n\nIt is impossible to modify a substring.\n\n# but are immutable\ns = \"a b c\"\ns[1] = 0\n\nInstead, one can replace a substring:\n\ns\n\n\ns.replace(' ', '🎻')\n\nOr use string interpolation\n\n# string interpolation (old school)\n\"ny name is {name}\".format(name=\"nobody\")\n\n\n\"calculation took {time}s\".format(time=10000)\n\n\n# number format can be tweaked\n\"I am {age:.0f} years old\".format(age=5.65)\n\n\n# formatted strings\nelapsed = 15914884.300292\n\nf\"computations took {elapsed/3600:.2f} hours\"\n\n\nname = \"arnaldur\"\n\n\n\"dasnfnaksujhn {name}\".format(name=\"whatever\")\n\n\n# basic string operations: str.split, str.join, etc...\n# fast regular expressions\n# more on it, with text processing lesson\n\n\nstr.split(\"me,you,others,them\",',')\n\n\nstr.join( \" | \",\n    str.split(\"me,you,others,them\",','),\n)\n\n\n\nEscaping characters\nThe example above used several special characters: \\n which corresponds to only one ascii character and the curly brackets { and } which disappears after the string formatting. If one desires to print these characters precisely one needs to escape them using \\ and { }.\n\nprint(\"This is a one \\\\nline string\")\nprint(\"This string keeps some {{curly}} brackets{}\".format('.'))\n\n\n\nOther operations on strings\n(check help(str) or help?)\n\nlen() : length\nstrip() : removes characters at the ends\nsplit() : split strings into several substrings separated by separator\njoin() : opposite of split\n\n\n'others,'\n\n\n',me,others,'.strip(',')\n\n\ns.count(',')\n\n\nhelp(str)"
  },
  {
    "objectID": "session_1/Python_basics.html#assignment",
    "href": "session_1/Python_basics.html#assignment",
    "title": "Introduction to Python",
    "section": "Assignment",
    "text": "Assignment\nAny object can be reused by assigning an identifier to it. This is done with assignment operator =.\n\na = 3\na\n\nNote that assignment operator = is different from comparison operator ==. Comparison operator is always True or False, while assignment operator has no value.\n\n(2==2) == True\n\n\n(a=2) == True"
  },
  {
    "objectID": "session_1/Python_basics.html#containers",
    "href": "session_1/Python_basics.html#containers",
    "title": "Introduction to Python",
    "section": "Containers",
    "text": "Containers\nAny object created in Python is identified by a unique id. One can think of it approximately as its reference. Object collections, contain arbitrary other python objects, that is they contain references to them.\n\nid(s)\n\n\ntuples\n\nconstruction\n\n(1,2,\"a\" )\n\nSince tuples are immutable, two identical tuples, will always contain the same data.\n\nt1  = (2,23)\nt2  = (2,23)\n\n\n# can contain any data\nt = (1,2,3,4,5,6)\nt1 = (t, \"a\", (1,2))\nt2 = (0,)  # note trailing coma for one element tuple\nt3 = (t, \"a\", (1,2))\n\n\nt[0] = 78\n\nSince tuples never change, they can be compared by hash values (if the data they hold can be hashed). Two tuples are identical if they contain the same data.\nRemark: hash function is any function that can be used to map data of arbitrary size to data of a fixed size. It is such that the probability of two data points of having the same hash is very small even if they are close to each other.\n\nt3 == t1\n\n\nprint(hash(t3))\nprint(hash(t1))\n\n\nid(t3), id(t1)\n\n\n\naccess elements\n\n# elements are accessed with brackets (0-based)\nt[0]\n\n\n# slice notation works too (  [min,max[ )\nt[1:3]\n\n\n# repeat with *\n(3,2)*5\n\n\n(0)*5\n\n\n(0,)*5\n\n\nt2*5\n\n\n# concatenate with +\nt+t1+t2\n\n\n# test for membership\n\n(1 in t)\n\n\n\n\nlists\nlists are enclosed by brackets are mutable ordered collections of elements\n\nl = [1,\"a\",4,5]\n\n\nl[1]\n\n\nl[1:] # if we omit the upper-bound it goes until the last element\n\n\nl[:2]\n\n\n# lists are concatenated with +\nl[:2] + l[2:] == l\n\n\n# test for membership\n(5 in l)\n\n\n# lists can be extended inplace\nll = [1,2,3]\nll.extend([4,5]) # several elements\nll.append(6)\nll\n\nSince lists are mutable, it makes no sense to compute them by hash value (or the hash needs to be recomputed every time the values change).\n\nhash(ll)\n\nSorted lists can be created with sorted (if elements can be ranked)\n\nll = [4,3,5]\n\n\nsorted(ll)\n\n\nll\n\nIt is also possible to sort in place.\n\nll.sort()\nll\n\n\nsorted(ll) # creates a new list\nll.sort()  # does it in place\n\n\n# in python internals:    ll.sort() equivalent sort(ll)\n\n\n\nset\nSets are unordered collections of unique elements.\n\ns1 = set([1,2,3,3,4,3,4])\ns2 = set([3,4,4,6,8])\nprint(s1, s2)\nprint(s1.intersection(s2))\n\n\n{3,4} == {4,3}\n\n\n\ndictionaries\nDictionaries are ordered associative collections of elements. They store values associated to keys.\n\n# construction with curly brackets\nd = {'a':0, 'b':1}\n\n\nd\n\n\n# values can be recovered by indexing the dict with a key\nd['b']\n\n\nd = dict()\n# d['a'] = 42\n# d['b'] = 78\nd\n\n\nd['a'] = 42\n\n\nd['b']\n\nKeys can be any hashable value:\n\nd[('a','b')] = 100\n\n\nd[ ['a','b'] ] = 100 # that won't work\n\nNote: until python 3.5 dictionaries were not ordered. Now the are guaranteed to keep the insertion order"
  },
  {
    "objectID": "session_1/Python_basics.html#control-flows",
    "href": "session_1/Python_basics.html#control-flows",
    "title": "Introduction to Python",
    "section": "Control flows",
    "text": "Control flows\n\nConditional blocks\nConditional blocks are preceeded by if and followed by an indented block. Note that it is advised to indent a block by a fixed set of space (usually 4) rather than use tabs.\n\nif 'sun'&gt;'moon':\n    print('warm')\n\nThey can also be followed by elif and else statements:\n\nx = 0.5\nif (x&lt;0):\n    y = 0.0\nelif (x&lt;1.0):\n    y = x\nelse:\n    y = 1+(x-1)*0.5\n\nRemark that in the conditions, any variable can be used. The following evaluate to False: - 0 - empty collection\n\nif 0: print(\"I won't print this.\")\nif 1: print(\"Maybe I will.\")\nif {}: print(\"Sir, your dictionary is empty\")\nif \"\": print(\"Sir, there is no string to speak of.\")\n\n\n\nWhile\nThe content of the while loop is repeated as long as a certain condition is met. Don’t forget to change that condition or the loop might run forever.\n\npoint_made = False\ni = 0\nwhile not point_made:\n    print(\"A fanatic is one who can't change his mind and won't change the subject.\")\n    i += 1 # this is a quasi-synonym of i = i + 1\n    if i&gt;=20:\n          point_made = True\n\n\n\nLoops\n\n# while loops\ni = 0\nwhile i&lt;=10:\n    print(str(i)+\" \",  end='')\n    i+=1\n\n\n# for loop\nfor i in [0,1,2,3,4,5,6,7,8,9,10]:\n    print(str(i)+\" \",  end='')\n\n\n# this works for any kind of iterable\n# for loop\nfor i in (0,1,2,3,4,5,6,7,8,9,10):\n    print(str(i)+\" \",  end='')\n\n\n# including range generator (note last value)\nfor i in range(11): \n    print(str(i)+\" \",  end='')\n\n\nrange(11)\n\n\n# one can also enumerate elements\ncountries = (\"france\", \"uk\", \"germany\")\nfor i,c in enumerate(countries): \n    print(f\"{i}: {c}\")\n\n\ns = set(c)\n\n\n# conditional blocks are constructed with if, elif, else\nfor i,c in enumerate(countries):\n    if len(set(c).intersection(set(\"brexit\"))):\n        print(c)\n    else:\n        print(c + \" 😢\")\n\nIt is possible to iterate over any iterable. This is true for a list or a generator:\n\nfor i in range(10): # range(10) is a generator\n    print(i)\n\n\nfor i in [0,1,2,3,4,5,6,7,8,9]:\n    print(i)\n\nWe can iterate of dictionary keys or values\n\nd = {1:2, 3:'i'}\nfor k in d.keys():\n    print(k, d[k])\nfor k in d.values():\n    print(k)\n\nor both at the same time:\n\nfor t in d.items():\n    print(t)\n\n# look at automatic unpacking\nfor (k,v) in d.items():\n    print(f\"key: {k}, value: {v}\")\n\n\n\nComprehension and generators\nThere is an easy syntax to construct lists/tuples/dicts: comprehension. Syntax is remminiscent of a for loop.\n\n[i**2 for i in range(10)]\n\n\nset(i-(i//2*2) for i in range(10))\n\n\n{i: i**2 for i in range(10)}\n\nComprehension can be combined with conditions:\n\n[i**2 for i in range(10) if i//3&gt;2]\n\nBehind the comprehension syntax, there is a special object called generator. Its role is to supply objects one by one like any other iterable.\n\n# note the bracket\ngen = (i**2 for i in range(10))\ngen # does nothing\n\n\ngen = (i**2 for i in range(10))\nfor e in gen:\n    print(e)\n\n\ngen = (i**2 for i in range(10))\nprint([e for e in gen])\n\nThere is a shortcut to converte a generator into a list: it’s called unpacking:\n\ngen = (i**2 for i in range(10))\n[*gen]"
  },
  {
    "objectID": "session_1/Python_basics.html#functions",
    "href": "session_1/Python_basics.html#functions",
    "title": "Introduction to Python",
    "section": "Functions",
    "text": "Functions\nWrong approach\n\na1 = 34\nb1 = (1+a1*a1)\nc1 = (a1+b1*b1)\n\na2 = 36\nb2 = (1+a2*a2)\nc2 = (a2+b2*b2)\n\nprint(c1,c2)\n\nBetter approach\n\ndef calc(a):\n    b = 1+a*a\n    c = a+b*b\n    return c\n\n(calc(34), calc(36))\n\nit is equivalent to replace the content of the function by:\n\na = 32\n_a = a          # def calc(a):\n_b = 1+_a*_a    #    b = 1+a*a\n_c = _a+_b*_b   #    c = a+b*b\nres = _c        #    return c\n\nNote that variable names within the function have different names. This is to avoid name conflicts as in:\n\ny = 1\ndef f(x):\n    y = x**2\n    return y+1\ndef g(x):\n    y = x**2+0.1\n    return y+1\nr1 = f(1.4)\nr2 = g(1.4)\nr3 = y\n(r1,r2,r3)\n\n\nl = ['france', 'germany']\ndef fun(i):\n    print(f\"Country: {l[i]}\")\nfun(0)\n\n\nl = ['france', 'germany']\ndef fun(i):\n    l = ['usa', 'japan']\n    l.append('spain')\n    print(f\"Country: {l[i]}\")\nfun(0)\n\n\nl\n\nIn the preceding code block, value of y has not been changed by calling the two functions. Check pythontutor.\n\nCalling conventions\nFunction definitions start with def and a colon indentation. Value are returned by return keyword. Otherwise the return value is None. Functions can have several arguments: def f(x,y) but always one return argument. It is however to return a tuple, and “unpack” it.\n\ndef f(x,y):\n    z1 = x+y\n    z2 = x-y\n    return (z1,z2)      # here brackets are optional:  `return z1,z2` works too\n\nres = f(0.1, 0.2)\nt1, t2 = f(0.2, 0.2)     # t1,t2=res works too\n\n\nres\n\nNamed arguments can be passed in any order and receive default values.\n\ndef problem(why=\"The moon shines.\", what=\"Curiosity killed the cat.\", where=\"Paris\"):\n    print(f\"Is it because {why.lower().strip('.')} that {what.lower().strip('.')}, in {where.strip('.')}?\")\n\n\nproblem(where='Paris')\n\n\nproblem(where=\"ESCP\", why=\"Square root of two is irrational\", what=\"Some regressions never work.\")\n\nPositional arguments and keyword arguments can be combined\n\ndef f(x, y, β=0.9, γ=4.0, δ=0.1):\n    return x*β+y**γ*δ\n\n\nf(0.1, 0.2)\n\n\n\nDocstrings\nFunctions are documented with a special string. Documentation It must follow the function signature immediately and explain what arguments are expected and what the function does\n\ndef f(x, y, β=0.9, γ=4.0, δ=0.1):   # kjhkugku\n    \"\"\"Compute the model residuals\n    \n    Parameters\n    ----------\n    x: (float) marginal propensity to do complicated stuff\n    y: (float) inverse of the elasticity of bifractional risk-neutral substitution\n    β: (float) time discount (default 0.9)\n    γ: (float) time discount (default 4.0)\n    δ: (float) time discount (default 0.1)\n    \n    Result\n    ------\n    res: beta-Hadamard measure of cohesiveness\n    \n    \"\"\"\n    res = x*β+y**γ*δ\n    return res\n\nRemark: Python 3.6 has introduced type indication for functions. They are useful as an element of indication and potentially for type checking. We do not cover them in this tutorial but this is what they look like:\n\ndef f(a: int, b:int)-&gt;int:\n    if a&lt;=1:\n        return 1\n    else:\n        return f(a-1,b) + f(a-2,b)*b\n\n\n\nPacking and unpacking\nA common case is when one wants to pass the elements of an iterable as positional argument and/or the elements of a dictionary as keyword arguments. This is espacially the case, when one wants to determine functions that act on a given calibration. Without unpacking all arguments would need to be passed separately.\n\nv = (0.1, 0.2)\np = dict(β=0.9, γ=4.0, δ=0.1)\n\nf(v[0], v[1], β=p['β'], γ=p['γ'], δ=p['δ'])\n\nThere is a special syntax for that: * unpacks positional arguments and ** unpacks keyword arguments. Here is an example:\n\nf(*v, **p)\n\nThe same characters * and ** can actually be used for the reverse operation, that is packing. This is useful to determine functions of a variable number of arguments.\n\ndef fun(**p):\n    β = p['β']\n    return β+1\nfun(β=1.0)\nfun(β=1.0, γ=2.0) # γ is just ignored\n\nInside the function, unpacked objects are lists and dictionaries respectively.\n\ndef fun(*args, **kwargs):\n    print(f\"Positional arguments: {len(args)}\")\n    for a in args:\n        print(f\"- {a}\")\n    print(f\"Keyword arguments: {len(args)}\")\n    for key,value in kwargs.items():\n        print(f\"- {key}: {value}\")\n\n\nfun(0.1, 0.2, a=2, b=3, c=4)\n\n\n\nFunctions are first class objects\nThis means they can be assigned and passed around.\n\ndef f(x): return 2*x*(1-x)\ng = f # now `g` and `f` point to the same function\ng(0.4)\n\n\ndef sumsfun(l, f):\n    return [f(e) for e in l]\n\n\nsumsfun([0.0, 0.1, 0.2], f)\n\n\ndef compute_recursive_series(x0, fun, T=50):\n    a = [x0]\n    for t in range(T):\n        x0 = a[-1]\n        x = fun(x0)\n        a.append(x)\n    return a\n\ncompute_recursive_series(0.3, f, T=5)\n\nThere is another syntax to define a function, without giving it a name first: lambda functions. It is useful when passing a function as argument.\n\nsorted(range(6), key=lambda x: (-2)**x)\n\nLambda functions are also useful to reduce quickly the number of arguments of a function (aka curryfication)\n\ndef logistic(μ,x): return μ*x*(1-x)\n# def chaotic(x): return logistic(3.7, x)\n# def convergent(x): return logistic(2.5, x)\nchaotic = lambda x: logistic(3.7, x)\nconvergent = lambda x: logistic(2.5, x)\n\n\nl = [compute_recursive_series(0.3,fun, T=20) for fun in [convergent, chaotic]]\n[*zip(*l)]\n\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\ntab = np.array(l)\nplt.plot(tab[0,:-1],tab[0,1:])\ntab = np.array(l)\nplt.plot(tab[1,:-1],tab[1,1:])\nplt.plot(np.linspace(0,1),np.linspace(0,1))\nplt.xlabel(\"$x_n$\")\nplt.ylabel(\"$x_{n+1}$\")\nplt.grid()\n\n\n\nFunctions pass arguments by reference\nMost of the time, variable affectation just create a reference.\n\na = [1,2,3]\nb = a\na[1] = 0\n(a, b)\n\nTo get a copy instead, one needs to specify it explicitly.\n\nimport copy\na = [1,2,3]\nb = copy.copy(a)\na[1] = 0\n(a, b)\n\nNot that copy follows only one level of references. Use deepcopy for more safety.\n\na0 = ['a','b']\na = [a0, 1, 2]\nb = copy.copy(a)\na[0][0] = 'ξ'\na, b\n\n\na0 = ['a','b']\na = [a0, 1, 2]\nb = copy.deepcopy(a)\na[0][0] = 'ξ'\na, b\n\nArguments in a function are references towards the original object. No data is copied. It is then easy to construct functions with side-effects.\n\ndef append_inplace(l1, obs):\n    l1.append(obs)\n    return l1\nl1, obs = ([1,2,3], 1.5)\nl2 = append_inplace(l1,obs)\nprint(l2, l1)\n# note that l1 and l2 point to the same object\nl1[0] = 'hey'\nprint(l2, l1)\n\nThis behaviour might feel unnatural but is very sensible. For instance if the argument is a database of several gigabytes and one wants to write a function which will modify a few of its elements, it is not reasonable to copy the db in full."
  },
  {
    "objectID": "session_1/Python_basics.html#objects",
    "href": "session_1/Python_basics.html#objects",
    "title": "Introduction to Python",
    "section": "Objects",
    "text": "Objects\nObjects ?\n\ncan be passed around / referred too\nhave properties (data) and methods (functions) attached to them\ninherit properties/methods from other objects\n\nObjects are defined by a class definition. By convention, classes names start with uppercase . To create an object, one calls the class name, possibly with additional arguments.\n\nclass Dog:\n    name = \"May\" # class property\n\nd1 = Dog()\nd2 = Dog()\n\nprint(f\"Class: d1-&gt;{type(d1)}, d2-&gt;{type(d2)}\")\nprint(f\"Instance address: d2-&gt;{d1},{d2}\")\n\nNow, d1 and d2 are two different instances of the same class Dog. Since properties are mutable, instances can have different data attached to it.\n\nd1.name = \"Boris\"\nprint([e.name for e in [d1,d2]])\n\nMethods are functions attached to a class / an instance. Their first argument is always an instance. The first argument can be used to acess data held by the instance.\n\nclass Dog:\n    name = None # default value\n    def bark(self):\n        print(\"Wouf\")\n    def converse(self):\n        n = self.name\n        print(f\"Hi, my name is {n}. I'm committed to a strong and stable government.\")\n        \nd = Dog()\nd.bark()   # bark(d)\nd.converse()\n\n\nConstructor\nThere is also a special method __init__ called the constructor. When an object is created, it is called on the instance. This is useful in order to initialize parameters of the instance.\n\nclass Calibration:\n    \n    def __init__(self, x=0.1, y=0.1, β=0.0):\n        if not (β&gt;0) and (β&lt;1):\n            raise Exception(\"Incorrect calibration\"})\n        self.x = x\n        self.y = y\n        self.β = β\n    \n\n\nc1 = Calibration()\nc2 = Calibration(x=3, y=4)\n\nTwo instances of the same class have the same method, but can hold different data. This can change the behaviour of these methods.\n\n# class Dog:\n    \n#     state = 'ok'\n    \n#     def bark(self):\n#         if self.state == 'ok':\n#             print(\"Wouf!\")\n#         else:\n#             print(\"Ahouuu!\")\n        \n# d = Dog()\n# d1 = Dog()\n# d1.state = 'hungry'\n\n# d.bark()\n# d1.bark()\n\nTo write a function which will manipulate properties and methods of an object, it is not required to know its type in advance. The function will succeed as long as the required method exist, fail other wise. This is called “Duck Typing”: if it walks like a duck, it must be a duck…\n\nclass Duck:\n    def walk(self): print(\"/-\\_/-\\_/\")\n        \nclass Dog:\n    def walk(self): print(\"/-\\_/*\\_/\")\n    def bark(self): print(\"Wouf\")\n\n\nanimals = [C() for C in (Duck,Dog)]\ndef go_in_the_park(animal):\n    for i in range(3): animal.walk()\nfor a in animals:\n    go_in_the_park(a)\n\n\nInheritance\nThe whole point of classes, is that one can construct hierarchies of classes to avoid redefining the same methods many times. This is done by using inheritance.\n\nclass Animal:\n    \n    def run(self): print(\"👣\"*4)\n\nclass Dog(Animal):\n    def bark(self): print(\"Wouf\")\n        \nclass Rabbit(Animal):\n    def run(self):\n        super().run() ; print( \"🐇\" )\n\n\nAnimal().run()\ndog = Dog()\ndog.run()\ndog.bark()\nRabbit().run()\n\nIn the above example, the Dog class inherits from inherits the method run from the Animal class: it doesn’t need to be redefined again. Essentially, when run(dog) is called, since the method is not defined for a dog, python looks for the first ancestor of dog and applies the method of the ancestor.\n\n\nSpecial methods\nBy conventions methods starting with double lowercase __ are hidden. They don’t appear in tab completion. Several special methods can be reimplemented that way.\n\nclass Calibration:\n    \n    def __init__(self, x=0.1, y=0.1, β=0.1):\n        if not (β&gt;0) and (β&lt;1):\n            raise Exception(\"Incorrect calibration\")\n        self.x = x\n        self.y = y\n        self.β = β\n    \n    def __str__(self):\n        return f\"Calibration(x={self.x},y={self.y}, β={self.β})\"\n\n\nstr(Calibration() )\n\n\n\ncomplement\nPython is not 100% object oriented. - some objects cannot be subclassed - basic types behave sometimes funny (interning strings)\n\na = 'a'*4192\nb = 'a'*4192\na is b\n\n\na = 'a'*512\nb = 'a'*512\na is b"
  },
  {
    "objectID": "session_1/transcript.html#so-what-will-we-do",
    "href": "session_1/transcript.html#so-what-will-we-do",
    "title": "Introduction",
    "section": "So what will we do ?",
    "text": "So what will we do ?\n\n\nProgramming\nEconometrics / Machine Learning\nTalk about economics\n\n\n\nData-based economics (1)\n\nMost economists use data all the time\n\nto illustrate facts\nto test theories\n\n\n\n\n\nData-based economics\nIn practice these are the tasks an economist needs to perform:\n\n\nimport data\n\nclean the data\n\ndeal with heterogenous sources, missing data, abnormal observerations\nsuper time consuming\nwe’ll make this part easy for you\n\n\ndescribe the data (statistics), visualize it\n\ninterpret it using a model\n\npresent results"
  },
  {
    "objectID": "session_1/transcript.html#econometrics",
    "href": "session_1/transcript.html#econometrics",
    "title": "Introduction",
    "section": "Econometrics",
    "text": "Econometrics\n\n\nEconometricks\n\nAn art invented by economists: \\[\\underbrace{y}\\_{\\text{dependent variable}} = a \\underbrace{x}\\_{\\text{explanatory variable}} + b\\]\n\nMain challenge:\n\ngiven dataset \\((x_i, y_i)\\)\nfind \\(a\\) while controlling for \\(b\\)\nunderstand robustness of results\npredict new values of \\(y\\) for new values of \\(x\\)\n\n\nExample 1:\n\nHow happy are you?\nWhat is \\(x\\) ? What is \\(y\\) ?\n\n\n\n\n\nEconometricks (2)\n\\[\\underbrace{y}\\_{\\text{dependent variable}} = a \\underbrace{x}\\_{\\text{explanatory variable}} + b\\]\n\n\nExample 2:\n\nyoung men who go to war receive in average lower wages when they return than men who didn’t go to war\n\n… is it because they skipped college?\n\n… or did they choose to go to war because they were less skilled for college?\n\nfind a way to extract causality\n\n-&gt; instrumental variables\n\n\nThis was worth a Nobel Prize! (D. Card, J. Angrist, G.W. Imbens)\n\n\n\n\nEconometricks (3)\n\\[\\underbrace{y}\\_{\\text{dependent variable}} = a \\underbrace{x}\\_{\\text{explanatory variable}} + b\\]\n\n\nExample 3:\n\nI want to establish a link between growth and trade openness\n\nbut my country has only 10 years of historical data… (within dimension)\n\n… let’s take 50 countries at the same time (between dimension)\n\nfind a way to analyze both dimensions at the same time\n\n-&gt; panel data\n\n\n\n\n\nBig Data Era and Machine Learning\n\nData has become very abundant\n\nLarge amounts of data of all kinds\n\nstructured (tables, …)\nunstructured (text, images, …)\n\n\nMachine learning:\n\na set of powerful algorithms…\n… so powerful some call it artificial intelligence\n\nthey learn by processing data\n\n… to extract information and relations in large data sets\n…\n\n\nComparison with econometrics\n\nML has it own, partially redundant, jargon\nmuch harder to understand causality, standard deviation (i.e. precision)\n\n\n\n\n\nMachine Learning\n\\[\\underbrace{y}\\_{\\text{predicted variable}} = f( \\underbrace{x}\\_{\\text{feature}} , a)\\]\n\nChallenge:\n\ngiven dataset \\((x_i, y_i)\\)\nfind \\(a\\), that is find a nonlinear relationship between \\(a\\) and \\(b\\)\npredict new values of \\(y\\) given new values of \\(x\\)\n\nWhat are the difference with econometrics?\n\n\n\n\nBig Data Era and Machine Learning (1)\n\nSentiment analysis: predict population’s optimism by analyzing tweets.\nCheck sentiment viz\n\n\n\nBig Data Era and Machine Learning (2)\n\nDeep learning: artificial neural nets"
  },
  {
    "objectID": "session_1/transcript.html#programming",
    "href": "session_1/transcript.html#programming",
    "title": "Introduction",
    "section": "Programming",
    "text": "Programming\n\n\nWhy program in Python?\n\nWhy learn and use Python?\nAnd not \n\nR\nSPSS\nStata\nMatlab\nC\nJavascript\nSQL\n…\n\n\n\n\n\nBecause\nPython is:\n\n\n&lt;img src=\"python_kids.jpg\" width=800&gt;&lt;br&gt;Easy\n\n\n&lt;img src=\"download_python.png\" width=1200&gt;&lt;br&gt;Free\n\n\n\nPopular (TIOBE Index) - lively community - lots of online ressources - libraries for virtually anything\n\n\n   \n\nThe lingua Franca of Machine learning\n\nAll major machine learning softwares are written or interface with Python"
  },
  {
    "objectID": "session_1/transcript.html#why-should-you-learn-programming",
    "href": "session_1/transcript.html#why-should-you-learn-programming",
    "title": "Introduction",
    "section": "Why Should you learn programming ?",
    "text": "Why Should you learn programming ?\n\n\nResearchers (econometricians or data scientists) spend 80% of their time writing code.\n\nPresentation (plots, interactive apps) is key and relies on\n\n… programming\n\n\nInteraction with code becomes unavoidable in business environment\nfixing the website\nquerying the database\n…\n\nWorth investing a bit of time to learn it\n\nyou can easily become an expert\n\n\nPlus it’s fun"
  },
  {
    "objectID": "session_1/transcript.html#programming-1",
    "href": "session_1/transcript.html#programming-1",
    "title": "Introduction",
    "section": "Programming",
    "text": "Programming"
  },
  {
    "objectID": "session_1/transcript.html#how-good-should-you-program",
    "href": "session_1/transcript.html#how-good-should-you-program",
    "title": "Introduction",
    "section": "How good should you program ?",
    "text": "How good should you program ?\n\n\n\n&lt;div class=\"fragment current-visible\" data-fragment-index=1&gt;   &lt;img src=\"anxious.jpg\"&gt; &lt;/div&gt;\n&lt;div class=\"fragment current-visible\" data-fragment-index=2&gt;   &lt;img src=\"furious.webp\"&gt;  &lt;/div&gt;\n&lt;div class=\"fragment current-visible\" data-fragment-index=3&gt;  &lt;img src=\"relieved.jpg\"&gt; &lt;/div&gt;\n&lt;div class=\"fragment current-visible\" data-fragment-index=4&gt;  &lt;img src=\"googleit.avif\"&gt;  &lt;/div&gt;\n&lt;div class=\"fragment current-visible\" data-fragment-index=5&gt;  &lt;img src=\"happy.jpeg\" width=600&gt;  &lt;/div&gt;\n\n\n\n\n\nWe will “assume” everybody as some prior experience with Python\n\nEven though some of you have possibly never touched it\n\nWe’ll do some catchup today\n\nAnd count on you to find the resources to learn what you need when you need it\n\nOf course you can always ask questions"
  },
  {
    "objectID": "session_1/transcript.html#additional-resources",
    "href": "session_1/transcript.html#additional-resources",
    "title": "Introduction",
    "section": "Additional resources",
    "text": "Additional resources\nPlenty of online resources to learn python/econometrics/machine learning\n\nlearnpython sponsored by datacamp\nquantecon: designed for economists, good examples of projects\nPython Data Science Handbook: by Jake Van der Plas, very complete. Online free version.\nIntroduction to Econometrics with R, in R but very clear (beginner and advanced versions)"
  },
  {
    "objectID": "session_1/transcript.html#quantecon",
    "href": "session_1/transcript.html#quantecon",
    "title": "Introduction",
    "section": "Quantecon",
    "text": "Quantecon\n\n\n\n\n&lt;div class=\"col\" &gt;\n&lt;img src=\"tom_sargent.jpg\" &gt;&lt;br&gt;\nTom Sargent\n&lt;/div&gt;\n&lt;div class=\"col\"&gt;\n&lt;img src=\"john_stachurski.jpg\" width=50%&gt;&lt;br&gt;\nJohn Stachurski\n&lt;/div&gt;\n\n\n\nTom Sargent @ ESCP &lt;br&gt;\n&lt;img  src=\"t3m_escp.jpg\"&gt;\n\n\n\n\nQuantecon: free online lectures to learn python programming and (advanced) economics\n\nnow with a section on datascience\nit is excellent!\nwe will use some of it today"
  },
  {
    "objectID": "session_10/graphs/inference.html",
    "href": "session_10/graphs/inference.html",
    "title": "Data-Based Economics",
    "section": "",
    "text": "from matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n\ndef generate_dataset(μ1, μ2, α, β, σ, N=10):\n    xvec = np.random.uniform(μ1, μ2, N)\n    yvec = α + β*xvec + np.random.normal(size=N)*σ\n    return pd.DataFrame({'x': xvec, 'y': yvec})\n\n\ndf = generate_dataset(0.0, 1.0, 0.1, 0.8, 0.1)\n\n\nplt.plot(df['x'], df['y'], 'o')\nplt.grid()\n\n\n\n\n\ndef plot_distribution(α, β, σ, N=100000, μ1=0.0, μ2=1.0):\n    xvec = np.random.uniform(μ1, μ2, N)\n    yvec = α + β*xvec + np.random.normal(size=N)*σ\n    plt.plot(xvec, yvec, '.r', alpha=0.005)\n    plt.plot(xvec, α + β*xvec, color='black')\n\n# missing ridge line\n\n\nimport statsmodels\n\n\nμ1 = 0\nμ2 = 1.0\nα = 0.1\nβ = 0.8\nσ = 0.2\nN = 20\nK = 1000\n\n\nimport statsmodels.formula.api as smf\n\n\ndf = generate_dataset(μ1, μ2, α, β, σ, N=N)\n\n\nres = smf.ols(formula='y ~ x + 1', data=df).fit()\nparams = res.params\nαhat = params['Intercept']\nβhat = params['x']\nσhat = res.resid.std()\n\n\nres.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\ny\nR-squared:\n0.692\n\n\nModel:\nOLS\nAdj. R-squared:\n0.675\n\n\nMethod:\nLeast Squares\nF-statistic:\n40.48\n\n\nDate:\nTue, 26 Jan 2021\nProb (F-statistic):\n5.41e-06\n\n\nTime:\n04:02:36\nLog-Likelihood:\n7.6662\n\n\nNo. Observations:\n20\nAIC:\n-11.33\n\n\nDf Residuals:\n18\nBIC:\n-9.341\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n0.1210\n0.077\n1.565\n0.135\n-0.041\n0.283\n\n\nx\n0.7941\n0.125\n6.362\n0.000\n0.532\n1.056\n\n\n\n\n\n\nOmnibus:\n1.410\nDurbin-Watson:\n1.507\n\n\nProb(Omnibus):\n0.494\nJarque-Bera (JB):\n0.890\n\n\nSkew:\n-0.081\nProb(JB):\n0.641\n\n\nKurtosis:\n1.979\nCond. No.\n4.20\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nres.predict(df['x'])\n\n0     0.326200\n1     0.211704\n2     0.798819\n3     0.603306\n4     0.573319\n5     0.823919\n6     0.740622\n7     0.503227\n8     0.292622\n9     0.489566\n10    0.138720\n11    0.355157\n12    0.594171\n13    0.883917\n14    0.266229\n15    0.827021\n16    0.912376\n17    0.163088\n18    0.684858\n19    0.732782\ndtype: float64\n\n\n\nfor i in [1,2,3]:\n    \n    fig = plt.figure(figsize=(10,14))\n    plt.subplot(311)\n    plot_distribution(0.1, 0.8, 0.2)\n    plt.grid()\n    plt.title(f\"True Distribution: $y = {α:.2f} + {β:.2f} x + {σ:.2f} u$\")\n    plt.xlim(0,1)\n    plt.ylim(-0.5, 1.5)\n\n    plt.subplot(312)\n    plt.xlim(0,1)\n    plt.ylim(-0.5, 1.5)\n    if i&gt;=2:\n        plt.plot(df['x'], df['y'], 'o')\n    if i&gt;=3:\n        plt.plot(df['x'], res.predict(), label=f'$\\hat{{α}}={αhat:.2f}; \\hat{{β}}={βhat:.2f}$')\n        plt.legend(loc='lower right')\n    plt.title(\"Random Draw\")\n    plt.grid()\n    \n    plt.savefig(f\"regression_uncertainty_{i}.png\")\n\n\n\n\n\n\n\n\n\n\n\nimport scipy.stats\n\n\ndatasets = [generate_dataset(μ1, μ2, αhat, βhat, σhat, N=N) for i in range(K)]\nall_params = [smf.ols(formula='x ~ y + 1', data=df).fit() for df in datasets]\nαvec = np.array( [e.params['Intercept'] for e in all_params] )\nβvec = np.array( [e.params['y'] for e in all_params] )\n\n\ngkd = scipy.stats.kde.gaussian_kde(βvec)\n\n\nfor i in [1,2,3,4,5,6,7,8,9,10,100]:\n\n    fig = plt.figure(figsize=(10,14))\n    plt.subplot(311)\n    plot_distribution(0.1, 0.8, 0.2)\n    plt.grid()\n    plt.title(f\"True Distribution: $y = {αhat:.2f} + {βhat:.2f} x + {σhat:.2f} u$\")\n    plt.xlim(0,1)\n    plt.ylim(-0.5, 1.5)\n    \n    plt.subplot(312)\n    plt.xlim(0,1)\n    plt.ylim(-0.5, 1.5)\n    df = datasets[i]\n    if i&gt;=2:\n        plt.plot(df['x'], df['y'], 'o')\n    plt.title(\"Random Draw\")\n    plt.grid()\n\n    plt.subplot(313)\n    if i==3:\n        plt.plot(βvec[i], βvec[i]*0, 'o')\n    if i&gt;4:\n        plt.plot(βvec[3:i], βvec[3:i]*0, 'o')\n    if i&gt;10:\n        xx = np.linspace(0.2, 1.4, 10000)\n        plt.plot( βvec, gkd.pdf(βvec), '.')\n    plt.title(\"Distribution of β\")\n    plt.xlim(0.2, 1.4)\n    plt.ylim(-0.1, 4)\n    plt.grid()\n\n    plt.tight_layout()\n\n    plt.savefig(f\"random_estimates_{i}.png\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplt.plot( βvec, βvec*0, 'o')"
  },
  {
    "objectID": "session_10/graphs/Untitled1.html",
    "href": "session_10/graphs/Untitled1.html",
    "title": "Data-Based Economics",
    "section": "",
    "text": "from matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n\nimport statsmodels.api as sm\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nincome\neducation\nprestige\n\n\n\n\ncount\n45.000000\n45.000000\n45.000000\n\n\nmean\n41.866667\n52.555556\n47.688889\n\n\nstd\n24.435072\n29.760831\n31.510332\n\n\nmin\n7.000000\n7.000000\n3.000000\n\n\n25%\n21.000000\n26.000000\n16.000000\n\n\n50%\n42.000000\n45.000000\n41.000000\n\n\n75%\n64.000000\n84.000000\n81.000000\n\n\nmax\n81.000000\n100.000000\n97.000000\n\n\n\n\n\n\n\n\ndf.cov()\n\n\n\n\n\n\n\n\nincome\neducation\nprestige\n\n\n\n\nincome\n597.072727\n526.871212\n645.071212\n\n\neducation\n526.871212\n885.707071\n798.904040\n\n\nprestige\n645.071212\n798.904040\n992.901010\n\n\n\n\n\n\n\n\nfrom matplotlib import pyplot as plt\n\n\nplt.figure(figsize=(8,6))\nplt.plot(df['education'],df['income'],'o')\nplt.grid()\nplt.xlabel(\"x (Education)\")\nplt.ylabel(\"y (Income)\")\nplt.savefig(\"data_description.png\")\n\n\n\n\n\nfor i in [1,2,3]:\n    xvec = np.linspace(10,100)\n\n    plt.figure(figsize=(12,8))\n    plt.plot(df['education'],df['income'],'o')\n\n    plt.plot(xvec, xvec * 0 + 50)\n    if i&gt;=2:\n        plt.plot(xvec, xvec )\n    if i&gt;=3:\n        plt.plot(xvec,  90- 0.6*xvec )\n\n    plt.grid()\n    plt.xlabel(\"x (Education)\")\n    plt.ylabel(\"y (Income)\")\n    plt.savefig(f\"which_line_{i}.png\")\n\n\n\n\n\n\n\n\n\n\n\nfrom ipywidgets import interact\n\n\nimport matplotlib.patches as patches\n\n\na = 0.1\nb = 1.0\nind = 23\n\n\napprox =  a + b*xvec\n\n# Create figure and axes\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\nplt.plot(df['education'],df['income'],'o')\nplt.plot(xvec, approx, color='red')\n\nx, y = df['education'][ind], df['income'][ind]\nplt.plot(x, y, 'o', color='red' )\np = a+b*x\nplt.grid(True)\nh = abs(p-y)\nplt.vlines(x, y+h, y, color='red')\n\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.savefig(f\"error_0.png\")\n\n\n\n\n\nplt.vlines?\n\n\nSignature:\nplt.vlines(\n    x,\n    ymin,\n    ymax,\n    colors=None,\n    linestyles='solid',\n    label='',\n    *,\n    data=None,\n    **kwargs,\n)\nDocstring:\nPlot vertical lines.\nPlot vertical lines at each *x* from *ymin* to *ymax*.\nParameters\n----------\nx : float or array-like\n    x-indexes where to plot the lines.\nymin, ymax : float or array-like\n    Respective beginning and end of each line. If scalars are\n    provided, all lines will have same length.\ncolors : list of colors, default: :rc:`lines.color`\nlinestyles : {'solid', 'dashed', 'dashdot', 'dotted'}, optional\nlabel : str, default: ''\nReturns\n-------\n`~matplotlib.collections.LineCollection`\nOther Parameters\n----------------\n**kwargs : `~matplotlib.collections.LineCollection` properties.\nSee Also\n--------\nhlines : horizontal lines\naxvline: vertical line across the axes\nNotes\n-----\n.. note::\n    In addition to the above described arguments, this function can take\n    a *data* keyword argument. If such a *data* argument is given,\n    the following arguments can also be string ``s``, which is\n    interpreted as ``data[s]`` (unless this raises an exception):\n    *x*, *ymin*, *ymax*, *colors*.\n    Objects passed as **data** must support item access (``data[s]``) and\n    membership test (``s in data``).\nFile:      ~/.local/opt/miniconda/lib/python3.8/site-packages/matplotlib/pyplot.py\nType:      function\n\n\n\n\n\na = 0.1\nb = 1.0\nind = 23\n\n\napprox =  a + b*xvec\n\n# Create figure and axes\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\nplt.plot(df['education'],df['income'],'o')\nplt.plot(xvec, approx, color='red')\n\nx, y = df['education'][ind], df['income'][ind]\nplt.plot(x, y, 'o', color='red' )\np = a+b*x\nplt.grid(True)\nh = abs(p-y)\nif p-y&gt;0:\n    # Create a Rectangle patch\n    rect = patches.Rectangle((x,y),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n    ax.add_patch(rect)\n    \nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.savefig(f\"errors_{1}.png\")\n\n\n\n\n\ndef L(a,b):\n    Δ = a + b*df['education'] - df['income']\n    return (Δ**2).sum()\n\n\na = 0.1\nb = 0.8\n\napprox =  a + b*xvec\n\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\n\n\nplt.plot(df['education'],df['income'],'o', label=f\"L({a,b})={L(a,b)}\")\nplt.plot(xvec, approx, color='red')\n\nplt.grid(True)\nfor ind in range(df.shape[0]):\n    \n    x, y = df['education'][ind], df['income'][ind]\n    p = a+b*x\n\n    h = abs(p-y)\n    if p-y&gt;0:\n        # Create a Rectangle patch\n        rect = patches.Rectangle((x,y),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\n    else:\n        rect = patches.Rectangle((x,y-h),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.legend(loc='upper right')\nplt.savefig(f\"errors_2.png\")\n\n\n\n\n\na = 90\nb = -0.6\n\napprox =  a + b*xvec\n\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\n\n\nplt.plot(df['education'],df['income'],'o', label=f\"L({a,b})={L(a,b)}\")\nplt.plot(xvec, approx, color='red')\n\nplt.grid(True)\nfor ind in range(df.shape[0]):\n    \n    x, y = df['education'][ind], df['income'][ind]\n    p = a+b*x\n\n    h = abs(p-y)\n    if p-y&gt;0:\n        # Create a Rectangle patch\n        rect = patches.Rectangle((x,y),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\n    else:\n        rect = patches.Rectangle((x,y-h),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.legend(loc='upper right')\nplt.savefig(f\"errors_3.png\")\n\n\n\n\n\nimport scipy.optimize\n\n\nscipy.optimize.minimize(lambda x: L(x[0], x[1]),np.array([0.5, 0.5]))\n\n      fun: 12480.970174488397\n hess_inv: array([[ 7.14169839e-09, -3.91281920e-09],\n       [-3.91281920e-09,  2.46663613e-09]])\n      jac: array([0.00024414, 0.00012207])\n  message: 'Desired error not necessarily achieved due to precision loss.'\n     nfev: 57\n      nit: 7\n     njev: 19\n   status: 2\n  success: False\n        x: array([10.60350224,  0.59485938])\n\n\n\na = 10\nb = 0.59\n\napprox =  a + b*xvec\n\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\n\n\nplt.plot(df['education'],df['income'],'o', label=f\"L({a,b})={L(a,b)}\")\nplt.plot(xvec, approx, color='red')\n\nplt.grid(True)\nfor ind in range(df.shape[0]):\n    \n    x, y = df['education'][ind], df['income'][ind]\n    p = a+b*x\n\n    h = abs(p-y)\n    if p-y&gt;0:\n        # Create a Rectangle patch\n        rect = patches.Rectangle((x,y),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\n    else:\n        rect = patches.Rectangle((x,y-h),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.legend(loc='upper right')\nplt.savefig(f\"errors_4.png\")\n\n\n\n\n\na = 10\nb = 0.59\n\napprox =  a + b*xvec\n\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\n\n\nplt.plot(df['education'],df['income'],'o', label=f\"L({a,b})={L(a,b)}\")\nplt.plot(xvec, approx, color='red', alpha=0.5)\n\nplt.plot(60, a + b*60, 'o', color='red',)\n\nprint(a+b*60)\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.legend(loc='upper right')\nplt.savefig(f\"prediction.png\")\n\n45.4\n\n\n\n\n\n\na = 10\nb = 0.59\n\napprox =  (a + b*df['education'] - df['income'])\n\nplt.figure(figsize=(12,6))\n\nplt.subplot(121)\nplt.plot(approx)\nplt.grid(False)\nplt.title(\"Residuals\")\n\n\nplt.subplot(122)\ndistplot(approx)\nplt.title(\"Distribution of residuals\")\nplt.grid()\n\nplt.savefig(\"residuals.png\")\n\n/home/pablo/.local/opt/miniconda/lib/python3.8/site-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n  warnings.warn(msg, FutureWarning)\n\n\n\n\n\n\n(a + b*df['education'] - df['income']).std()\n\n16.842782676352154\n\n\n\n\n\n/home/pablo/.local/opt/miniconda/lib/python3.8/site-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n  warnings.warn(msg, FutureWarning)\n\n\n&lt;AxesSubplot:ylabel='Density'&gt;\n\n\n\n\n\n\nfrom scipy.stats import f\n\n\nf(0.3)\n\nTypeError: _parse_args() missing 1 required positional argument: 'dfd'\n\n\n\nnp.rand\n\n\nK = 100\nxvec = np.linspace(0,1,K)\ne1 = np.random.randn(K)*0.1\nyvec = 0.1 + xvec*0.4 + e1\ne2 = np.random.randn(K)*0.05\nyvec2 = 0.1 + xvec*(xvec-1)/2 + e2\ne3 = np.random.randn(K)*xvec/2\nyvec3 = 0.1 + xvec + e3\n\nyvec4 = 0.1 + np.sin(xvec*6) + np.random.randn(K)*xvec/2\n\n\nfrom dolo.numeric.processes import VAR1\n\n\nsim = VAR1( ρ=0.8, Σ=0.001).simulate(N=1,T=100)\nyvec4 = 0.1 + xvec*0.4 + sim.ravel()\n\n\nplt.figure(figsize=(18,6))\nplt.subplot(241)\nplt.plot(xvec, yvec,'o')\nplt.plot(xvec, 0.1 + xvec*0.4 )\nplt.ylabel(\"Series\")\nplt.title(\"white noise\")\nplt.subplot(242)\nplt.plot(xvec, yvec2, 'o')\nplt.plot(xvec, yvec2*0)\nplt.title('nonlinear')\nplt.subplot(243)\nplt.plot(xvec, yvec3,'o')\nplt.plot(xvec, 0.1 + xvec)\nplt.title('heteroskedastic')\nplt.subplot(244)\nplt.plot(xvec, yvec4,'o')\nplt.plot(xvec, xvec*0.6)\n\nplt.title('correlated')\n\n\nplt.subplot(245)\nplt.plot(xvec, e1,'o')\nplt.ylabel(\"Residuals\")\nplt.subplot(246)\nplt.plot(xvec, yvec2-0.075, 'o')\n\nplt.subplot(247)\nplt.plot(xvec, e3,'o')\nplt.subplot(248)\nplt.plot(xvec, sim.ravel(),'o')\n\nplt.tight_layout()\n\nplt.savefig(\"residuals_circus.png\")"
  },
  {
    "objectID": "session_10/index.html",
    "href": "session_10/index.html",
    "title": "Data-Based Economics",
    "section": "",
    "text": "A model is fitted (trained / regressed) on a given amount of data\nA model can be more or less flexible\n\nhave more or less independent parameters (aka degrees of freedom)\nex: \\(y = a + b x\\) (2) vs \\(y = a + b x_1 + c x_1^2 + e x_2 + f x_3\\) (5)\n\nMore flexible models fit the training data better…\n…but tend to perform worse for predictions\nThis is known as:\n\nThe Bias (fit) vs Variance (prediction) tradeoff\nThe no free lunch theorem\n\n\n\n\n\n\n\n\n\n\n\n\nThe goal of machine learning consists in making the best predictions:\n\nuse enough data to maximize the fit…\n… but control the number of independent parameters to prevent overfitting\n\nex: LASSO regression has lots of parameters, but tries to keep most of them zero\n\nultimately quality of prediction is evaluated on a test set, independent from the training set\n\nIn econometrics we can perform\n\npredictions: sames issues as ML\nexplanatory analysis: focus on the effect of one (or a few) explanatory variables\n\nthis does not necessary require strong predictive power\n\n\n\n\n\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.252\nModel:                            OLS   Adj. R-squared:                  0.245\nMethod:                 Least Squares   F-statistic:                     33.08\nDate:                Tue, 30 Mar 2021   Prob (F-statistic):           1.01e-07\nTime:                        02:34:12   Log-Likelihood:                -111.39\nNo. Observations:                 100   AIC:                             226.8\nDf Residuals:                      98   BIC:                             232.0\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n==============================================================================\nIntercept     -0.1750      0.162     -1.082      0.282      -0.496       0.146\nx              0.1377      0.024      5.751      0.000       0.090       0.185\n==============================================================================\nOmnibus:                        2.673   Durbin-Watson:                   1.118\nProb(Omnibus):                  0.263   Jarque-Bera (JB):                2.654\nSkew:                           0.352   Prob(JB):                        0.265\nKurtosis:                       2.626   Cond. No.                         14.9\n==============================================================================\n\n\nUnderstand p-value: chances that a given statistics might have been obtained, under the H0 hypothesis\n\nCheck:\n\n\nglobal significance (Fisher test): chances would have obtained this R2 if all real coefficients were actually 0 (H0 hypothesis)\n\nR2: provides an indication of predictive power. Does not prevent overfitting.\n\nadj. R2: predictive power corrected for excessive degrees of freedom\n\ncoefficient:\n\np-value probability that coefficient might have been greater than observed, if it was actually 0.\nif p-value is smaller than 5%: the coefficient is significant at a 5% level\nconfidence intervals (5%): if the true coefficient was out of this interval, observed value would be very implausible\n\nhigher confidence levels -&gt; bigger intervals\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverfitting\n\n\nbad predictions\n\n\nColinearity\n\n\ncan bias a coefficient of interest\nnot a problem for prediction\nexact colinearity makes traditional OLS fail\n\nTo choose the right amount of variables find a combination which maximizes adjusted R2 or an information criterium\n\n\n\n\n\n\n\\(x\\) is colinear with \\(y\\) if \\(cor(x,y)\\) very close to 1\n\nmore generally \\(x\\) is colinear with \\(y_1, ... y_n\\) if \\(x\\) can be deduced linearly from \\(y_1...y_n\\)\nthere exists \\(\\lambda_1, ... \\lambda_n\\) such that \\(x = \\lambda_1 x_1 + ... + \\lambda_n x_n\\)\nexample: hours of sleep / hours awake (sleep=24-awake)\n\nperfect colinearity is a problem: coefficients are not well defined\n\\(\\text{productivity} = 0.1 + 0.5 \\text{sleep} + 0.5 \\text{awake}\\) or \\(\\text{productivity} = -11.9 + 1 \\text{sleep} + 1 \\text{awake}\\) ?\n\nbest regressions have regressors that:\nexplain independent variable\nare independent from each other (as much as possible)\n\n\n\n\n\nWhat if you don’t have enough variables?\n\\(y = a + bx\\)\n\nR2 can be low. It’s ok for explanatory analysis.\nas long as residuals are normally distributed\n\ncheck graphically to be sure\n(more advanced): there are statistical tests\n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\nSuppose we want to know the effect of \\(x\\) on \\(y\\).\n\nWe run the regression \\(y = a + b x\\)\n\nwe find \\(y = 0.21 + \\color{red}{0.15} x\\)\n\n\nWe then realize we have access to a categorical variable \\(gender \\in {male, female}\\)\n\nWe then add the \\(\\delta\\) dummy variable to the regression: \\(y = a + bx + c \\delta\\)\n\nwe find $ y = -0.04 + x - 0.98 $\n\n\nNote that adding the indicator\n\n\nimproved the fit (\\(R^2\\) is 0.623 instead of 0.306)\n\ncorrected for the omitted variable bias (true value of b is actually 0.2)\n\nprovided an estimate for the effect of variable gender\n\n\n\n\n\n\n\n\n\n\n\n    \n\n\n\n\nIndividual\nGender\nShirt color\nEducation\n…\n\n\n\n\n1\nM\n?\n?\n?\n\n\n2\nF\n?\n?\n?\n\n\n3\nM\n?\n?\n?\n\n\n…\n…\n…\n…\n…\n\n\n\n\n\n\n\nSuppose we want to know the effect of \\(x\\) on \\(y\\).\n\nWe run the regression \\(y = a + b x\\)\n\nwe find \\(y = 1.09 + \\color{red}{0.24} x\\)\n\n\nBut now, the data has a special panel structure.\n\nwe have a categorical variable corresponding to each individual\nit is linked to several omitted variables, most of which we don’t observe\n\n\nFixed Effects: use an indicator for each individual\nwe need to drop the individual specific dummies that we observe (i.e. gender)\n\nRun the regression $y_{i,n} = a_i + b x_{i,n} $\n\nwe find $ y_{i,n} = a_i + x_{i,n} $\n\n\nNote that adding the indicator\n\n\nimproved the fit (\\(R^2\\) is 0.631 instead of 0.278)\n\ncorrected for the unobserved heterogeneity bias (true value of b is actually 0.2)\n\n\n\n\n\n\n\n\n\nEssentially: the intuition behind correcting for unobserved heterogeneity is the same as the one behind ommitted variable bias.\nFixed Effects are essentially dummies specific to some categories of observations\nIn panel data, there are two categories: individual index and time\n\nindividual ( Entity Effects)\ntime (Time Effects)\n\nWe index regressions by them: \\[y_{it} = \\color{red}{a_i} + \\color{blue}{a_t} + b x_{i t} + ... \\]\nFixed effects are not very useful for prediction:\n\none can predict \\(y_{it}\\) only for \\(i,t\\) already in the database\nimpossible to predict new individuals/new dates\n\nRemedy: random fixed effects\n\n\n\n\n\n\nConsider the regression model \\(y = a + b x + \\epsilon\\)\nWhen \\(\\epsilon\\) is correlated with \\(x\\) we have an endogeneity problem.\n\nwe can check in the regression results whether the residuals ares correlated with \\(y\\) or \\(x\\)\n\nEndogeneity can have several sources: omitted variable, measurement error, simultaneity\n\nit creates a bias in the estimate of \\(a\\) and \\(b\\)\n\nWe say we control for endogeneity by adding some variables\nA special case of endogeneity is a confounding factor a variable \\(z\\) which causes at the same time \\(x\\) and \\(y\\)\n\n\n\n\n\n\\[y = a + b x + \\epsilon\\]\n\nRecall: endogeneity issue when \\(\\epsilon\\) is correlated with \\(x\\)\nInstrument: a way to keep only the variability of \\(x\\) that is independent from \\(\\epsilon\\)\n\nit needs to be correlated with \\(x\\)\nnot with all components of \\(\\epsilon\\)\n\nAn instrument can be used to solve endogeneity issues\nIt can also establish the causality from \\(x\\) to \\(y\\):\n\nsince it is independent from \\(\\epsilon\\), all its effect on \\(y\\) goes through \\(x\\)"
  },
  {
    "objectID": "session_10/index.html#important-points",
    "href": "session_10/index.html#important-points",
    "title": "Data-Based Economics",
    "section": "",
    "text": "A model is fitted (trained / regressed) on a given amount of data\nA model can be more or less flexible\n\nhave more or less independent parameters (aka degrees of freedom)\nex: \\(y = a + b x\\) (2) vs \\(y = a + b x_1 + c x_1^2 + e x_2 + f x_3\\) (5)\n\nMore flexible models fit the training data better…\n…but tend to perform worse for predictions\nThis is known as:\n\nThe Bias (fit) vs Variance (prediction) tradeoff\nThe no free lunch theorem\n\n\n\n\n\n\n\n\n\n\n\n\nThe goal of machine learning consists in making the best predictions:\n\nuse enough data to maximize the fit…\n… but control the number of independent parameters to prevent overfitting\n\nex: LASSO regression has lots of parameters, but tries to keep most of them zero\n\nultimately quality of prediction is evaluated on a test set, independent from the training set\n\nIn econometrics we can perform\n\npredictions: sames issues as ML\nexplanatory analysis: focus on the effect of one (or a few) explanatory variables\n\nthis does not necessary require strong predictive power\n\n\n\n\n\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.252\nModel:                            OLS   Adj. R-squared:                  0.245\nMethod:                 Least Squares   F-statistic:                     33.08\nDate:                Tue, 30 Mar 2021   Prob (F-statistic):           1.01e-07\nTime:                        02:34:12   Log-Likelihood:                -111.39\nNo. Observations:                 100   AIC:                             226.8\nDf Residuals:                      98   BIC:                             232.0\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n==============================================================================\nIntercept     -0.1750      0.162     -1.082      0.282      -0.496       0.146\nx              0.1377      0.024      5.751      0.000       0.090       0.185\n==============================================================================\nOmnibus:                        2.673   Durbin-Watson:                   1.118\nProb(Omnibus):                  0.263   Jarque-Bera (JB):                2.654\nSkew:                           0.352   Prob(JB):                        0.265\nKurtosis:                       2.626   Cond. No.                         14.9\n==============================================================================\n\n\nUnderstand p-value: chances that a given statistics might have been obtained, under the H0 hypothesis\n\nCheck:\n\n\nglobal significance (Fisher test): chances would have obtained this R2 if all real coefficients were actually 0 (H0 hypothesis)\n\nR2: provides an indication of predictive power. Does not prevent overfitting.\n\nadj. R2: predictive power corrected for excessive degrees of freedom\n\ncoefficient:\n\np-value probability that coefficient might have been greater than observed, if it was actually 0.\nif p-value is smaller than 5%: the coefficient is significant at a 5% level\nconfidence intervals (5%): if the true coefficient was out of this interval, observed value would be very implausible\n\nhigher confidence levels -&gt; bigger intervals\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverfitting\n\n\nbad predictions\n\n\nColinearity\n\n\ncan bias a coefficient of interest\nnot a problem for prediction\nexact colinearity makes traditional OLS fail\n\nTo choose the right amount of variables find a combination which maximizes adjusted R2 or an information criterium\n\n\n\n\n\n\n\\(x\\) is colinear with \\(y\\) if \\(cor(x,y)\\) very close to 1\n\nmore generally \\(x\\) is colinear with \\(y_1, ... y_n\\) if \\(x\\) can be deduced linearly from \\(y_1...y_n\\)\nthere exists \\(\\lambda_1, ... \\lambda_n\\) such that \\(x = \\lambda_1 x_1 + ... + \\lambda_n x_n\\)\nexample: hours of sleep / hours awake (sleep=24-awake)\n\nperfect colinearity is a problem: coefficients are not well defined\n\\(\\text{productivity} = 0.1 + 0.5 \\text{sleep} + 0.5 \\text{awake}\\) or \\(\\text{productivity} = -11.9 + 1 \\text{sleep} + 1 \\text{awake}\\) ?\n\nbest regressions have regressors that:\nexplain independent variable\nare independent from each other (as much as possible)\n\n\n\n\n\nWhat if you don’t have enough variables?\n\\(y = a + bx\\)\n\nR2 can be low. It’s ok for explanatory analysis.\nas long as residuals are normally distributed\n\ncheck graphically to be sure\n(more advanced): there are statistical tests\n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\nSuppose we want to know the effect of \\(x\\) on \\(y\\).\n\nWe run the regression \\(y = a + b x\\)\n\nwe find \\(y = 0.21 + \\color{red}{0.15} x\\)\n\n\nWe then realize we have access to a categorical variable \\(gender \\in {male, female}\\)\n\nWe then add the \\(\\delta\\) dummy variable to the regression: \\(y = a + bx + c \\delta\\)\n\nwe find $ y = -0.04 + x - 0.98 $\n\n\nNote that adding the indicator\n\n\nimproved the fit (\\(R^2\\) is 0.623 instead of 0.306)\n\ncorrected for the omitted variable bias (true value of b is actually 0.2)\n\nprovided an estimate for the effect of variable gender\n\n\n\n\n\n\n\n\n\n\n\n    \n\n\n\n\nIndividual\nGender\nShirt color\nEducation\n…\n\n\n\n\n1\nM\n?\n?\n?\n\n\n2\nF\n?\n?\n?\n\n\n3\nM\n?\n?\n?\n\n\n…\n…\n…\n…\n…\n\n\n\n\n\n\n\nSuppose we want to know the effect of \\(x\\) on \\(y\\).\n\nWe run the regression \\(y = a + b x\\)\n\nwe find \\(y = 1.09 + \\color{red}{0.24} x\\)\n\n\nBut now, the data has a special panel structure.\n\nwe have a categorical variable corresponding to each individual\nit is linked to several omitted variables, most of which we don’t observe\n\n\nFixed Effects: use an indicator for each individual\nwe need to drop the individual specific dummies that we observe (i.e. gender)\n\nRun the regression $y_{i,n} = a_i + b x_{i,n} $\n\nwe find $ y_{i,n} = a_i + x_{i,n} $\n\n\nNote that adding the indicator\n\n\nimproved the fit (\\(R^2\\) is 0.631 instead of 0.278)\n\ncorrected for the unobserved heterogeneity bias (true value of b is actually 0.2)\n\n\n\n\n\n\n\n\n\nEssentially: the intuition behind correcting for unobserved heterogeneity is the same as the one behind ommitted variable bias.\nFixed Effects are essentially dummies specific to some categories of observations\nIn panel data, there are two categories: individual index and time\n\nindividual ( Entity Effects)\ntime (Time Effects)\n\nWe index regressions by them: \\[y_{it} = \\color{red}{a_i} + \\color{blue}{a_t} + b x_{i t} + ... \\]\nFixed effects are not very useful for prediction:\n\none can predict \\(y_{it}\\) only for \\(i,t\\) already in the database\nimpossible to predict new individuals/new dates\n\nRemedy: random fixed effects\n\n\n\n\n\n\nConsider the regression model \\(y = a + b x + \\epsilon\\)\nWhen \\(\\epsilon\\) is correlated with \\(x\\) we have an endogeneity problem.\n\nwe can check in the regression results whether the residuals ares correlated with \\(y\\) or \\(x\\)\n\nEndogeneity can have several sources: omitted variable, measurement error, simultaneity\n\nit creates a bias in the estimate of \\(a\\) and \\(b\\)\n\nWe say we control for endogeneity by adding some variables\nA special case of endogeneity is a confounding factor a variable \\(z\\) which causes at the same time \\(x\\) and \\(y\\)\n\n\n\n\n\n\\[y = a + b x + \\epsilon\\]\n\nRecall: endogeneity issue when \\(\\epsilon\\) is correlated with \\(x\\)\nInstrument: a way to keep only the variability of \\(x\\) that is independent from \\(\\epsilon\\)\n\nit needs to be correlated with \\(x\\)\nnot with all components of \\(\\epsilon\\)\n\nAn instrument can be used to solve endogeneity issues\nIt can also establish the causality from \\(x\\) to \\(y\\):\n\nsince it is independent from \\(\\epsilon\\), all its effect on \\(y\\) goes through \\(x\\)"
  },
  {
    "objectID": "session_2/DataFrames.html",
    "href": "session_2/DataFrames.html",
    "title": "Data Frames",
    "section": "",
    "text": "A DataFrame (aka a table) is a 2-D labeled data structure with columns of potentially different types.\n\ntypes: quantitative, qualitative (ordered, non-ordered, …)\n\nFirst column is special: the index\n\n\n\n\n\n\nfirst goal of an econometrician: constitute a good dataframe\n\n“cleaning the data”\n\nsometimes data comes from several linked dataframes\n\nrelational database\n\ndataframes / relational databases are so ubiquitous a language has been developed for them: SQL"
  },
  {
    "objectID": "session_2/DataFrames.html#tabular-data",
    "href": "session_2/DataFrames.html#tabular-data",
    "title": "Data Frames",
    "section": "",
    "text": "A DataFrame (aka a table) is a 2-D labeled data structure with columns of potentially different types.\n\ntypes: quantitative, qualitative (ordered, non-ordered, …)\n\nFirst column is special: the index\n\n\n\n\n\n\nfirst goal of an econometrician: constitute a good dataframe\n\n“cleaning the data”\n\nsometimes data comes from several linked dataframes\n\nrelational database\n\ndataframes / relational databases are so ubiquitous a language has been developed for them: SQL"
  },
  {
    "objectID": "session_2/DataFrames.html#pandas",
    "href": "session_2/DataFrames.html#pandas",
    "title": "Data Frames",
    "section": "Pandas",
    "text": "Pandas\n\nimport pandas as pd\n\n\npandas\n\npandas = panel + datas\ncreated by WesMcKinsey, very optimized\nmany options\nif in doubt: minimally sufficient pandas\n\nsmall subset of pandas to do everything\n\ntons of online tutorials ex: doc\n\n\n\ncreating a dataframe (1)\n\n# from a dictionary\nd = {\n    \"country\": [\"USA\", \"UK\", \"France\"],\n    \"comics\": [13, 10, 12]   \n}\npd.DataFrame(d)\n\n\n\n\n\n\n\n\ncountry\ncomics\n\n\n\n\n0\nUSA\n13\n\n\n1\nUK\n10\n\n\n2\nFrance\n12\n\n\n\n\n\n\n\n\n\ncreating a dataframe (2)\n\n# from a matrix\nimport numpy as np\nM = np.array([\n    [18, 150],\n    [21, 200],\n    [29, 1500]\n])\n    \ndf = pd.DataFrame( M, columns=[\"age\", \"travel\"] )\ndf\n\n\n\n\n\n\n\n\nage\ntravel\n\n\n\n\n0\n18\n150\n\n\n1\n21\n200\n\n\n2\n29\n1500"
  },
  {
    "objectID": "session_2/DataFrames.html#file-formats",
    "href": "session_2/DataFrames.html#file-formats",
    "title": "Data Frames",
    "section": "File Formats",
    "text": "File Formats\n\nCommon file formats\n\ncomma separated files: csv file\n\noften distributed online\ncan be exported easily from Excel or LibreOffice\n\nstata files: use pd.read_dta()\nexcel files: use pd.read_excel() or xlsreader if unlucky\n\n\n\nComma separated file\n\ntxt = \"\"\"year,country,measure\n2018,\"france\",950.0\n2019,\"france\",960.0\n2020,\"france\",1000.0\n2018,\"usa\",2500.0\n2019,\"usa\",2150.0\n2020,\"usa\",2300.0\n\"\"\"\nopen('dummy_file.csv','w').write(txt) # we write it to a file\n\n136\n\n\n\ndf = pd.read_csv('dummy_file.csv') # what index should we use ?\ndf\n\n\n\n\n\n\n\n\nyear\ncountry\nmeasure\n\n\n\n\n0\n2018\nfrance\n950.0\n\n\n1\n2019\nfrance\n960.0\n\n\n2\n2020\nfrance\n1000.0\n\n\n3\n2018\nusa\n2500.0\n\n\n4\n2019\nusa\n2150.0\n\n\n5\n2020\nusa\n2300.0\n\n\n\n\n\n\n\n\n\n“Annoying” Comma Separated File\n\nSometimes, comma-separated files, are not quite comma-separated…\n\ninspect the file with a text editor to see what it contains\nadd options to pd.read_csv\n\n\n\ntxt = \"\"\"year;country;measure\n2018;\"france\";950.0\n2019;\"france\";960.0\n2020;\"france\";1000.0\n2018;\"usa\";2500.0\n2019;\"usa\";2150.0\n2020;\"usa\";2300.0\n\"\"\"\nopen('annoying_dummy_file.csv','w').write(txt) # we write it to a file\n\n136\n\n\n\npd.read_csv(\"annoying_dummy_file.csv\", sep=\";\")\n\n\n\n\n\n\n\n\nyear\ncountry\nmeasure\n\n\n\n\n0\n2018\nfrance\n950.0\n\n\n1\n2019\nfrance\n960.0\n\n\n2\n2020\nfrance\n1000.0\n\n\n3\n2018\nusa\n2500.0\n\n\n4\n2019\nusa\n2150.0\n\n\n5\n2020\nusa\n2300.0\n\n\n\n\n\n\n\n\n\nExporting a DataFrame\n\npandas can export to many formats: df.to_...\n\n\nprint( df.to_csv() )\n\n,year,country,measure\n0,2018,france,950.0\n1,2019,france,960.0\n2,2020,france,1000.0\n3,2018,usa,2500.0\n4,2019,usa,2150.0\n5,2020,usa,2300.0\n\n\n\n\ndf.to_stata('dummy_example.dta')"
  },
  {
    "objectID": "session_2/DataFrames.html#data-sources",
    "href": "session_2/DataFrames.html#data-sources",
    "title": "Data Frames",
    "section": "Data Sources",
    "text": "Data Sources\n\nTypes of Data Sources\n\nWhere can we get data from ?\nOfficial websites\n\noften in csv form\nunpractical applications\nsometimes unavoidable\nopen data trend: more unstructured data\n\nData providers\n\nsupply an API (i.e. easy to use function)"
  },
  {
    "objectID": "session_2/DataFrames.html#data-providers",
    "href": "session_2/DataFrames.html#data-providers",
    "title": "Data Frames",
    "section": "Data providers",
    "text": "Data providers\n\ncommercial ones:\n\nbloomberg, macrobond, factsets, quandl …\n\nfree ones:\n\ndbnomics: many official time-series\nqeds: databases used by quantecon\nvega-datasets: distributed with altair\ncovid*: lots of datasets…\n\nreminder: python packages, can be installed in the notebook with\n\n!pip install ...\n\n\n\n!pip install vega_datasets\n\nRequirement already satisfied: vega_datasets in /home/pablo/.local/opt/miniconda3/lib/python3.8/site-packages (0.9.0)\nRequirement already satisfied: pandas in /home/pablo/.local/opt/miniconda3/lib/python3.8/site-packages (from vega_datasets) (1.2.1)\nRequirement already satisfied: python-dateutil&gt;=2.7.3 in /home/pablo/.local/opt/miniconda3/lib/python3.8/site-packages (from pandas-&gt;vega_datasets) (2.8.1)\nRequirement already satisfied: pytz&gt;=2017.3 in /home/pablo/.local/opt/miniconda3/lib/python3.8/site-packages (from pandas-&gt;vega_datasets) (2020.5)\nRequirement already satisfied: numpy&gt;=1.16.5 in /home/pablo/.local/opt/miniconda3/lib/python3.8/site-packages (from pandas-&gt;vega_datasets) (1.19.5)\nRequirement already satisfied: six&gt;=1.5 in /home/pablo/.local/opt/miniconda3/lib/python3.8/site-packages (from python-dateutil&gt;=2.7.3-&gt;pandas-&gt;vega_datasets) (1.15.0)\n\n\n\nimport vega_datasets\ndf = vega_datasets.data('iris')\ndf\n\n\n\n\n\n\n\n\nsepalLength\nsepalWidth\npetalLength\npetalWidth\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n...\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\nvirginica\n\n\n146\n6.3\n2.5\n5.0\n1.9\nvirginica\n\n\n147\n6.5\n3.0\n5.2\n2.0\nvirginica\n\n\n148\n6.2\n3.4\n5.4\n2.3\nvirginica\n\n\n149\n5.9\n3.0\n5.1\n1.8\nvirginica\n\n\n\n\n150 rows × 5 columns\n\n\n\n\nInspecting data\n\nonce the data is loaded as df, we want to look at some basic properties:\n\ndf.head(5) # 5 first lines\ndf.tail(5) # 5 first lines\ndf.describe() # summary\ndf.mean() # averages\ndf.std() # standard deviations\n\n\n\ndf.head(2)\n\n\n\n\n\n\n\n\nsepalLength\nsepalWidth\npetalLength\npetalWidth\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n\n\n\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nsepalLength\nsepalWidth\npetalLength\npetalWidth\n\n\n\n\ncount\n150.000000\n150.000000\n150.000000\n150.000000\n\n\nmean\n5.843333\n3.057333\n3.758000\n1.199333\n\n\nstd\n0.828066\n0.435866\n1.765298\n0.762238\n\n\nmin\n4.300000\n2.000000\n1.000000\n0.100000\n\n\n25%\n5.100000\n2.800000\n1.600000\n0.300000\n\n\n50%\n5.800000\n3.000000\n4.350000\n1.300000\n\n\n75%\n6.400000\n3.300000\n5.100000\n1.800000\n\n\nmax\n7.900000\n4.400000\n6.900000\n2.500000\n\n\n\n\n\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nsepalLength\nsepalWidth\npetalLength\npetalWidth\n\n\n\n\ncount\n150.000000\n150.000000\n150.000000\n150.000000\n\n\nmean\n5.843333\n3.057333\n3.758000\n1.199333\n\n\nstd\n0.828066\n0.435866\n1.765298\n0.762238\n\n\nmin\n4.300000\n2.000000\n1.000000\n0.100000\n\n\n25%\n5.100000\n2.800000\n1.600000\n0.300000\n\n\n50%\n5.800000\n3.000000\n4.350000\n1.300000\n\n\n75%\n6.400000\n3.300000\n5.100000\n1.800000\n\n\nmax\n7.900000\n4.400000\n6.900000\n2.500000\n\n\n\n\n\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nsepalLength\nsepalWidth\npetalLength\npetalWidth\n\n\n\n\ncount\n150.000000\n150.000000\n150.000000\n150.000000\n\n\nmean\n5.843333\n3.057333\n3.758000\n1.199333\n\n\nstd\n0.828066\n0.435866\n1.765298\n0.762238\n\n\nmin\n4.300000\n2.000000\n1.000000\n0.100000\n\n\n25%\n5.100000\n2.800000\n1.600000\n0.300000\n\n\n50%\n5.800000\n3.000000\n4.350000\n1.300000\n\n\n75%\n6.400000\n3.300000\n5.100000\n1.800000\n\n\nmax\n7.900000\n4.400000\n6.900000\n2.500000"
  },
  {
    "objectID": "session_2/DataFrames.html#manipulating-dataframes",
    "href": "session_2/DataFrames.html#manipulating-dataframes",
    "title": "Data Frames",
    "section": "Manipulating DataFrames",
    "text": "Manipulating DataFrames\n\nColumns\nColumns are defined by attribute df.columns\n\ndf.columns\n\nIndex(['sepalLength', 'sepalWidth', 'petalLength', 'petalWidth', 'species'], dtype='object')\n\n\nThis attribute can be set\n\ndf.columns = ['sLength', 'sWidth', 'pLength', 'pWidth', 'species']\ndf.head(2)\n\n\n\n\n\n\n\n\nsLength\nsWidth\npLength\npWidth\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n\n\n\n\n\n\n\nIndexing a column\nA column can be extracted using its name as in a dictionary (like df['sLength'])\n\nseries = df['sWidth'] # note the resulting object: a series\nseries\n\n0      3.5\n1      3.0\n2      3.2\n3      3.1\n4      3.6\n      ... \n145    3.0\n146    2.5\n147    3.0\n148    3.4\n149    3.0\nName: sWidth, Length: 150, dtype: float64\n\n\n\nseries.plot()\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\n\nCreating a new column\n\ndf['totalLength'] = df['pLength'] + df['sLength']\ndf.head(2)\n\n\n\n\n\n\n\n\nsLength\nsWidth\npLength\npWidth\nspecies\ntotalLength\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n6.5\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n6.3\n\n\n\n\n\n\n\n\n\nReplacing a column\n\ndf['totalLength'] = df['pLength'] + df['sLength']*0.5\ndf.head(2)\n\n\n\n\n\n\n\n\nsLength\nsWidth\npLength\npWidth\nspecies\ntotalLength\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n3.95\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n3.85\n\n\n\n\n\n\n\n\n\nSelecting several columns\n\nIndex with a list of column names\n\n\ne = df[ ['pLength', 'sLength'] ]\ne.head(3)\n\n\n\n\n\n\n\n\npLength\nsLength\n\n\n\n\n0\n1.4\n5.1\n\n\n1\n1.4\n4.9\n\n\n2\n1.3\n4.7\n\n\n\n\n\n\n\n\n\nSelecting lines (1)\n\nuse index range\n\n\ndf[2:4]\n\n\n\n\n\n\n\n\nsLength\nsWidth\npLength\npWidth\nspecies\ntotalLength\n\n\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n3.65\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n3.80\n\n\n\n\n\n\n\n\n\nSelecting lines (2)\n\nuse boolean\n\n\ndf['species'].unique()\n\narray(['setosa', 'versicolor', 'virginica'], dtype=object)\n\n\n\nbool_ind = df['species'] == 'virginica' # this is a boolean serie\n\n\ne = df[ bool_ind ]\ne.head(4)\n\n\n\n\n\n\n\n\nsLength\nsWidth\npLength\npWidth\nspecies\ntotalLength\n\n\n\n\n100\n6.3\n3.3\n6.0\n2.5\nvirginica\n9.15\n\n\n101\n5.8\n2.7\n5.1\n1.9\nvirginica\n8.00\n\n\n102\n7.1\n3.0\n5.9\n2.1\nvirginica\n9.45\n\n\n103\n6.3\n2.9\n5.6\n1.8\nvirginica\n8.75\n\n\n\n\n\n\n\n\n\nSelecting lines and columns\n\nsometimes, one wants finer control about which lines and columns to select:\n\nuse df.loc[...] which can be indexed as a matrix\n\n\n\ndf.loc[0:4, 'species']\n\n0    setosa\n1    setosa\n2    setosa\n3    setosa\n4    setosa\nName: species, dtype: object\n\n\n\n\nCombine everything\n\n# Let's change the way totalLength is computed, only for 'virginica'\nindex = (df['species']=='virginica')\ndf.loc[index,'totalLength'] = df.loc[index,'sLength'] + 1.5*df[index]['pLength']"
  },
  {
    "objectID": "session_2/DataFrames.html#reshaping-dataframes",
    "href": "session_2/DataFrames.html#reshaping-dataframes",
    "title": "Data Frames",
    "section": "Reshaping DataFrames",
    "text": "Reshaping DataFrames\n\ntxt_wide = \"\"\"year,france,usa\n2018,950.0,2500.0\n2019,960.0,2150.0\n2020,1000.0,2300.0\n\"\"\"\nopen('dummy_file_wide.csv','w').write(txt_wide) # we write it to a file\n\n71\n\n\n\ntxt_long = \"\"\"year,country,measure\n2018,\"france\",950.0\n2019,\"france\",960.0\n2020,\"france\",1000.0\n2018,\"usa\",2500.0\n2019,\"usa\",2150.0\n2020,\"usa\",2300.0\n\"\"\"\nopen('dummy_file_long.csv','w').write(txt_long) # we write it to a file\n\n136\n\n\n\ndf_long = pd.read_csv(\"dummy_file_long.csv\")\ndf_wide = pd.read_csv(\"dummy_file_wide.csv\")\n\n\nWide vs Long format (1)\n\ncompare the following tables\n\n\ndf_wide\n\n\n\n\n\n\n\n\nyear\nfrance\nusa\n\n\n\n\n0\n2018\n950.0\n2500.0\n\n\n1\n2019\n960.0\n2150.0\n\n\n2\n2020\n1000.0\n2300.0\n\n\n\n\n\n\n\n\ndf_long\n\n\n\n\n\n\n\n\nyear\ncountry\nmeasure\n\n\n\n\n0\n2018\nfrance\n950.0\n\n\n1\n2019\nfrance\n960.0\n\n\n2\n2020\nfrance\n1000.0\n\n\n3\n2018\nusa\n2500.0\n\n\n4\n2019\nusa\n2150.0\n\n\n5\n2020\nusa\n2300.0\n\n\n\n\n\n\n\n\n\nWide vs Long format (2)\n\nin long format: each line is an independent observation\n\ntwo lines mayb belong to the same category (year, or country)\n\nin wide format: some observations are grouped\n\nin the example it is grouped by year\n\nboth representations are useful\n\n\n\nConverting from Wide to Long\n\ndf_wide.melt(id_vars='year')\n\n\n\n\n\n\n\n\nyear\nvariable\nvalue\n\n\n\n\n0\n2018\nfrance\n950.0\n\n\n1\n2019\nfrance\n960.0\n\n\n2\n2020\nfrance\n1000.0\n\n\n3\n2018\nusa\n2500.0\n\n\n4\n2019\nusa\n2150.0\n\n\n5\n2020\nusa\n2300.0\n\n\n\n\n\n\n\n\n\nConverting from Long to Wide\n\ndf_ = df_long.pivot(index='year', columns='country')\ndf_\n\n\n\n\n\n\n\n\nmeasure\n\n\ncountry\nfrance\nusa\n\n\nyear\n\n\n\n\n\n\n2018\n950.0\n2500.0\n\n\n2019\n960.0\n2150.0\n\n\n2020\n1000.0\n2300.0\n\n\n\n\n\n\n\n\n# the result of pivot has a \"hierarchical index\"\n# let's change columns names\ndf_.columns = df_.columns.get_level_values(1)\ndf_\n\n\n\n\n\n\n\ncountry\nfrance\nusa\n\n\nyear\n\n\n\n\n\n\n2018\n950.0\n2500.0\n\n\n2019\n960.0\n2150.0\n\n\n2020\n1000.0\n2300.0\n\n\n\n\n\n\n\n\n\ngroupby\ngroupby is a very powerful function which can be used to work directly on data in the long format.\n\ndf_long.groupby(\"country\").agg('mean')\n\nNameError: name 'df_long' is not defined"
  },
  {
    "objectID": "session_2/DataFrames.html#merging",
    "href": "session_2/DataFrames.html#merging",
    "title": "Data Frames",
    "section": "Merging",
    "text": "Merging\n\nMerging two dataframes\n\nSuppose we have two dataframes, with related observations\nHow can we construct one single database with all informations?\nAnswer:\n\nconcatenate if long format\nmerge databases if wide format\n\nLots of subtleties when data gets complicated\n\nwe’ll see them in due time\n\n\n\ntxt_long_1 = \"\"\"year,country,measure\n2018,\"france\",950.0\n2019,\"france\",960.0\n2020,\"france\",1000.0\n2018,\"usa\",2500.0\n2019,\"usa\",2150.0\n2020,\"usa\",2300.0\n\"\"\"\nopen(\"dummy_long_1.csv\",'w').write(txt_long_1)\n\n136\n\n\n\ntxt_long_2 = \"\"\"year,country,recipient\n2018,\"france\",maxime\n2019,\"france\",mauricette\n2020,\"france\",mathilde\n2018,\"usa\",sherlock\n2019,\"usa\",watson\n2020,\"usa\",moriarty\n\"\"\"\nopen(\"dummy_long_2.csv\",'w').write(txt_long_2)\n\n150\n\n\n\ndf_long_1 = pd.read_csv('dummy_long_1.csv')\ndf_long_2 = pd.read_csv('dummy_long_2.csv')\n\n\n\nMerging two DataFrames with pandas\n\ndf_long_1.merge(df_long_2)\n\nNameError: name 'df_long_1' is not defined"
  },
  {
    "objectID": "session_2/Exercises.html",
    "href": "session_2/Exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "Define a vector x with 1000 regularly spaced elements between 0 and 10\nDefine a vector y representing \\(y=sin(x)\\)\nDefine a vector y1 representing \\(y=abs(sin(x))\\)\nDefine a vector y2 representing \\(y=sin(x) \\text{if} y&gt;0.1 \\text{else} 0.1\\)\nPlot y, y1, y2 against x\n\n\n\n\n\nSet T=100, rho=0.9, sigma=0.01. We consider an autoregressive process \\(x_t=\\rho x_{t-1} + \\epsilon_t\\) where \\(\\epsilon_t\\) is normally distributed with standard deviation \\(\\sigma\\)\nCreate an empty vector x = np.zeros(T)\nLoop over t&gt;0 and fill x[t] so that x represents a simulation for t periods of process \\(x_t\\)\nUse function hpfilter from statsmodels (google it). It returns a and a residual\nPlot the simulated series, the filtered series and the residual"
  },
  {
    "objectID": "session_2/Exercises.html#numerical-python",
    "href": "session_2/Exercises.html#numerical-python",
    "title": "Exercises",
    "section": "",
    "text": "Define a vector x with 1000 regularly spaced elements between 0 and 10\nDefine a vector y representing \\(y=sin(x)\\)\nDefine a vector y1 representing \\(y=abs(sin(x))\\)\nDefine a vector y2 representing \\(y=sin(x) \\text{if} y&gt;0.1 \\text{else} 0.1\\)\nPlot y, y1, y2 against x\n\n\n\n\n\nSet T=100, rho=0.9, sigma=0.01. We consider an autoregressive process \\(x_t=\\rho x_{t-1} + \\epsilon_t\\) where \\(\\epsilon_t\\) is normally distributed with standard deviation \\(\\sigma\\)\nCreate an empty vector x = np.zeros(T)\nLoop over t&gt;0 and fill x[t] so that x represents a simulation for t periods of process \\(x_t\\)\nUse function hpfilter from statsmodels (google it). It returns a and a residual\nPlot the simulated series, the filtered series and the residual"
  },
  {
    "objectID": "session_2/Exercises.html#iris-data-set",
    "href": "session_2/Exercises.html#iris-data-set",
    "title": "Exercises",
    "section": "Iris Data Set",
    "text": "Iris Data Set\nYou will need the library vega_datasets and the altair library. You can install them with !pip install vega_datasets and !pip install altair Load the iris database.\n\nPrint statistics (mean, std), by flower, for each characteristics.\nUse matplotlib to make correlation plots, betwen flowers characteristics. (for instance, plot sepalWidth against sepalLength. Ideally, use different shapes or colors for various flowers.\nConvert the database to long format\nUse altair, to plot correlation between two characteristics, with different color for each flower. Plot all correlations."
  },
  {
    "objectID": "session_2/Exercises.html#philips-curve-and-okuns-law",
    "href": "session_2/Exercises.html#philips-curve-and-okuns-law",
    "title": "Exercises",
    "section": "Philips Curve and Okun’s Law",
    "text": "Philips Curve and Okun’s Law\nIf needed, install dbnomics with !pip install dbnomics.\n\nDownload inflation, unemployment and gdp series from France.\nCompute growth rate of gdp.\nPlot two graphs two verify graphically the Phillips curve (unemployment against inflation) and Okun’s law (unemployment against output)."
  },
  {
    "objectID": "session_2/Exercises_correction.html",
    "href": "session_2/Exercises_correction.html",
    "title": "Exercises",
    "section": "",
    "text": "Define a vector x with 1000 regularly spaced elements between 0 and 10\n\n\nimport numpy as np\nx = np.linspace(0,10, 1000)\n\n\nDefine a vector y representing \\(y=sin(x)\\)\n\n\ny = np.sin(x)\n\n\nDefine a vector y1 representing \\(y=abs(sin(x))\\)\n\n\ny1 = np.abs(np.sin(x))\n\n\nDefine a vector y2 representing \\(y=sin(x) \\text{if} y&gt;0.1 \\text{else} 0.1\\)\n\n\ncond = (y&gt;0.1)\n\n\ny2 = np.sin(x)*cond + 0.1*(~cond)\n\n\nPlot y, y1, y2 against x\n\n\nfrom matplotlib import pyplot as plt\n\nplt.plot(x,y, label=\"y\")\nplt.plot(x,y1, label='y1')\nplt.plot(x,y2, label='y2')\nplt.grid(True)\nplt.legend(loc='lower right')\n\n&lt;matplotlib.legend.Legend at 0x7fc33d86d8e0&gt;\n\n\n\n\n\n\n\n\n\nSet T=100, rho=0.9, sigma=0.01. We consider an autoregressive process \\(x_t=\\rho x_{t-1} + \\epsilon_t\\) where \\(\\epsilon_t\\) is normally distributed with standard deviation \\(\\sigma\\)\n\n\nT = 100\nrho = 0.9\nsigma = 0.01\n\n\nCreate an empty vector x = np.zeros(T)\n\n\nimport numpy as np\nx = np.zeros(T)\nx\n\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n\n\n\nLoop over t&gt;0 and fill x[t] so that x represents a simulation for t periods of process \\(x_t\\)\n\n\nnp.random.normal(scale=sigma)\nnp.random.randn()*sigma\n\n-0.012179445299749143\n\n\n\nfor t in range(1, T):\n    # press tabulation to indent\n    ϵ = np.random.randn()*sigma # random normal variable with standard deviation sigma (google numpy random variable)\n    x[t] = rho*x[t-1] + ϵ\n\n\nfrom matplotlib import pyplot as plt\nplt.plot(x)\n\n\n\n\n\nUse function hpfilter from statsmodels (google it). It returns a and a residual\n\n\nfrom statsmodels.tsa.filters.hp_filter import hpfilter\ncycle, trend = hpfilter(x)\n\n#cycle is residual\n\n\nPlot the simulated series, the filtered series and the residual\n\n\n\nplt.subplot(2,1,1)\nplt.plot(x, label='data')\nplt.plot(trend, label='trend')\nplt.legend()\nplt.subplot(2,1,2)\nplt.plot(cycle, label='cycle (residual)')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7fc32bc7cfa0&gt;"
  },
  {
    "objectID": "session_2/Exercises_correction.html#numerical-python",
    "href": "session_2/Exercises_correction.html#numerical-python",
    "title": "Exercises",
    "section": "",
    "text": "Define a vector x with 1000 regularly spaced elements between 0 and 10\n\n\nimport numpy as np\nx = np.linspace(0,10, 1000)\n\n\nDefine a vector y representing \\(y=sin(x)\\)\n\n\ny = np.sin(x)\n\n\nDefine a vector y1 representing \\(y=abs(sin(x))\\)\n\n\ny1 = np.abs(np.sin(x))\n\n\nDefine a vector y2 representing \\(y=sin(x) \\text{if} y&gt;0.1 \\text{else} 0.1\\)\n\n\ncond = (y&gt;0.1)\n\n\ny2 = np.sin(x)*cond + 0.1*(~cond)\n\n\nPlot y, y1, y2 against x\n\n\nfrom matplotlib import pyplot as plt\n\nplt.plot(x,y, label=\"y\")\nplt.plot(x,y1, label='y1')\nplt.plot(x,y2, label='y2')\nplt.grid(True)\nplt.legend(loc='lower right')\n\n&lt;matplotlib.legend.Legend at 0x7fc33d86d8e0&gt;\n\n\n\n\n\n\n\n\n\nSet T=100, rho=0.9, sigma=0.01. We consider an autoregressive process \\(x_t=\\rho x_{t-1} + \\epsilon_t\\) where \\(\\epsilon_t\\) is normally distributed with standard deviation \\(\\sigma\\)\n\n\nT = 100\nrho = 0.9\nsigma = 0.01\n\n\nCreate an empty vector x = np.zeros(T)\n\n\nimport numpy as np\nx = np.zeros(T)\nx\n\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n\n\n\nLoop over t&gt;0 and fill x[t] so that x represents a simulation for t periods of process \\(x_t\\)\n\n\nnp.random.normal(scale=sigma)\nnp.random.randn()*sigma\n\n-0.012179445299749143\n\n\n\nfor t in range(1, T):\n    # press tabulation to indent\n    ϵ = np.random.randn()*sigma # random normal variable with standard deviation sigma (google numpy random variable)\n    x[t] = rho*x[t-1] + ϵ\n\n\nfrom matplotlib import pyplot as plt\nplt.plot(x)\n\n\n\n\n\nUse function hpfilter from statsmodels (google it). It returns a and a residual\n\n\nfrom statsmodels.tsa.filters.hp_filter import hpfilter\ncycle, trend = hpfilter(x)\n\n#cycle is residual\n\n\nPlot the simulated series, the filtered series and the residual\n\n\n\nplt.subplot(2,1,1)\nplt.plot(x, label='data')\nplt.plot(trend, label='trend')\nplt.legend()\nplt.subplot(2,1,2)\nplt.plot(cycle, label='cycle (residual)')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7fc32bc7cfa0&gt;"
  },
  {
    "objectID": "session_2/Exercises_correction.html#iris-data-set",
    "href": "session_2/Exercises_correction.html#iris-data-set",
    "title": "Exercises",
    "section": "Iris Data Set",
    "text": "Iris Data Set\nYou will need the library vega_datasets and the altair library. You can install them with !pip install vega_datasets and !pip install altair Load the iris database.\n\n# uncomment and run the following if vega_datasets is not already installed\n# !pip install vega_datasets # on linux\n# pip install vega_datasets # try if the former doesn't work\n\n\n# uncomment and run the following if altair is not already installed\n#!pip install altair\n\n\nPrint statistics (mean, std), by flower, for each characteristics.\n\n\n# we start by importing the library\nimport vega_datasets\n\n\ndf = vega_datasets.data.iris()\ndf\n\n\n\n\n\n\n\n\nsepalLength\nsepalWidth\npetalLength\npetalWidth\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n...\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\nvirginica\n\n\n146\n6.3\n2.5\n5.0\n1.9\nvirginica\n\n\n147\n6.5\n3.0\n5.2\n2.0\nvirginica\n\n\n148\n6.2\n3.4\n5.4\n2.3\nvirginica\n\n\n149\n5.9\n3.0\n5.1\n1.8\nvirginica\n\n\n\n\n150 rows × 5 columns\n\n\n\n\n# we can print a summary for the whole database\ndf.describe()\n\n\n\n\n\n\n\n\nsepalLength\nsepalWidth\npetalLength\npetalWidth\n\n\n\n\ncount\n150.000000\n150.000000\n150.000000\n150.000000\n\n\nmean\n5.843333\n3.057333\n3.758000\n1.199333\n\n\nstd\n0.828066\n0.435866\n1.765298\n0.762238\n\n\nmin\n4.300000\n2.000000\n1.000000\n0.100000\n\n\n25%\n5.100000\n2.800000\n1.600000\n0.300000\n\n\n50%\n5.800000\n3.000000\n4.350000\n1.300000\n\n\n75%\n6.400000\n3.300000\n5.100000\n1.800000\n\n\nmax\n7.900000\n4.400000\n6.900000\n2.500000\n\n\n\n\n\n\n\n\n# but this mixes all kinds of flowers\n# here is how we do it for the 'setosa' type:\ndf[df['species']=='setosa'].describe()\n\n\n\n\n\n\n\n\nsepalLength\nsepalWidth\npetalLength\npetalWidth\n\n\n\n\ncount\n50.00000\n50.000000\n50.000000\n50.000000\n\n\nmean\n5.00600\n3.428000\n1.462000\n0.246000\n\n\nstd\n0.35249\n0.379064\n0.173664\n0.105386\n\n\nmin\n4.30000\n2.300000\n1.000000\n0.100000\n\n\n25%\n4.80000\n3.200000\n1.400000\n0.200000\n\n\n50%\n5.00000\n3.400000\n1.500000\n0.200000\n\n\n75%\n5.20000\n3.675000\n1.575000\n0.300000\n\n\nmax\n5.80000\n4.400000\n1.900000\n0.600000\n\n\n\n\n\n\n\n\n# or we can print the statistics for all species (here we do it for the mean)\nfor spec in ['setosa', 'virginica', 'versicolor']:\n    print(f\"\\nMean for: '{spec}'\")\n    m = df[df['species']==spec].mean()\n    print(m)\n\n\nMean for: 'setosa'\nsepalLength    5.006\nsepalWidth     3.428\npetalLength    1.462\npetalWidth     0.246\ndtype: float64\n\nMean for: 'virginica'\nsepalLength    6.588\nsepalWidth     2.974\npetalLength    5.552\npetalWidth     2.026\ndtype: float64\n\nMean for: 'versicolor'\nsepalLength    5.936\nsepalWidth     2.770\npetalLength    4.260\npetalWidth     1.326\ndtype: float64\n\n\n\n# the same result can be obtained using pandas' groubpy function\ndf.groupby('species').apply( lambda x: x.mean())\n\n\n\n\n\n\n\n\nsepalLength\nsepalWidth\npetalLength\npetalWidth\n\n\nspecies\n\n\n\n\n\n\n\n\nsetosa\n5.006\n3.428\n1.462\n0.246\n\n\nversicolor\n5.936\n2.770\n4.260\n1.326\n\n\nvirginica\n6.588\n2.974\n5.552\n2.026\n\n\n\n\n\n\n\n\n# same for the standard deviation\ndf.groupby('species').apply( lambda x: x.std())\n\n\n\n\n\n\n\n\nsepalLength\nsepalWidth\npetalLength\npetalWidth\n\n\nspecies\n\n\n\n\n\n\n\n\nsetosa\n0.352490\n0.379064\n0.173664\n0.105386\n\n\nversicolor\n0.516171\n0.313798\n0.469911\n0.197753\n\n\nvirginica\n0.635880\n0.322497\n0.551895\n0.274650\n\n\n\n\n\n\n\n\n# we can get all statistics at once, by group, with .describe\ndf.groupby('species').apply( lambda x: x.describe())\n\n\n\n\n\n\n\n\n\nsepalLength\nsepalWidth\npetalLength\npetalWidth\n\n\nspecies\n\n\n\n\n\n\n\n\n\nsetosa\ncount\n50.000000\n50.000000\n50.000000\n50.000000\n\n\nmean\n5.006000\n3.428000\n1.462000\n0.246000\n\n\nstd\n0.352490\n0.379064\n0.173664\n0.105386\n\n\nmin\n4.300000\n2.300000\n1.000000\n0.100000\n\n\n25%\n4.800000\n3.200000\n1.400000\n0.200000\n\n\n50%\n5.000000\n3.400000\n1.500000\n0.200000\n\n\n75%\n5.200000\n3.675000\n1.575000\n0.300000\n\n\nmax\n5.800000\n4.400000\n1.900000\n0.600000\n\n\nversicolor\ncount\n50.000000\n50.000000\n50.000000\n50.000000\n\n\nmean\n5.936000\n2.770000\n4.260000\n1.326000\n\n\nstd\n0.516171\n0.313798\n0.469911\n0.197753\n\n\nmin\n4.900000\n2.000000\n3.000000\n1.000000\n\n\n25%\n5.600000\n2.525000\n4.000000\n1.200000\n\n\n50%\n5.900000\n2.800000\n4.350000\n1.300000\n\n\n75%\n6.300000\n3.000000\n4.600000\n1.500000\n\n\nmax\n7.000000\n3.400000\n5.100000\n1.800000\n\n\nvirginica\ncount\n50.000000\n50.000000\n50.000000\n50.000000\n\n\nmean\n6.588000\n2.974000\n5.552000\n2.026000\n\n\nstd\n0.635880\n0.322497\n0.551895\n0.274650\n\n\nmin\n4.900000\n2.200000\n4.500000\n1.400000\n\n\n25%\n6.225000\n2.800000\n5.100000\n1.800000\n\n\n50%\n6.500000\n3.000000\n5.550000\n2.000000\n\n\n75%\n6.900000\n3.175000\n5.875000\n2.300000\n\n\nmax\n7.900000\n3.800000\n6.900000\n2.500000\n\n\n\n\n\n\n\n\nUse matplotlib to make correlation plots, betwen flowers characteristics. (for instance, plot sepalWidth against sepalLength. Ideally, use different shapes or colors for various flowers.\n\nFirst, let’s do the correlation plot for one pair of two characteristics and one species type.\n\n# we need to import the plotting library:\nfrom matplotlib import pyplot as plt\n\n\n# we do it for setosa\nddf = df[df['species']=='setosa'] # extract subdataframe where species=='setosa'\nplt.plot(ddf['sepalLength'], ddf['sepalWidth'], 'o', label=spec)\nplt.xlabel(\"sepalLength\")\nplt.ylabel(\"sepalWidth\")\n\nText(0, 0.5, 'sepalWidth')\n\n\n\n\n\nHere is how we can plot the same plot for all species on the same graph. Not that matplotlib chooses a new color by default, for each new call to function plot()\n\n# let's get a list of all species\nspecies = df['species'].unique()\nspecies\n\narray(['setosa', 'versicolor', 'virginica'], dtype=object)\n\n\n\nfor spec in species:\n    ddf = df[df['species']==spec]\n    plt.plot(ddf['sepalLength'], ddf['sepalWidth'], 'o', label=spec)\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7fc32baf4b20&gt;\n\n\n\n\n\nNow we can produce the full graph. We use the subplots function to arrange the graphs on a 4x4 grid.\n\n# we compute the list of characteristics from the columns of the tables \ncharacteristics = df.columns[:4] # we ignore the 5th column which is 'species'\ncharacteristics\n\nIndex(['sepalLength', 'sepalWidth', 'petalLength', 'petalWidth'], dtype='object')\n\n\n\nplt.figure(figsize=(16,16))\n# we loop over lines (i from 0 to 3)\nfor i in range(4):\n    ch_i = characteristics[i]\n    # we loop over columns (j from 0 to 3)\n    for j in range(4):\n        ch_j = characteristics[j]\n\n        # create the subplot \n        # we compute the position of the current subplot (goes from 1 to 16)\n        position = i*4 + j + 1\n        plt.subplot(4, 4, position)\n        for spec in species:\n            ddf = df[df['species']==spec]\n            plt.plot(ddf[ch_i], ddf[ch_j], 'o', label=spec)\n            plt.xlabel(ch_i)\n            plt.ylabel(ch_j)\nplt.tight_layout()\n#         plt.legend()\n\n\n\n\n\nConvert the database to long format\n\n\ndf_long = df.melt(value_vars=['sepalLength','sepalWidth','petalLength','petalWidth'], \n                  id_vars=\"species\" )\ndf_long.head()\n\n\n\n\n\n\n\n\nspecies\nvariable\nvalue\n\n\n\n\n0\nsetosa\nsepalLength\n5.1\n\n\n1\nsetosa\nsepalLength\n4.9\n\n\n2\nsetosa\nsepalLength\n4.7\n\n\n3\nsetosa\nsepalLength\n4.6\n\n\n4\nsetosa\nsepalLength\n5.0\n\n\n\n\n\n\n\n\nUse altair, to plot correlation between two characteristics, with different color for each flower. Plot all correlations.\n\nAn introduction about how to use altair is on youtube: Altair Otherwise, the online doc is very useful and complete. It has many demos that can be adapted to your need: demos\n\nimport altair as alt\n\n\nch = alt.Chart(df).mark_point().encode(\n    x='sepalWidth',\n    y='petalWidth',\n    color='species',\n)\nch\n\n\n\n\n\n\nTo plot all correlations, best practice is to use altair’s repeat function. Note that the result is an interactive graph where all subplots move in a synchronized way. This is a typical feature of “visualization” libraries.\n\nalt.Chart(df).mark_point().encode(\n    alt.X(alt.repeat(\"column\"), type='quantitative'),\n    alt.Y(alt.repeat(\"row\"), type='quantitative'),\n    color='species:N'\n).properties(\n    width=200,\n    height=200\n).repeat(\n    row=['petalLength', 'petalWidth','sepalLength', 'sepalWidth'],\n    column=['petalLength', 'petalWidth','sepalLength', 'sepalWidth']\n).interactive()"
  },
  {
    "objectID": "session_2/Exercises_correction.html#philips-curve-and-okuns-law",
    "href": "session_2/Exercises_correction.html#philips-curve-and-okuns-law",
    "title": "Exercises",
    "section": "Philips Curve and Okun’s Law",
    "text": "Philips Curve and Okun’s Law\nIf needed, install dbnomics with !pip install dbnomics.\n\nDownload inflation, unemployment and gdp series from France.\n\nThere is a clear tutorial on how to use dbnomics available from Quantecon. There are two ways to import a dbnomics series:\n\nuse api link\nuse the organization/database/series identifiers\n\nDownload with the API was broken when we tried in class (the website returned an incorrect json file), so we will use the second method. (update: this seems to be fixed now)\nOn the dbnomics website we search for “inflation france” and decide to use OECD database. We eventually obtain the following page:\n\nFrom this page we obtain the series identifier: OECD/MEI/FRA.CPALTT01.CTGY.M\nIt is split in three parts: - organization: OECD - database: KEY (Key Economic Indicators) - series: CPALTT01.FRA.GY.A\nWe use these elements to import a series with dbnomics:\n\nimport dbnomics\ndf_inflation = dbnomics.fetch_series('OECD', 'KEI', 'CPALTT01.FRA.GY.A')\ndf_inflation.head()\n\n\n\n\n\n\n\n\n@frequency\nprovider_code\ndataset_code\ndataset_name\nseries_code\nseries_name\noriginal_period\nperiod\noriginal_value\nvalue\nSUBJECT\nLOCATION\nMEASURE\nFREQUENCY\nSubject\nCountry\nMeasure\nFrequency\n\n\n\n\n0\nannual\nOECD\nKEI\nKey Short-Term Economic Indicators\nCPALTT01.FRA.GY.A\nConsumer prices: all items – France – Growth o...\n1956\n1956-01-01\n1.897315\n1.897315\nCPALTT01\nFRA\nGY\nA\nConsumer prices: all items\nFrance\nGrowth on the same period of the previous year\nAnnual\n\n\n1\nannual\nOECD\nKEI\nKey Short-Term Economic Indicators\nCPALTT01.FRA.GY.A\nConsumer prices: all items – France – Growth o...\n1957\n1957-01-01\n3.057669\n3.057669\nCPALTT01\nFRA\nGY\nA\nConsumer prices: all items\nFrance\nGrowth on the same period of the previous year\nAnnual\n\n\n2\nannual\nOECD\nKEI\nKey Short-Term Economic Indicators\nCPALTT01.FRA.GY.A\nConsumer prices: all items – France – Growth o...\n1958\n1958-01-01\n15.260526\n15.260526\nCPALTT01\nFRA\nGY\nA\nConsumer prices: all items\nFrance\nGrowth on the same period of the previous year\nAnnual\n\n\n3\nannual\nOECD\nKEI\nKey Short-Term Economic Indicators\nCPALTT01.FRA.GY.A\nConsumer prices: all items – France – Growth o...\n1959\n1959-01-01\n5.815255\n5.815255\nCPALTT01\nFRA\nGY\nA\nConsumer prices: all items\nFrance\nGrowth on the same period of the previous year\nAnnual\n\n\n4\nannual\nOECD\nKEI\nKey Short-Term Economic Indicators\nCPALTT01.FRA.GY.A\nConsumer prices: all items – France – Growth o...\n1960\n1960-01-01\n4.139938\n4.139938\nCPALTT01\nFRA\nGY\nA\nConsumer prices: all items\nFrance\nGrowth on the same period of the previous year\nAnnual\n\n\n\n\n\n\n\n\n# equivalent:\n# df_inflation = dbnomics.fetch_series_by_api_link(...)\n# df_inflation\n\n\n# let's check it is not empty:\ndf_inflation.head()\n\n\n\n\n\n\n\n\n@frequency\nprovider_code\ndataset_code\ndataset_name\nseries_code\nseries_name\noriginal_period\nperiod\noriginal_value\nvalue\nSUBJECT\nLOCATION\nMEASURE\nFREQUENCY\nSubject\nCountry\nMeasure\nFrequency\n\n\n\n\n0\nannual\nOECD\nKEI\nKey Short-Term Economic Indicators\nCPALTT01.FRA.GY.A\nConsumer prices: all items – France – Growth o...\n1956\n1956-01-01\n1.897315\n1.897315\nCPALTT01\nFRA\nGY\nA\nConsumer prices: all items\nFrance\nGrowth on the same period of the previous year\nAnnual\n\n\n1\nannual\nOECD\nKEI\nKey Short-Term Economic Indicators\nCPALTT01.FRA.GY.A\nConsumer prices: all items – France – Growth o...\n1957\n1957-01-01\n3.057669\n3.057669\nCPALTT01\nFRA\nGY\nA\nConsumer prices: all items\nFrance\nGrowth on the same period of the previous year\nAnnual\n\n\n2\nannual\nOECD\nKEI\nKey Short-Term Economic Indicators\nCPALTT01.FRA.GY.A\nConsumer prices: all items – France – Growth o...\n1958\n1958-01-01\n15.260526\n15.260526\nCPALTT01\nFRA\nGY\nA\nConsumer prices: all items\nFrance\nGrowth on the same period of the previous year\nAnnual\n\n\n3\nannual\nOECD\nKEI\nKey Short-Term Economic Indicators\nCPALTT01.FRA.GY.A\nConsumer prices: all items – France – Growth o...\n1959\n1959-01-01\n5.815255\n5.815255\nCPALTT01\nFRA\nGY\nA\nConsumer prices: all items\nFrance\nGrowth on the same period of the previous year\nAnnual\n\n\n4\nannual\nOECD\nKEI\nKey Short-Term Economic Indicators\nCPALTT01.FRA.GY.A\nConsumer prices: all items – France – Growth o...\n1960\n1960-01-01\n4.139938\n4.139938\nCPALTT01\nFRA\nGY\nA\nConsumer prices: all items\nFrance\nGrowth on the same period of the previous year\nAnnual\n\n\n\n\n\n\n\n\n# we see the column associated with the values is called 'value'\n# the one associated with date is called `period`\ndisplay( df_inflation['value'].head() )\ndisplay( df_inflation['period'].head() )\n\n0     1.897315\n1     3.057669\n2    15.260526\n3     5.815255\n4     4.139938\nName: value, dtype: float64\n\n\n0   1956-01-01\n1   1957-01-01\n2   1958-01-01\n3   1959-01-01\n4   1960-01-01\nName: period, dtype: datetime64[ns]\n\n\n\nplt.plot(df_inflation['period'],df_inflation['value'])\n\n\n\n\nWe follow the same steps for unemployment and gdp. For the sake of simplicity, we choose annual frequency for all series. Not that the series on unemployment starts on only in 2004. That will be enough for the current purpose.\n\n# we proceed similarly for unemployment and gdp\ndf_gdp = dbnomics.fetch_series('OECD', 'MEI', 'FRA.NAEXCP01.STSA.A')\ndf_unemployment = dbnomics.fetch_series('OECD', 'CSPCUBE', 'UNEMPLRT_T1C.FRA')\n\n\n# let's look at what we have\nplt.figure(figsize=(10,5))\nplt.subplot(131)\nplt.plot(df_unemployment['period'], df_unemployment['value'])\nplt.title('unemployment')\nplt.subplot(132)\nplt.plot(df_gdp['period'], df_gdp['value'])\nplt.title('gdp')\nplt.subplot(133)\nplt.plot(df_inflation['period'], df_inflation['value'])\nplt.title(\"inflation\")\nplt.tight_layout()\n\n\n\n\n\n# before we proceed, let's create some new columns to avoid conflicts\ndf_inflation['inflation'] = df_inflation['value']\ndf_inflation.head()\n\n\n\n\n\n\n\n\n@frequency\nprovider_code\ndataset_code\ndataset_name\nseries_code\nseries_name\noriginal_period\nperiod\noriginal_value\nvalue\nSUBJECT\nLOCATION\nMEASURE\nFREQUENCY\nSubject\nCountry\nMeasure\nFrequency\ninflation\n\n\n\n\n0\nannual\nOECD\nKEI\nKey Short-Term Economic Indicators\nCPALTT01.FRA.GY.A\nConsumer prices: all items – France – Growth o...\n1956\n1956-01-01\n1.897315\n1.897315\nCPALTT01\nFRA\nGY\nA\nConsumer prices: all items\nFrance\nGrowth on the same period of the previous year\nAnnual\n1.897315\n\n\n1\nannual\nOECD\nKEI\nKey Short-Term Economic Indicators\nCPALTT01.FRA.GY.A\nConsumer prices: all items – France – Growth o...\n1957\n1957-01-01\n3.057669\n3.057669\nCPALTT01\nFRA\nGY\nA\nConsumer prices: all items\nFrance\nGrowth on the same period of the previous year\nAnnual\n3.057669\n\n\n2\nannual\nOECD\nKEI\nKey Short-Term Economic Indicators\nCPALTT01.FRA.GY.A\nConsumer prices: all items – France – Growth o...\n1958\n1958-01-01\n15.260526\n15.260526\nCPALTT01\nFRA\nGY\nA\nConsumer prices: all items\nFrance\nGrowth on the same period of the previous year\nAnnual\n15.260526\n\n\n3\nannual\nOECD\nKEI\nKey Short-Term Economic Indicators\nCPALTT01.FRA.GY.A\nConsumer prices: all items – France – Growth o...\n1959\n1959-01-01\n5.815255\n5.815255\nCPALTT01\nFRA\nGY\nA\nConsumer prices: all items\nFrance\nGrowth on the same period of the previous year\nAnnual\n5.815255\n\n\n4\nannual\nOECD\nKEI\nKey Short-Term Economic Indicators\nCPALTT01.FRA.GY.A\nConsumer prices: all items – France – Growth o...\n1960\n1960-01-01\n4.139938\n4.139938\nCPALTT01\nFRA\nGY\nA\nConsumer prices: all items\nFrance\nGrowth on the same period of the previous year\nAnnual\n4.139938\n\n\n\n\n\n\n\n\ndf_gdp['gdp'] = df_gdp['value']\ndf_unemployment['unemployment'] = df_unemployment['value']\n\n\n# the following table contains both gdp and inflation\n# note that when there was any ambiguity_x suffixes were added to the gdp table, and _y suffixes added to inflation table\n# this is why we added another column with the good name.\n# as for the period column, since it had the same meaning for both tables, it is not renamed\nddf = df_gdp.merge(df_inflation, on='period')\nddf.head()\n\n\n\n\n\n\n\n\n@frequency_x\nprovider_code_x\ndataset_code_x\ndataset_name_x\nseries_code_x\nseries_name_x\noriginal_period_x\nperiod\noriginal_value_x\nvalue_x\n...\nvalue_y\nSUBJECT_y\nLOCATION_y\nMEASURE_y\nFREQUENCY_y\nSubject_y\nCountry_y\nMeasure_y\nFrequency_y\ninflation\n\n\n\n\n0\nannual\nOECD\nMEI\nMain Economic Indicators Publication\nFRA.NAEXCP01.STSA.A\nFrance – National Accounts &gt; GDP by Expenditur...\n1980\n1980-01-01\n451.772\n451.772\n...\n13.562578\nCPALTT01\nFRA\nGY\nA\nConsumer prices: all items\nFrance\nGrowth on the same period of the previous year\nAnnual\n13.562578\n\n\n1\nannual\nOECD\nMEI\nMain Economic Indicators Publication\nFRA.NAEXCP01.STSA.A\nFrance – National Accounts &gt; GDP by Expenditur...\n1981\n1981-01-01\n509.984\n509.984\n...\n13.314400\nCPALTT01\nFRA\nGY\nA\nConsumer prices: all items\nFrance\nGrowth on the same period of the previous year\nAnnual\n13.314400\n\n\n2\nannual\nOECD\nMEI\nMain Economic Indicators Publication\nFRA.NAEXCP01.STSA.A\nFrance – National Accounts &gt; GDP by Expenditur...\n1982\n1982-01-01\n585.990\n585.990\n...\n11.978476\nCPALTT01\nFRA\nGY\nA\nConsumer prices: all items\nFrance\nGrowth on the same period of the previous year\nAnnual\n11.978476\n\n\n3\nannual\nOECD\nMEI\nMain Economic Indicators Publication\nFRA.NAEXCP01.STSA.A\nFrance – National Accounts &gt; GDP by Expenditur...\n1983\n1983-01-01\n650.514\n650.514\n...\n9.459548\nCPALTT01\nFRA\nGY\nA\nConsumer prices: all items\nFrance\nGrowth on the same period of the previous year\nAnnual\n9.459548\n\n\n4\nannual\nOECD\nMEI\nMain Economic Indicators Publication\nFRA.NAEXCP01.STSA.A\nFrance – National Accounts &gt; GDP by Expenditur...\n1984\n1984-01-01\n707.030\n707.030\n...\n7.673803\nCPALTT01\nFRA\nGY\nA\nConsumer prices: all items\nFrance\nGrowth on the same period of the previous year\nAnnual\n7.673803\n\n\n\n\n5 rows × 37 columns\n\n\n\n\nddf.columns\n\nIndex(['@frequency_x', 'provider_code_x', 'dataset_code_x', 'dataset_name_x',\n       'series_code_x', 'series_name_x', 'original_period_x', 'period',\n       'original_value_x', 'value_x', 'LOCATION_x', 'SUBJECT_x', 'MEASURE_x',\n       'FREQUENCY_x', 'Country_x', 'Subject_x', 'Measure_x', 'Frequency_x',\n       'gdp', '@frequency_y', 'provider_code_y', 'dataset_code_y',\n       'dataset_name_y', 'series_code_y', 'series_name_y', 'original_period_y',\n       'original_value_y', 'value_y', 'SUBJECT_y', 'LOCATION_y', 'MEASURE_y',\n       'FREQUENCY_y', 'Subject_y', 'Country_y', 'Measure_y', 'Frequency_y',\n       'inflation'],\n      dtype='object')\n\n\n\n# plot time series:\nplt.figure(figsize=(12,5))\nplt.subplot(121)\nplt.plot(ddf['period'], ddf['gdp'])\nplt.title(\"gdp\")\nplt.subplot(122)\nplt.plot(ddf['period'], ddf['inflation'])\nplt.title('inflation')\nplt.tight_layout()\n\n\n\n\n\n# let's add unemployment too\nddf = ddf.merge(df_unemployment, on='period')\nddf.head()\n\n\n\n\n\n\n\n\n@frequency_x\nprovider_code_x\ndataset_code_x\ndataset_name_x\nseries_code_x\nseries_name_x\noriginal_period_x\nperiod\noriginal_value_x\nvalue_x\n...\nseries_code\nseries_name\noriginal_period\noriginal_value\nvalue\nSUB\nLOCATION\nSubject\nCountry\nunemployment\n\n\n\n\n0\nannual\nOECD\nMEI\nMain Economic Indicators Publication\nFRA.NAEXCP01.STSA.A\nFrance – National Accounts &gt; GDP by Expenditur...\n2003\n2003-01-01\n1630.666\n1630.666\n...\nUNEMPLRT_T1C.FRA\nUnemployment rates: total – France\n2003\n8.099563\n8.099563\nUNEMPLRT_T1C\nFRA\nUnemployment rates: total\nFrance\n8.099563\n\n\n1\nannual\nOECD\nMEI\nMain Economic Indicators Publication\nFRA.NAEXCP01.STSA.A\nFrance – National Accounts &gt; GDP by Expenditur...\n2004\n2004-01-01\n1704.017\n1704.017\n...\nUNEMPLRT_T1C.FRA\nUnemployment rates: total – France\n2004\n8.468398\n8.468398\nUNEMPLRT_T1C\nFRA\nUnemployment rates: total\nFrance\n8.468398\n\n\n2\nannual\nOECD\nMEI\nMain Economic Indicators Publication\nFRA.NAEXCP01.STSA.A\nFrance – National Accounts &gt; GDP by Expenditur...\n2005\n2005-01-01\n1765.903\n1765.903\n...\nUNEMPLRT_T1C.FRA\nUnemployment rates: total – France\n2005\n8.493855\n8.493855\nUNEMPLRT_T1C\nFRA\nUnemployment rates: total\nFrance\n8.493855\n\n\n3\nannual\nOECD\nMEI\nMain Economic Indicators Publication\nFRA.NAEXCP01.STSA.A\nFrance – National Accounts &gt; GDP by Expenditur...\n2006\n2006-01-01\n1848.150\n1848.150\n...\nUNEMPLRT_T1C.FRA\nUnemployment rates: total – France\n2006\n8.449007\n8.449007\nUNEMPLRT_T1C\nFRA\nUnemployment rates: total\nFrance\n8.449007\n\n\n4\nannual\nOECD\nMEI\nMain Economic Indicators Publication\nFRA.NAEXCP01.STSA.A\nFrance – National Accounts &gt; GDP by Expenditur...\n2007\n2007-01-01\n1941.361\n1941.361\n...\nUNEMPLRT_T1C.FRA\nUnemployment rates: total – France\n2007\n7.658579\n7.658579\nUNEMPLRT_T1C\nFRA\nUnemployment rates: total\nFrance\n7.658579\n\n\n\n\n5 rows × 51 columns\n\n\n\n\n# let's keep only what we need\nddf = ddf[['period', 'inflation', 'gdp', 'unemployment']]\n\n\n# let the period be the index of the dataframe\nddf.index = ddf['period']\nddf.head()\n\n\n\n\n\n\n\n\nperiod\ninflation\ngdp\nunemployment\n\n\nperiod\n\n\n\n\n\n\n\n\n2003-01-01\n2003-01-01\n2.098472\n1630.666\n8.099563\n\n\n2004-01-01\n2004-01-01\n2.142090\n1704.017\n8.468398\n\n\n2005-01-01\n2005-01-01\n1.745869\n1765.903\n8.493855\n\n\n2006-01-01\n2006-01-01\n1.675124\n1848.150\n8.449007\n\n\n2007-01-01\n2007-01-01\n1.487998\n1941.361\n7.658579\n\n\n\n\n\n\n\n\n# to keep things tidy, we can remove the period column\nddf = ddf.drop(columns=['period'])\nddf.head()\n\n\n\n\n\n\n\n\ninflation\ngdp\nunemployment\n\n\nperiod\n\n\n\n\n\n\n\n2003-01-01\n2.098472\n1630.666\n8.099563\n\n\n2004-01-01\n2.142090\n1704.017\n8.468398\n\n\n2005-01-01\n1.745869\n1765.903\n8.493855\n\n\n2006-01-01\n1.675124\n1848.150\n8.449007\n\n\n2007-01-01\n1.487998\n1941.361\n7.658579\n\n\n\n\n\n\n\nNow we’ve got a nice, easy to use, dataframe !\n\nCompute growth rate of gdp.\n\n\n#a new series with the observations from period before can be obtained using .shift()(\n# note the missing value for the initial date\nddf.shift(1).head()\n\n\n\n\n\n\n\n\ninflation\ngdp\nunemployment\n\n\nperiod\n\n\n\n\n\n\n\n2003-01-01\nNaN\nNaN\nNaN\n\n\n2004-01-01\n2.098472\n1630.666\n8.099563\n\n\n2005-01-01\n2.142090\n1704.017\n8.468398\n\n\n2006-01-01\n1.745869\n1765.903\n8.493855\n\n\n2007-01-01\n1.675124\n1848.150\n8.449007\n\n\n\n\n\n\n\n\n# now we can compute growth rates\nddf['gdp_growth'] = (ddf['gdp']-ddf['gdp'].shift(1))/(ddf['gdp'].shift(1))*100\n\n\nddf['gdp_growth'].head()\n\nperiod\n2003-01-01         NaN\n2004-01-01    4.498223\n2005-01-01    3.631771\n2006-01-01    4.657504\n2007-01-01    5.043476\nName: gdp_growth, dtype: float64\n\n\n\nPlot two graphs two verify graphically the Phillips curve (unemployment against inflation) and Okun’s law (unemployment against output).\n\n\nddf.columns\n\nIndex(['inflation', 'gdp', 'unemployment', 'gdp_growth'], dtype='object')\n\n\n\nplt.plot(ddf['unemployment'], ddf['inflation'], 'o')\nplt.xlabel(\"Unemployment (%)\")\nplt.ylabel(\"Inflation (%)\")\nplt.title(\"Phillips curve (2004-2020)\")\n\nText(0.5, 1.0, 'Phillips curve (2004-2020)')\n\n\n\n\n\nWithout any econometric, work, it would seem that the Phillips relationship holds pretty well in France from 2004 to 2020.\n\nplt.plot(ddf['unemployment'], ddf['gdp_growth'], 'o')\nplt.xlabel(\"Unemployment (%)\")\nplt.ylabel(\"GDP growth (%)\")\nplt.title(\"Okun's law (France: 2004-2020)\")\n\nText(0.5, 1.0, \"Okun's law (France: 2004-2020)\")\n\n\n\n\n\nAs for Okun’s law, again, the negeative relationship between GDP growth and unemployment holds fairly well, save for one very abnormal point.\n\nBonus: alternative solution to import the data\n\nIt is possible to import all series at once, by supplying all identifiers to the ‘fetch_series’ method.\n\nfull_df = dbnomics.fetch_series(['OECD/KEI/CPALTT01.USA.GP.A', 'OECD/MEI/FRA.NAEXCP01.STSA.A', 'OECD/CSPCUBE/UNEMPLRT_T1C.FRA'])\nfull_df.head()\n\n\n\n\n\n\n\n\n@frequency\nprovider_code\ndataset_code\ndataset_name\nseries_code\nseries_name\noriginal_period\nperiod\noriginal_value\nvalue\n...\nSubject\nCountry\nSubject\nCountry\nMeasure\nFrequency\nCountry\nSubject\nMeasure\nFrequency\n\n\n\n\n0\nannual\nOECD\nKEI\nKey Short-Term Economic Indicators\nCPALTT01.USA.GP.A\nConsumer prices: all items – United States – G...\n1956\n1956-01-01\n1.525054\n1.525054\n...\nConsumer prices: all items\nUnited States\nConsumer prices: all items\nUnited States\nGrowth previous period\nAnnual\nUnited States\nConsumer prices: all items\nGrowth previous period\nAnnual\n\n\n1\nannual\nOECD\nKEI\nKey Short-Term Economic Indicators\nCPALTT01.USA.GP.A\nConsumer prices: all items – United States – G...\n1957\n1957-01-01\n3.341508\n3.341508\n...\nConsumer prices: all items\nUnited States\nConsumer prices: all items\nUnited States\nGrowth previous period\nAnnual\nUnited States\nConsumer prices: all items\nGrowth previous period\nAnnual\n\n\n2\nannual\nOECD\nKEI\nKey Short-Term Economic Indicators\nCPALTT01.USA.GP.A\nConsumer prices: all items – United States – G...\n1958\n1958-01-01\n2.729160\n2.729160\n...\nConsumer prices: all items\nUnited States\nConsumer prices: all items\nUnited States\nGrowth previous period\nAnnual\nUnited States\nConsumer prices: all items\nGrowth previous period\nAnnual\n\n\n3\nannual\nOECD\nKEI\nKey Short-Term Economic Indicators\nCPALTT01.USA.GP.A\nConsumer prices: all items – United States – G...\n1959\n1959-01-01\n1.010684\n1.010684\n...\nConsumer prices: all items\nUnited States\nConsumer prices: all items\nUnited States\nGrowth previous period\nAnnual\nUnited States\nConsumer prices: all items\nGrowth previous period\nAnnual\n\n\n4\nannual\nOECD\nKEI\nKey Short-Term Economic Indicators\nCPALTT01.USA.GP.A\nConsumer prices: all items – United States – G...\n1960\n1960-01-01\n1.457976\n1.457976\n...\nConsumer prices: all items\nUnited States\nConsumer prices: all items\nUnited States\nGrowth previous period\nAnnual\nUnited States\nConsumer prices: all items\nGrowth previous period\nAnnual\n\n\n\n\n5 rows × 30 columns\n\n\n\n\nfull_df['series_name'].unique()\n\narray(['Consumer prices: all items – United States – Growth previous period – Annual',\n       'France – National Accounts &gt; GDP by Expenditure &gt; Current Prices &gt; Gross Domestic Product - Total – Level, rate or national currency, s.a. – Annual',\n       'Unemployment rates: total – France'], dtype=object)\n\n\nIn the result, each line corresponds to an observation. The column series_name contains the relevant observation. Let’s keep only the relevant column to get a clearer view.\n\ndf_long = full_df[['period', 'series_name','value']]\n\nThis is essentially the long format. We can use it as is, or convert to the wide format.\n\ndf_long.columns\n\nIndex(['period', 'series_name', 'value'], dtype='object')\n\n\n\ndf_long\n\n\n\n\n\n\n\n\nperiod\nseries_name\nvalue\n\n\n\n\n0\n1956-01-01\nConsumer prices: all items – United States – G...\n1.525054\n\n\n1\n1957-01-01\nConsumer prices: all items – United States – G...\n3.341508\n\n\n2\n1958-01-01\nConsumer prices: all items – United States – G...\n2.729160\n\n\n3\n1959-01-01\nConsumer prices: all items – United States – G...\n1.010684\n\n\n4\n1960-01-01\nConsumer prices: all items – United States – G...\n1.457976\n\n\n...\n...\n...\n...\n\n\n11\n2014-01-01\nUnemployment rates: total – France\n10.291710\n\n\n12\n2015-01-01\nUnemployment rates: total – France\n10.359810\n\n\n13\n2016-01-01\nUnemployment rates: total – France\n10.056610\n\n\n14\n2017-01-01\nUnemployment rates: total – France\n9.398605\n\n\n15\n2018-01-01\nUnemployment rates: total – France\n9.059228\n\n\n\n\n120 rows × 3 columns\n\n\n\nTo convert it to the wide format, use the pivot function.\n\ndf_wide = df_long.pivot(index='period', columns=['series_name'])\n\n\ndf_wide\n\n\n\n\n\n\n\n\nvalue\n\n\nseries_name\nConsumer prices: all items – United States – Growth previous period – Annual\nFrance – National Accounts &gt; GDP by Expenditure &gt; Current Prices &gt; Gross Domestic Product - Total – Level, rate or national currency, s.a. – Annual\nUnemployment rates: total – France\n\n\nperiod\n\n\n\n\n\n\n\n1956-01-01\n1.525054\nNaN\nNaN\n\n\n1957-01-01\n3.341508\nNaN\nNaN\n\n\n1958-01-01\n2.729160\nNaN\nNaN\n\n\n1959-01-01\n1.010684\nNaN\nNaN\n\n\n1960-01-01\n1.457976\nNaN\nNaN\n\n\n...\n...\n...\n...\n\n\n2015-01-01\n0.118627\n2198.432\n10.359810\n\n\n2016-01-01\n1.261583\n2234.129\n10.056610\n\n\n2017-01-01\n2.130110\n2297.244\n9.398605\n\n\n2018-01-01\n2.442583\n2360.686\n9.059228\n\n\n2019-01-01\n1.812210\n2425.710\nNaN\n\n\n\n\n64 rows × 3 columns\n\n\n\n\n# rename columns\ndf_wide.columns = ['inflation','gdp', 'unemployment']\n\n\n# and here is our tidy dataframe !\ndf_wide\n\n\n\n\n\n\n\n\ninflation\ngdp\nunemployment\n\n\nperiod\n\n\n\n\n\n\n\n1956-01-01\n1.525054\nNaN\nNaN\n\n\n1957-01-01\n3.341508\nNaN\nNaN\n\n\n1958-01-01\n2.729160\nNaN\nNaN\n\n\n1959-01-01\n1.010684\nNaN\nNaN\n\n\n1960-01-01\n1.457976\nNaN\nNaN\n\n\n...\n...\n...\n...\n\n\n2015-01-01\n0.118627\n2198.432\n10.359810\n\n\n2016-01-01\n1.261583\n2234.129\n10.056610\n\n\n2017-01-01\n2.130110\n2297.244\n9.398605\n\n\n2018-01-01\n2.442583\n2360.686\n9.059228\n\n\n2019-01-01\n1.812210\n2425.710\nNaN\n\n\n\n\n64 rows × 3 columns"
  },
  {
    "objectID": "session_2/transcript.html#tabular-data",
    "href": "session_2/transcript.html#tabular-data",
    "title": "Dataframes",
    "section": "Tabular Data",
    "text": "Tabular Data"
  },
  {
    "objectID": "session_2/transcript.html#pandas",
    "href": "session_2/transcript.html#pandas",
    "title": "Dataframes",
    "section": "Pandas",
    "text": "Pandas"
  },
  {
    "objectID": "session_2/transcript.html#file-formats",
    "href": "session_2/transcript.html#file-formats",
    "title": "Dataframes",
    "section": "File Formats",
    "text": "File Formats"
  },
  {
    "objectID": "session_2/transcript.html#data-sources",
    "href": "session_2/transcript.html#data-sources",
    "title": "Dataframes",
    "section": "Data Sources",
    "text": "Data Sources"
  },
  {
    "objectID": "session_2/transcript.html#inspect-describe-data",
    "href": "session_2/transcript.html#inspect-describe-data",
    "title": "Dataframes",
    "section": "Inspect / describe data",
    "text": "Inspect / describe data"
  },
  {
    "objectID": "session_2/transcript.html#manipulating-dataframes",
    "href": "session_2/transcript.html#manipulating-dataframes",
    "title": "Dataframes",
    "section": "Manipulating DataFrames",
    "text": "Manipulating DataFrames"
  },
  {
    "objectID": "session_2/transcript.html#reshaping-dataframes",
    "href": "session_2/transcript.html#reshaping-dataframes",
    "title": "Dataframes",
    "section": "Reshaping DataFrames",
    "text": "Reshaping DataFrames"
  },
  {
    "objectID": "session_2/transcript.html#merging",
    "href": "session_2/transcript.html#merging",
    "title": "Dataframes",
    "section": "Merging",
    "text": "Merging"
  },
  {
    "objectID": "session_2/transcript.html#merging-two-dataframes-with-pandas",
    "href": "session_2/transcript.html#merging-two-dataframes-with-pandas",
    "title": "Dataframes",
    "section": "Merging two DataFrames with pandas",
    "text": "Merging two DataFrames with pandas\ndf_long_1.merge(df_long_2)\n\n\n\n\n\n\n\n\n\nyear\n\n\ncountry\n\n\nmeasure\n\n\nrecipient\n\n\n\n\n\n\n0\n\n\n2018\n\n\nfrance\n\n\n950.0\n\n\nmaxime\n\n\n\n\n1\n\n\n2019\n\n\nfrance\n\n\n960.0\n\n\nmauricette\n\n\n\n\n2\n\n\n2020\n\n\nfrance\n\n\n1000.0\n\n\nmathilde\n\n\n\n\n3\n\n\n2018\n\n\nusa\n\n\n2500.0\n\n\nsherlock\n\n\n\n\n4\n\n\n2019\n\n\nusa\n\n\n2150.0\n\n\nwatson\n\n\n\n\n5\n\n\n2020\n\n\nusa\n\n\n2300.0\n\n\nmoriarty"
  },
  {
    "objectID": "session_2/index.html#tabular-data",
    "href": "session_2/index.html#tabular-data",
    "title": "Dataframes",
    "section": "Tabular Data",
    "text": "Tabular Data\n\n\nDataFrame\n\n\n\nA DataFrame (aka a table) is a 2-D labeled data structure with columns\n\neach column has a specific type and a column name\ntypes: quantitative, qualitative (ordered, non-ordered, …)\n\nFirst column is special: the index\nfirst goal of an econometrician: constitute a good dataframe\n\naka “cleaning the data”\n\n\n\n\n\n\n\n\n\n\nDataFrames are everywhere\n\n\n\n\n\nsometimes data comes from several linked dataframes\n\nrelational database\ncan still be seen conceptually as one dataframe…\n… through a join operation\n\n\n\n\n\ndataframes / relational databases are so ubiquitous a language has been developed for them\n\nSQL\nin the 80s…\n\nprobably worth looking at if you have some “data” ambitions\nyou will see the shadow of SQL everywhere\nplenty of resources to learn:"
  },
  {
    "objectID": "session_2/index.html#pandas",
    "href": "session_2/index.html#pandas",
    "title": "Dataframes",
    "section": "Pandas",
    "text": "Pandas\n\n\npandas\n\npandas = panel + datas\n\na python library created by WesMcKinney\nvery optimized\n\nessentially a dataframe object\nmany options but if in doubt:\n\nminimally sufficient pandas is asmall subset of pandas to do everything\n\ntons of online tutorials\n\nofficial documenation doc\nquantecon\n\n\nTODO: sweet pandas ?\n\n\n\ncreating a dataframe (1)\n\nImport pandas\n\npreferably with standard alias pd\n\nimport pandas as pd\nImport a dataframe\n\neach line a different entry in a dictionary\n\n# from a dictionary\nd = {\n  \"country\": [\"USA\", \"UK\", \"France\"],\n  \"comics\": [13, 10, 12]   \n}\npd.DataFrame(d)\n\n\n\n\n\n\n\n\n\n\ncountry\n\n\ncomics\n\n\n\n\n\n\n0\n\n\nUSA\n\n\n13\n\n\n\n\n1\n\n\nUK\n\n\n10\n\n\n\n\n2\n\n\nFrance\n\n\n12\n\n\n\n\n\n\n\n\n\ncreating a dataframe (2)\n\nthere are many other ways to create a dataframe\n\nfor instance using a numpy matrix (numpy is a linear algebra library)\n\n# from a matrix\nimport numpy as np\nM = np.array(\n    [[18, 150],\n     [21, 200],\n     [29, 1500]]\n)   \ndf = pd.DataFrame( M, columns=[\"age\", \"travel\"] )\ndf\n\n\n\n\n\n\n\n\n\n\nage\n\n\ntravel\n\n\n\n\n\n\n0\n\n\n18\n\n\n150\n\n\n\n\n1\n\n\n21\n\n\n200\n\n\n\n\n2\n\n\n29\n\n\n1500"
  },
  {
    "objectID": "session_2/index.html#file-formats",
    "href": "session_2/index.html#file-formats",
    "title": "Dataframes",
    "section": "File Formats",
    "text": "File Formats\n\n\nCommon file formats\n\ncomma separated files: csv file\n\noften distributed online\ncan be exported easily from Excel or LibreOffice\n\nstata files: use pd.read_dta()\nexcel files: use pd.read_excel() or xlsreader if unlucky\n\nnote that excel does not store a dataframe (each cell is potentially different)\npostprocessing is needed\n\n\n\n\n\n\nComma separated file\n\none can actually a file from python\n\ntxt = \"\"\"year,country,measure\n2018,\"france\",950.0\n2019,\"france\",960.0\n2020,\"france\",1000.0\n2018,\"usa\",2500.0\n2019,\"usa\",2150.0\n2020,\"usa\",2300.0\n\"\"\"\nopen('dummy_file.csv','w').write(txt) # we write it to a file\n\nand import it\n\ndf = pd.read_csv('dummy_file.csv') # what index should we use ?\ndf\n\n\n\n\n\n\n\n\n\nyear\n\n\ncountry\n\n\nmeasure\n\n\n\n\n\n\n0\n\n\n2018\n\n\nfrance\n\n\n950.0\n\n\n\n\n1\n\n\n2019\n\n\nfrance\n\n\n960.0\n\n\n\n\n2\n\n\n2020\n\n\nfrance\n\n\n1000.0\n\n\n\n\n3\n\n\n2018\n\n\nusa\n\n\n2500.0\n\n\n\n\n4\n\n\n2019\n\n\nusa\n\n\n2150.0\n\n\n\n\n5\n\n\n2020\n\n\nusa\n\n\n2300.0\n\n\n\n\n\n\n\n\n\n“Annoying” Comma Separated File\n\nSometimes, comma-separated files, are not quite comma-separated…\n\ninspect the file with a text editor to see what it contains\n\nthe kind of separator, whether there are quotes…\n\ntxt = \"\"\"year;country;measure\n2018;\"france\";950.0\n2019;\"france\";960.0\n2020;\"france\";1000.0\n2018;\"usa\";2500.0\n2019;\"usa\";2150.0\n2020;\"usa\";2300.0\n\"\"\"\nopen('annoying_dummy_file.csv','w').write(txt) # we write it to a file\n\nadd relevant options to pd.read_csv and check result\n\npd.read_csv(\"annoying_dummy_file.csv\", sep=\";\")\n\n\n\n\n\n\n\n\n\nyear\n\n\ncountry\n\n\nmeasure\n\n\n\n\n\n\n0\n\n\n2018\n\n\nfrance\n\n\n950.0\n\n\n\n\n1\n\n\n2019\n\n\nfrance\n\n\n960.0\n\n\n\n\n2\n\n\n2020\n\n\nfrance\n\n\n1000.0\n\n\n\n\n3\n\n\n2018\n\n\nusa\n\n\n2500.0\n\n\n\n\n4\n\n\n2019\n\n\nusa\n\n\n2150.0\n\n\n\n\n5\n\n\n2020\n\n\nusa\n\n\n2300.0\n\n\n\n\n\n\n\n\n\nExporting a DataFrame\n\npandas can export to many formats: df.to_...\nto (standard) CSV\n\nprint( df.to_csv() )\n,year,country,measure\n0,2018,france,950.0\n1,2019,france,960.0\n2,2020,france,1000.0\n3,2018,usa,2500.0\n4,2019,usa,2150.0\n5,2020,usa,2300.0\n\nor to stata\n\ndf.to_stata('dummy_example.dta')"
  },
  {
    "objectID": "session_2/index.html#data-sources",
    "href": "session_2/index.html#data-sources",
    "title": "Dataframes",
    "section": "Data Sources",
    "text": "Data Sources\n\n\nTypes of Data Sources\n\n\n\nWhere can we get data from ?\n\ncheck one of the databases lists kaggle, econ network\n\nOfficial websites\n\noften in csv form\nunpractical applications\nsometimes unavoidable\nopen data trend: more unstructured data\n\nData providers\n\nsupply an API (i.e. easy to use function)\n\n\n\n\n\n\n\n\n\n\nData providers\n\ncommercial ones:\n\nbloomberg, macrobond, factsets, quandl …\n\nfree ones available as a python library\n\ndbnomics: many official time-series\nqeds: databases used by quantecon\nvega-datasets: distributed with altair\n\n\n\nimport vega_datasets\ndf = vega_datasets.data('iris')\ndf\n\n\n\n\n\n\n\n\n\nsepalLength\n\n\nsepalWidth\n\n\npetalLength\n\n\npetalWidth\n\n\nspecies\n\n\n\n\n\n\n0\n\n\n5.1\n\n\n3.5\n\n\n1.4\n\n\n0.2\n\n\nsetosa\n\n\n\n\n1\n\n\n4.9\n\n\n3.0\n\n\n1.4\n\n\n0.2\n\n\nsetosa\n\n\n\n\n2\n\n\n4.7\n\n\n3.2\n\n\n1.3\n\n\n0.2\n\n\nsetosa\n\n\n\n\n3\n\n\n4.6\n\n\n3.1\n\n\n1.5\n\n\n0.2\n\n\nsetosa\n\n\n\n\n4\n\n\n5.0\n\n\n3.6\n\n\n1.4\n\n\n0.2\n\n\nsetosa\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\n145\n\n\n6.7\n\n\n3.0\n\n\n5.2\n\n\n2.3\n\n\nvirginica\n\n\n\n\n146\n\n\n6.3\n\n\n2.5\n\n\n5.0\n\n\n1.9\n\n\nvirginica\n\n\n\n\n147\n\n\n6.5\n\n\n3.0\n\n\n5.2\n\n\n2.0\n\n\nvirginica\n\n\n\n\n148\n\n\n6.2\n\n\n3.4\n\n\n5.4\n\n\n2.3\n\n\nvirginica\n\n\n\n\n149\n\n\n5.9\n\n\n3.0\n\n\n5.1\n\n\n1.8\n\n\nvirginica\n\n\n\n\n\n\n150 rows × 5 columns\n\n\n\n\n\n\n\nDBnomics example\n\nDBnomics aggregates time series from various public sources\ndata is organized as provider/database/series\ntry to find the identifer of one or several series\n\nimport dbnomics\ndf = dbnomics.fetch_series('AMECO/ZUTN/EA19.1.0.0.0.ZUTN')\n\ntip: in case one python package is missing, it can be installed on the fly as in\n\n!pip install dbnomics"
  },
  {
    "objectID": "session_2/index.html#inspect-describe-data",
    "href": "session_2/index.html#inspect-describe-data",
    "title": "Dataframes",
    "section": "Inspect / describe data",
    "text": "Inspect / describe data\n\n\nInspecting data\n\nonce the data is loaded as df, we want to look at some basic properties:\ngeneral\n\ndf.head(5) # 5 first lines\ndf.tail(5) # 5 first lines\ndf.describe() # general summary\n\ncentral tendency\n\ndf.mean() # average\ndf.median() # median\n\nspread\n\ndf.std() # standard deviations\ndf.var() # variance\ndf.min(), df.max() # bounds\n\ncounts (for categorical variable\n\ndf.count()\n\n\n\ndf.head(2)\n\n\n\n\n\n\n\n\n\nsepalLength\n\n\nsepalWidth\n\n\npetalLength\n\n\npetalWidth\n\n\nspecies\n\n\n\n\n\n\n0\n\n\n5.1\n\n\n3.5\n\n\n1.4\n\n\n0.2\n\n\nsetosa\n\n\n\n\n1\n\n\n4.9\n\n\n3.0\n\n\n1.4\n\n\n0.2\n\n\nsetosa\n\n\n\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\n\nsepalLength\n\n\nsepalWidth\n\n\npetalLength\n\n\npetalWidth\n\n\n\n\n\n\ncount\n\n\n150.000000\n\n\n150.000000\n\n\n150.000000\n\n\n150.000000\n\n\n\n\nmean\n\n\n5.843333\n\n\n3.057333\n\n\n3.758000\n\n\n1.199333\n\n\n\n\nstd\n\n\n0.828066\n\n\n0.435866\n\n\n1.765298\n\n\n0.762238\n\n\n\n\nmin\n\n\n4.300000\n\n\n2.000000\n\n\n1.000000\n\n\n0.100000\n\n\n\n\n25%\n\n\n5.100000\n\n\n2.800000\n\n\n1.600000\n\n\n0.300000\n\n\n\n\n50%\n\n\n5.800000\n\n\n3.000000\n\n\n4.350000\n\n\n1.300000\n\n\n\n\n75%\n\n\n6.400000\n\n\n3.300000\n\n\n5.100000\n\n\n1.800000\n\n\n\n\nmax\n\n\n7.900000\n\n\n4.400000\n\n\n6.900000\n\n\n2.500000"
  },
  {
    "objectID": "session_2/index.html#manipulating-dataframes",
    "href": "session_2/index.html#manipulating-dataframes",
    "title": "Dataframes",
    "section": "Manipulating DataFrames",
    "text": "Manipulating DataFrames\n\n\nChanging names of columns\n\nColumns are defined by property df.columns\n\ndf.columns\nIndex(['sepalLength', 'sepalWidth', 'petalLength', 'petalWidth', 'species'], dtype='object')\n\nThis property can be set with a list of the right length\n\ndf.columns = ['sLength', 'sWidth', 'pLength', 'pWidth', 'species']\ndf.head(2)\n\n\n\n\n\n\n\n\n\nsLength\n\n\nsWidth\n\n\npLength\n\n\npWidth\n\n\nspecies\n\n\n\n\n\n\n0\n\n\n5.1\n\n\n3.5\n\n\n1.4\n\n\n0.2\n\n\nsetosa\n\n\n\n\n1\n\n\n4.9\n\n\n3.0\n\n\n1.4\n\n\n0.2\n\n\nsetosa\n\n\n\n\n\n\n\n\n\nIndexing a column\n\nA column can be extracted using its name as in a dictionary (like df['sLength'])\n\nseries = df['sWidth'] # note the resulting object: a series\nseries\n0      3.5\n1      3.0\n      ... \n148    3.4\n149    3.0\nName: sWidth, Length: 150, dtype: float64\n\nThe result is a series object (typed values with a name and an index)\nIt has its own set of methods\n\ntry:\n\nseries.mean(), series.std()\nseries.plot()\nseries.diff()\n\ncreates \\(y_t = x_t-x_{t-1}\\)\n\nseries.pct_change()\n\ncreates \\(y_t = \\frac{x_t-x_{t-1}}{x_{t-1}}\\)\n\n\n\n\n\n\n\n\nCreating a new column\n\nIt is possible to create a new column by combining existing ones\n\ndf['totalLength'] = df['pLength'] + df['sLength']\n# this would also work\ndf['totalLength'] = 0.5*df['pLength'] + 0.5*df['sLength']\ndf.head(2)\n\n\n\n\n\n\n\n\n\nsLength\n\n\nsWidth\n\n\npLength\n\n\npWidth\n\n\nspecies\n\n\ntotalLength\n\n\n\n\n\n\n0\n\n\n5.1\n\n\n3.5\n\n\n1.4\n\n\n0.2\n\n\nsetosa\n\n\n6.5\n\n\n\n\n1\n\n\n4.9\n\n\n3.0\n\n\n1.4\n\n\n0.2\n\n\nsetosa\n\n\n6.3\n\n\n\n\n\n\n\n\n\nReplacing a column\n\nAn existing column can be replaced with the same syntax.\n\ndf['totalLength'] = df['pLength'] + df['sLength']*0.5\ndf.head(2)\n\n\n\n\n\n\n\n\n\nsLength\n\n\nsWidth\n\n\npLength\n\n\npWidth\n\n\nspecies\n\n\ntotalLength\n\n\n\n\n\n\n0\n\n\n5.1\n\n\n3.5\n\n\n1.4\n\n\n0.2\n\n\nsetosa\n\n\n3.95\n\n\n\n\n1\n\n\n4.9\n\n\n3.0\n\n\n1.4\n\n\n0.2\n\n\nsetosa\n\n\n3.85\n\n\n\n\n\n\n\n\n\nSelecting several columns\n\nIndex with a list of column names\n\ne = df[ ['pLength', 'sLength'] ]\ne.head(3)\n\n\n\n\n\n\n\n\n\npLength\n\n\nsLength\n\n\n\n\n\n\n0\n\n\n1.4\n\n\n5.1\n\n\n\n\n1\n\n\n1.4\n\n\n4.9\n\n\n\n\n2\n\n\n1.3\n\n\n4.7\n\n\n\n\n\n\n\n\n\nSelecting lines (1)\n\nuse index range\n\n☡: in Python the end of a range is not included !\n\ndf[2:4]\n\n\n\n\n\n\n\n\n\n\nsLength\n\n\nsWidth\n\n\npLength\n\n\npWidth\n\n\nspecies\n\n\ntotalLength\n\n\n\n\n\n\n2\n\n\n4.7\n\n\n3.2\n\n\n1.3\n\n\n0.2\n\n\nsetosa\n\n\n3.65\n\n\n\n\n3\n\n\n4.6\n\n\n3.1\n\n\n1.5\n\n\n0.2\n\n\nsetosa\n\n\n3.80\n\n\n\n\n\n\n\n\n\nSelecting lines (2)\n\nlet’s look at unique species\n\ndf['species'].unique()\narray(['setosa', 'versicolor', 'virginica'], dtype=object)\n\nwe would like to keep only the lines with virginica\n\nbool_ind = df['species'] == 'virginica' # this is a boolean serie\n\nthe result is a boolean series, where each element tells whether a line should be kept or not\n\ne = df[ bool_ind ]\ne.head(4)\n\nif you want you can keep the recipe:\n\ndf[df['species'] == 'virginica']\n\nto keep lines where species is equal to virginica\n\n\n\n\n\n\n\n\n\n\nsLength\n\n\nsWidth\n\n\npLength\n\n\npWidth\n\n\nspecies\n\n\ntotalLength\n\n\n\n\n\n\n100\n\n\n6.3\n\n\n3.3\n\n\n6.0\n\n\n2.5\n\n\nvirginica\n\n\n9.15\n\n\n\n\n101\n\n\n5.8\n\n\n2.7\n\n\n5.1\n\n\n1.9\n\n\nvirginica\n\n\n8.00\n\n\n\n\n102\n\n\n7.1\n\n\n3.0\n\n\n5.9\n\n\n2.1\n\n\nvirginica\n\n\n9.45\n\n\n\n\n103\n\n\n6.3\n\n\n2.9\n\n\n5.6\n\n\n1.8\n\n\nvirginica\n\n\n8.75\n\n\n\n\n\n\n\n\n\nSelecting lines and columns\n\nsometimes, one wants finer control about which lines and columns to select:\n\nuse df.loc[...] which can be indexed as a matrix\n\n\ndf.loc[0:4, 'species']\n0    setosa\n1    setosa\n2    setosa\n3    setosa\n4    setosa\nName: species, dtype: object\n\n\n\nCombine everything\n\nHere is an example combiing serveral techniques\n\nLet’s change the way totalLength is computed, but only for ‘virginica’\n\nindex = (df['species']=='virginica')\ndf.loc[index,'totalLength'] = df.loc[index,'sLength'] + 1.5*df[index]['pLength']"
  },
  {
    "objectID": "session_2/index.html#reshaping-dataframes",
    "href": "session_2/index.html#reshaping-dataframes",
    "title": "Dataframes",
    "section": "Reshaping DataFrames",
    "text": "Reshaping DataFrames\n\nThe following code creates two example databases.\ntxt_wide = \"\"\"year,france,usa\n2018,950.0,2500.0\n2019,960.0,2150.0\n2020,1000.0,2300.0\n\"\"\"\nopen('dummy_file_wide.csv','w').write(txt_wide) # we write it to a file\n71\ntxt_long = \"\"\"year,country,measure\n2018,\"france\",950.0\n2019,\"france\",960.0\n2020,\"france\",1000.0\n2018,\"usa\",2500.0\n2019,\"usa\",2150.0\n2020,\"usa\",2300.0\n\"\"\"\nopen('dummy_file_long.csv','w').write(txt_long) # we write it to a file\n136\ndf_long = pd.read_csv(\"dummy_file_long.csv\")\ndf_wide = pd.read_csv(\"dummy_file_wide.csv\")\n\n\nWide vs Long format (1)\nCompare the following tables\n\n\ndf_wide\n\n\n\n\n\n\n\n\n\nyear\n\n\nfrance\n\n\nusa\n\n\n\n\n\n\n0\n\n\n2018\n\n\n950.0\n\n\n2500.0\n\n\n\n\n1\n\n\n2019\n\n\n960.0\n\n\n2150.0\n\n\n\n\n2\n\n\n2020\n\n\n1000.0\n\n\n2300.0\n\n\n\n\n\n\n\n\ndf_long\n\n\n\n\n\n\n\n\n\nyear\n\n\ncountry\n\n\nmeasure\n\n\n\n\n\n\n0\n\n\n2018\n\n\nfrance\n\n\n950.0\n\n\n\n\n1\n\n\n2019\n\n\nfrance\n\n\n960.0\n\n\n\n\n2\n\n\n2020\n\n\nfrance\n\n\n1000.0\n\n\n\n\n3\n\n\n2018\n\n\nusa\n\n\n2500.0\n\n\n\n\n4\n\n\n2019\n\n\nusa\n\n\n2150.0\n\n\n\n\n5\n\n\n2020\n\n\nusa\n\n\n2300.0\n\n\n\n\n\n\n\n\n\n\n\nWide vs Long format (2)\n\nin long format: each line is an independent observation\n\ntwo lines may belong to the same category (year, or country)\nall values are given in the same column\ntheir types/categories are given in another column\n\nin wide format: some observations are grouped\n\nin the example it is grouped by year\nvalues of different kinds are in different columns\nthe types/categories are stored as column names\n\nboth representations are useful\ntidy data:\n\nevery column is a variable.\nevery row is an observation.\nevery cell is a single value.\n\n\n\n\n\nConverting from Wide to Long\ndf_wide.melt(id_vars='year')\n\n\n\n\n\n\n\n\n\nyear\n\n\nvariable\n\n\nvalue\n\n\n\n\n\n\n0\n\n\n2018\n\n\nfrance\n\n\n950.0\n\n\n\n\n1\n\n\n2019\n\n\nfrance\n\n\n960.0\n\n\n\n\n2\n\n\n2020\n\n\nfrance\n\n\n1000.0\n\n\n\n\n3\n\n\n2018\n\n\nusa\n\n\n2500.0\n\n\n\n\n4\n\n\n2019\n\n\nusa\n\n\n2150.0\n\n\n\n\n5\n\n\n2020\n\n\nusa\n\n\n2300.0\n\n\n\n\n\n\n\n\n\nConverting from Long to Wide\ndf_ = df_long.pivot(index='year', columns='country')\ndf_\n\n\n\n\n\n\n\n\n\nmeasure\n\n\n\n\ncountry\n\n\nfrance\n\n\nusa\n\n\n\n\nyear\n\n\n\n\n\n\n\n\n\n\n2018\n\n\n950.0\n\n\n2500.0\n\n\n\n\n2019\n\n\n960.0\n\n\n2150.0\n\n\n\n\n2020\n\n\n1000.0\n\n\n2300.0\n\n\n\n\n\n\n# the result of pivot has a \"hierarchical index\"\n# let's change columns names\ndf_.columns = df_.columns.get_level_values(1)\ndf_\n\n\n\n\n\n\n\ncountry\n\n\nfrance\n\n\nusa\n\n\n\n\nyear\n\n\n\n\n\n\n\n\n\n\n2018\n\n\n950.0\n\n\n2500.0\n\n\n\n\n2019\n\n\n960.0\n\n\n2150.0\n\n\n\n\n2020\n\n\n1000.0\n\n\n2300.0\n\n\n\n\n\n\n\n\n\ngroupby\n\ngroupby is a very powerful function which can be used to work directly on data in the long format.\n\nfor instance to compute averages per country\n\ndf_long.groupby(\"country\").mean()\n\n\n\n\n\n\n\n\n\n\nyear\n\n\nmeasure\n\n\n\n\ncountry\n\n\n\n\n\n\n\n\n\n\nfrance\n\n\n2019.0\n\n\n970.000000\n\n\n\n\nusa\n\n\n2019.0\n\n\n2316.666667\n\n\n\n\n\n\n\nYou can perform several aggregations at the same time:\n\ndf_long.groupby(\"country\").agg(['mean','std'])"
  },
  {
    "objectID": "session_2/index.html#merging",
    "href": "session_2/index.html#merging",
    "title": "Dataframes",
    "section": "Merging",
    "text": "Merging\n\n\nMerging two dataframes\n\nSuppose we have two dataframes, with related observations\nHow can we construct one single database with all informations?\nAnswer:\n\nconcat if long format\nmerge databases if wide format\n\nLots of subtleties when data gets complicated\n\nwe’ll see them in due time\n\n\n\ntxt_long_1 = \"\"\"year,country,measure\n2018,\"france\",950.0\n2019,\"france\",960.0\n2020,\"france\",1000.0\n2018,\"usa\",2500.0\n2019,\"usa\",2150.0\n2020,\"usa\",2300.0\n\"\"\"\nopen(\"dummy_long_1.csv\",'w').write(txt_long_1)\ntxt_long_2 = \"\"\"year,country,recipient\n2018,\"france\",maxime\n2019,\"france\",mauricette\n2020,\"france\",mathilde\n2018,\"usa\",sherlock\n2019,\"usa\",watson\n2020,\"usa\",moriarty\n\"\"\"\nopen(\"dummy_long_2.csv\",'w').write(txt_long_2)\ndf_long_1 = pd.read_csv('dummy_long_1.csv')\ndf_long_2 = pd.read_csv('dummy_long_2.csv')"
  },
  {
    "objectID": "session_2/index.html#merging-two-dataframes-with-pandas",
    "href": "session_2/index.html#merging-two-dataframes-with-pandas",
    "title": "Dataframes",
    "section": "Merging two DataFrames with pandas",
    "text": "Merging two DataFrames with pandas\ndf_long_1.merge(df_long_2)\n\n\n\n\n\n\n\n\n\nyear\n\n\ncountry\n\n\nmeasure\n\n\nrecipient\n\n\n\n\n\n\n0\n\n\n2018\n\n\nfrance\n\n\n950.0\n\n\nmaxime\n\n\n\n\n1\n\n\n2019\n\n\nfrance\n\n\n960.0\n\n\nmauricette\n\n\n\n\n2\n\n\n2020\n\n\nfrance\n\n\n1000.0\n\n\nmathilde\n\n\n\n\n3\n\n\n2018\n\n\nusa\n\n\n2500.0\n\n\nsherlock\n\n\n\n\n4\n\n\n2019\n\n\nusa\n\n\n2150.0\n\n\nwatson\n\n\n\n\n5\n\n\n2020\n\n\nusa\n\n\n2300.0\n\n\nmoriarty"
  },
  {
    "objectID": "session_2/Numerical Python.html",
    "href": "session_2/Numerical Python.html",
    "title": "Numerical Python",
    "section": "",
    "text": "Most python scientists, use the following libraries:\n\nnumpy: performant array library (vectors and matrices)\nmatplotlib: plotting library\nscipy: all kinds of mathematical routines\n\nIn the rest of the course, we’ll make some use of numpy and matplotlib\nThey are included in all python distributions like Anaconda Python\nAll additional libraries use numpy and matplotlib: pandas, statsmodels, sklearn\n\n\n\n\nIt is standard to import the libraries as np, and plt. We’ll follow this convention here.\n\n# these lines need to be run only once per program\nimport numpy as np\nimport matplotlib as plt\n\n\nprint(f\"Numpy version {np.__version__}\")\nprint(f\"Matplotlib version {plt.__version__}\")\n\nNumpy version 1.19.5\nMatplotlib version3.3.3"
  },
  {
    "objectID": "session_2/Numerical Python.html#scientific-stack",
    "href": "session_2/Numerical Python.html#scientific-stack",
    "title": "Numerical Python",
    "section": "",
    "text": "Most python scientists, use the following libraries:\n\nnumpy: performant array library (vectors and matrices)\nmatplotlib: plotting library\nscipy: all kinds of mathematical routines\n\nIn the rest of the course, we’ll make some use of numpy and matplotlib\nThey are included in all python distributions like Anaconda Python\nAll additional libraries use numpy and matplotlib: pandas, statsmodels, sklearn\n\n\n\n\nIt is standard to import the libraries as np, and plt. We’ll follow this convention here.\n\n# these lines need to be run only once per program\nimport numpy as np\nimport matplotlib as plt\n\n\nprint(f\"Numpy version {np.__version__}\")\nprint(f\"Matplotlib version {plt.__version__}\")\n\nNumpy version 1.19.5\nMatplotlib version3.3.3"
  },
  {
    "objectID": "session_2/Numerical Python.html#numpy",
    "href": "session_2/Numerical Python.html#numpy",
    "title": "Numerical Python",
    "section": "Numpy",
    "text": "Numpy\n\nVector Creation\n\nVectors and matrices are created with the np.array(...) function.\nSpecial vectors can be created with np.zeros, np.ones, np.linspace\n\n\n# an array can be created from a list of numbers\nnp.array( [1.0, 2.0, 3.0] )\n\narray([1., 2., 3.])\n\n\n\n# or initialized by specifying the length of the array\nnp.zeros(5)\n\narray([0., 0., 0., 0., 0.])\n\n\n\n# 10 regularly spaced points between 0 and 1\nnp.linspace(0, 1, 10)\n\narray([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])\n\n\n\n\nMatrix Creation\n\nA matrix is a 2-dimensional array and is created with np.array\nFunction np.matrix() has been deprecated: do not use it.\nThere are functions to create specific matrices: np.eye, np.diag, …\n\n\n# an array can be created from a list of (equal size) lists\nnp.array([\n    [1.0, 2.0, 3.0],\n    [4  ,   5,   6] \n])\n\narray([[1., 2., 3.],\n       [4., 5., 6.]])\n\n\n\n# initialize an empty matrix with the dimensions as a tuple\nA = np.zeros( (2, 3) )\nA\n\narray([[0., 0., 0.],\n       [0., 0., 0.]])\n\n\n\n# matrix dimensions are contained in the shape attribute\nA.shape\n\n(2, 3)\n\n\n\n\nLinear Algebra\nVector multiplications and Matrix multiplications can be performed using special sign @\n\nA = np.array([[1.0, 2.0], [2,4]])\nA\n\narray([[1., 2.],\n       [2., 4.]])\n\n\n\nB = np.array([1.0, 2.0])\nB\n\narray([1., 2.])\n\n\n\nA@B\n\narray([ 5., 10.])\n\n\n\nA@A\n\narray([[ 5., 10.],\n       [10., 20.]])\n\n\n\n\nScalar types\nNumpy arrays can contain data of several scalar types.\n\n[True, False, True]\n\n[True, False, True]\n\n\n\n# vector of boolean\nboolean_vector = np.array( [True, False, True] )\nprint(f\"type of scalar '{boolean_vector.dtype}'\")\nboolean_vector\n\ntype of scalar 'bool'\n\n\narray([ True, False,  True])\n\n\n\n# vector of integers\nint_vector = np.array([1, 2, 0])\nprint(f\"type of scalar '{int_vector.dtype}'\")\nint_vector\n\ntype of scalar 'int64'\n\n\narray([1, 2, 0])\n\n\n\n\nSubscripting Vectors\n\nElements and subarrays, can be retrieved using the same syntax as lists and strings.\n\nRemember that indexing starts at 0.\n\n\n\nV = np.array([0., 1., 2., 3., 4.])\ndisplay(V[1])  # second element\n\n1.0\n\n\n\nV = np.array([0., 1., 2., 3., 4.])\ndisplay(V[1:3])  # second, third and fourth element\n\narray([1., 2.])\n\n\n\n\nModifying Vector Content\n\nElements and suvectors, can be assigned to new values, as long as they have the right dimensions.\n\n\nV = np.array([1., 1., 2., 4., 5., 8., 13.])\nV[3] = 3.0\nV\n\narray([ 1.,  1.,  2.,  3.,  5.,  8., 13.])\n\n\n\nV = np.array([1., 1., 2., 4., 5., 8., 13.])\n# V[1:4] = [1,2,3,4] # this doesn't work\nV[1:4] = [2,3,4] # this works\n\n\n\nSubscripting Matrices\n\nIndexing generalizes to matrices: there are two indices istead of one: M[i,j]\nOne can extract a row, or a column (a slice) with M[i,:] or M[:,i]\nA submatrix is defining with two intervals: M[i:j, k:l] or M[i:j, :], …\n\n\nM = np.array([[1,2,3],[4,5,6],[7,8,9]])\nM\n\narray([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])\n\n\n\nM[0,1] # access element (1,2)\n\n2\n\n\n\nM[2,:] # third row\n\narray([7, 8, 9])\n\n\n\nM[:,1] # second column     # M[i,1] for any i\n\narray([2, 5, 8])\n\n\n\nM[1:3, :] # lines from 1 (included) to 3 (excluded) ; all columns\n\narray([[4, 5, 6],\n       [7, 8, 9]])\n\n\n\n\nModifying matrix content\n\nM = np.array([[1,2,3],[4,5,6],[7,8,9]])\nM\n\narray([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])\n\n\n\nM[0,0] = 0\nM\n\narray([[0, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])\n\n\n\nM[1:3, 1:3] = np.array([[0,1],[1,0]]) # dimensions must match\nM\n\narray([[0, 2, 3],\n       [4, 0, 1],\n       [7, 1, 0]])\n\n\n\n\nElement-wise algebraic operations\n\nThe following algebraic operations are defined on arrays: +, -, *, /, **.\nComparisons operators (&lt;,&lt;=, &gt;, &gt;=, ==) are defined are return boolean arrays.\nThey operate element by element.\n\n\nA = np.array([1,2,3,4])\nB = np.array([4,3,2,1])\nA+B\n\narray([5, 5, 5, 5])\n\n\n\nA*B    # note the difference with A@B\n\narray([4, 6, 6, 4])\n\n\n\nA&gt;B\n\narray([False, False,  True,  True])\n\n\n\n\nElement-wise logical operations\n\nThe following logical operations are defined element-wise on arrays: & (and), | (or), ~ (not)\n\n\nA = np.array([False, False, True, True])\nB = np.array([False, True, False, True])\n\n\n~A\n\narray([ True,  True, False, False])\n\n\n\nA | B\n\narray([False,  True,  True,  True])\n\n\n\nA & B\n\narray([False, False, False,  True])\n\n\n\n\nVector indexing\n\nArrays can be indexed by boolean arrays instead of ranges.\nOnly elements corresponding to true are retrieved\n\n\nx = np.linspace(0,1,6)\nx\n\narray([0. , 0.2, 0.4, 0.6, 0.8, 1. ])\n\n\n\n# indexes such that (x^2) &gt; (x/2)\nx**2 &gt; (x/2)\n\narray([False, False, False,  True,  True,  True])\n\n\n\ncond = x**2 &gt; (x/2)\nx[ cond ] \n\narray([0.6, 0.8, 1. ])\n\n\n\n\nGoing further: broadcasting rules\n\nNumpy library has defined very consistent conventions, to match inconsistent dimensions.\nIgnore them for now…\n\n\nM = np.eye(4)\nM\n\narray([[1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [0., 0., 1., 0.],\n       [0., 0., 0., 1.]])\n\n\n\nM[2:4, 2:4] = 0.5 # float\nM\n\narray([[1. , 0. , 0. , 0. ],\n       [0. , 1. , 0. , 0. ],\n       [0. , 0. , 0.5, 0.5],\n       [0. , 0. , 0.5, 0.5]])\n\n\n\nM[:,:2] = np.array([[0.1, 0.2]])  # 1x2 array\nM\n\narray([[0.1, 0.2, 0. , 0. ],\n       [0.1, 0.2, 0. , 0. ],\n       [0.1, 0.2, 0.5, 0.5],\n       [0.1, 0.2, 0.5, 0.5]])\n\n\n\n\nGoing Further\n\nOther useful functions (easy to google):\n\nnp.arange() regularly spaced integers\nnp.where() find elements in\n…"
  },
  {
    "objectID": "session_2/Numerical Python.html#matplotlib",
    "href": "session_2/Numerical Python.html#matplotlib",
    "title": "Numerical Python",
    "section": "Matplotlib",
    "text": "Matplotlib\n\nMatplotlib\n\nmatplotlib is …\nobject oriented api optional Matlab-like syntax\nmain function is plt.plot(x,y) where x and y are vectors (or iterables like lists)\n\nlots of optional arguments\n\n\n\nfrom matplotlib import pyplot as plt\n\n\n\nExample\n\nx = np.linspace(-1,1,6)\n\n\ny = np.sin(x)/x # sinus cardinal\n\n\nplt.plot(x,y,'o')\nplt.plot(x,y)\n\n\n\n\n\n\nExample (2)\n\nx = np.linspace(-5,5,100)\n\nfig = plt.figure() # keep a figure open to draw on it\nfor k in range(1,5):\n    y = np.sin(x*k)/(x*k)\n    plt.plot(x, y, label=f\"$sinc({k} x)$\") # label each line\nplt.plot(x, x*0, color='black', linestyle='--')\nplt.grid(True) # add a grid\nplt.title(\"Looking for the right hat.\")\nplt.legend(loc=\"upper right\")\n\n&lt;matplotlib.legend.Legend at 0x7f8772c5e610&gt;\n\n\n\n\n\n\n\nExample (3)\n\nx = np.linspace(-5,5,100)\n\nplt.figure()\nplt.subplot(2,2,1) # create a 2x2 subplot and draw in first quadrant\nplt.plot(x,x)\nplt.subplot(2,2,2) # create a 2x2 subplot and draw in second quadrant\nplt.plot(x,-x)\nplt.subplot(2,2,3) # create a 2x2 subplot and draw in third quadrant\nplt.plot(x,-x)\nplt.subplot(2,2,4) # create a 2x2 subplot and draw in fourth quadrant\nplt.plot(x,x)\n\nplt.tight_layout() # save some space\n\n\n\n\n\n\nAlternatives to matplotlib\n\nplotly (nice javascript graphs)\naltair (good for datavisualisation/interactivity)\n\npython wrapper to Vega-lite"
  },
  {
    "objectID": "session_2/Numerical Python.html#next",
    "href": "session_2/Numerical Python.html#next",
    "title": "Numerical Python",
    "section": "Next",
    "text": "Next\n\nDataFrames and pandas"
  },
  {
    "objectID": "session_3/graphs/inference.html",
    "href": "session_3/graphs/inference.html",
    "title": "Data-Based Economics",
    "section": "",
    "text": "from matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n\ndef generate_dataset(μ1, μ2, α, β, σ, N=10):\n    xvec = np.random.uniform(μ1, μ2, N)\n    yvec = α + β*xvec + np.random.normal(size=N)*σ\n    return pd.DataFrame({'x': xvec, 'y': yvec})\n\n\ndf = generate_dataset(0.0, 1.0, 0.1, 0.8, 0.1)\n\n\nplt.plot(df['x'], df['y'], 'o')\nplt.grid()\n\n\n\n\n\ndef plot_distribution(α, β, σ, N=100000, μ1=0.0, μ2=1.0):\n    xvec = np.random.uniform(μ1, μ2, N)\n    yvec = α + β*xvec + np.random.normal(size=N)*σ\n    plt.plot(xvec, yvec, '.r', alpha=0.005)\n    plt.plot(xvec, α + β*xvec, color='black')\n\n# missing ridge line\n\n\nimport statsmodels\n\n\nμ1 = 0\nμ2 = 1.0\nα = 0.1\nβ = 0.8\nσ = 0.2\nN = 20\nK = 1000\n\n\nimport statsmodels.formula.api as smf\n\n\ndf = generate_dataset(μ1, μ2, α, β, σ, N=N)\n\n\nres = smf.ols(formula='y ~ x + 1', data=df).fit()\nparams = res.params\nαhat = params['Intercept']\nβhat = params['x']\nσhat = res.resid.std()\n\n\nres.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\ny\nR-squared:\n0.692\n\n\nModel:\nOLS\nAdj. R-squared:\n0.675\n\n\nMethod:\nLeast Squares\nF-statistic:\n40.48\n\n\nDate:\nTue, 26 Jan 2021\nProb (F-statistic):\n5.41e-06\n\n\nTime:\n04:02:36\nLog-Likelihood:\n7.6662\n\n\nNo. Observations:\n20\nAIC:\n-11.33\n\n\nDf Residuals:\n18\nBIC:\n-9.341\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n0.1210\n0.077\n1.565\n0.135\n-0.041\n0.283\n\n\nx\n0.7941\n0.125\n6.362\n0.000\n0.532\n1.056\n\n\n\n\n\n\nOmnibus:\n1.410\nDurbin-Watson:\n1.507\n\n\nProb(Omnibus):\n0.494\nJarque-Bera (JB):\n0.890\n\n\nSkew:\n-0.081\nProb(JB):\n0.641\n\n\nKurtosis:\n1.979\nCond. No.\n4.20\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nres.predict(df['x'])\n\n0     0.326200\n1     0.211704\n2     0.798819\n3     0.603306\n4     0.573319\n5     0.823919\n6     0.740622\n7     0.503227\n8     0.292622\n9     0.489566\n10    0.138720\n11    0.355157\n12    0.594171\n13    0.883917\n14    0.266229\n15    0.827021\n16    0.912376\n17    0.163088\n18    0.684858\n19    0.732782\ndtype: float64\n\n\n\nfor i in [1,2,3]:\n    \n    fig = plt.figure(figsize=(10,14))\n    plt.subplot(311)\n    plot_distribution(0.1, 0.8, 0.2)\n    plt.grid()\n    plt.title(f\"True Distribution: $y = {α:.2f} + {β:.2f} x + {σ:.2f} u$\")\n    plt.xlim(0,1)\n    plt.ylim(-0.5, 1.5)\n\n    plt.subplot(312)\n    plt.xlim(0,1)\n    plt.ylim(-0.5, 1.5)\n    if i&gt;=2:\n        plt.plot(df['x'], df['y'], 'o')\n    if i&gt;=3:\n        plt.plot(df['x'], res.predict(), label=f'$\\hat{{α}}={αhat:.2f}; \\hat{{β}}={βhat:.2f}$')\n        plt.legend(loc='lower right')\n    plt.title(\"Random Draw\")\n    plt.grid()\n    \n    plt.savefig(f\"regression_uncertainty_{i}.png\")\n\n\n\n\n\n\n\n\n\n\n\nimport scipy.stats\n\n\ndatasets = [generate_dataset(μ1, μ2, αhat, βhat, σhat, N=N) for i in range(K)]\nall_params = [smf.ols(formula='x ~ y + 1', data=df).fit() for df in datasets]\nαvec = np.array( [e.params['Intercept'] for e in all_params] )\nβvec = np.array( [e.params['y'] for e in all_params] )\n\n\ngkd = scipy.stats.kde.gaussian_kde(βvec)\n\n\nfor i in [1,2,3,4,5,6,7,8,9,10,100]:\n\n    fig = plt.figure(figsize=(10,14))\n    plt.subplot(311)\n    plot_distribution(0.1, 0.8, 0.2)\n    plt.grid()\n    plt.title(f\"True Distribution: $y = {αhat:.2f} + {βhat:.2f} x + {σhat:.2f} u$\")\n    plt.xlim(0,1)\n    plt.ylim(-0.5, 1.5)\n    \n    plt.subplot(312)\n    plt.xlim(0,1)\n    plt.ylim(-0.5, 1.5)\n    df = datasets[i]\n    if i&gt;=2:\n        plt.plot(df['x'], df['y'], 'o')\n    plt.title(\"Random Draw\")\n    plt.grid()\n\n    plt.subplot(313)\n    if i==3:\n        plt.plot(βvec[i], βvec[i]*0, 'o')\n    if i&gt;4:\n        plt.plot(βvec[3:i], βvec[3:i]*0, 'o')\n    if i&gt;10:\n        xx = np.linspace(0.2, 1.4, 10000)\n        plt.plot( βvec, gkd.pdf(βvec), '.')\n    plt.title(\"Distribution of β\")\n    plt.xlim(0.2, 1.4)\n    plt.ylim(-0.1, 4)\n    plt.grid()\n\n    plt.tight_layout()\n\n    plt.savefig(f\"random_estimates_{i}.png\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplt.plot( βvec, βvec*0, 'o')"
  },
  {
    "objectID": "session_3/graphs/Untitled1.html",
    "href": "session_3/graphs/Untitled1.html",
    "title": "Data-Based Economics",
    "section": "",
    "text": "from matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n\nimport statsmodels.api as sm\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nincome\neducation\nprestige\n\n\n\n\ncount\n45.000000\n45.000000\n45.000000\n\n\nmean\n41.866667\n52.555556\n47.688889\n\n\nstd\n24.435072\n29.760831\n31.510332\n\n\nmin\n7.000000\n7.000000\n3.000000\n\n\n25%\n21.000000\n26.000000\n16.000000\n\n\n50%\n42.000000\n45.000000\n41.000000\n\n\n75%\n64.000000\n84.000000\n81.000000\n\n\nmax\n81.000000\n100.000000\n97.000000\n\n\n\n\n\n\n\n\ndf.cov()\n\n\n\n\n\n\n\n\nincome\neducation\nprestige\n\n\n\n\nincome\n597.072727\n526.871212\n645.071212\n\n\neducation\n526.871212\n885.707071\n798.904040\n\n\nprestige\n645.071212\n798.904040\n992.901010\n\n\n\n\n\n\n\n\nfrom matplotlib import pyplot as plt\n\n\nplt.figure(figsize=(8,6))\nplt.plot(df['education'],df['income'],'o')\nplt.grid()\nplt.xlabel(\"x (Education)\")\nplt.ylabel(\"y (Income)\")\nplt.savefig(\"data_description.png\")\n\n\n\n\n\nfor i in [1,2,3]:\n    xvec = np.linspace(10,100)\n\n    plt.figure(figsize=(12,8))\n    plt.plot(df['education'],df['income'],'o')\n\n    plt.plot(xvec, xvec * 0 + 50)\n    if i&gt;=2:\n        plt.plot(xvec, xvec )\n    if i&gt;=3:\n        plt.plot(xvec,  90- 0.6*xvec )\n\n    plt.grid()\n    plt.xlabel(\"x (Education)\")\n    plt.ylabel(\"y (Income)\")\n    plt.savefig(f\"which_line_{i}.png\")\n\n\n\n\n\n\n\n\n\n\n\nfrom ipywidgets import interact\n\n\nimport matplotlib.patches as patches\n\n\na = 0.1\nb = 1.0\nind = 23\n\n\napprox =  a + b*xvec\n\n# Create figure and axes\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\nplt.plot(df['education'],df['income'],'o')\nplt.plot(xvec, approx, color='red')\n\nx, y = df['education'][ind], df['income'][ind]\nplt.plot(x, y, 'o', color='red' )\np = a+b*x\nplt.grid(True)\nh = abs(p-y)\nplt.vlines(x, y+h, y, color='red')\n\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.savefig(f\"error_0.png\")\n\n\n\n\n\nplt.vlines?\n\n\nSignature:\nplt.vlines(\n    x,\n    ymin,\n    ymax,\n    colors=None,\n    linestyles='solid',\n    label='',\n    *,\n    data=None,\n    **kwargs,\n)\nDocstring:\nPlot vertical lines.\nPlot vertical lines at each *x* from *ymin* to *ymax*.\nParameters\n----------\nx : float or array-like\n    x-indexes where to plot the lines.\nymin, ymax : float or array-like\n    Respective beginning and end of each line. If scalars are\n    provided, all lines will have same length.\ncolors : list of colors, default: :rc:`lines.color`\nlinestyles : {'solid', 'dashed', 'dashdot', 'dotted'}, optional\nlabel : str, default: ''\nReturns\n-------\n`~matplotlib.collections.LineCollection`\nOther Parameters\n----------------\n**kwargs : `~matplotlib.collections.LineCollection` properties.\nSee Also\n--------\nhlines : horizontal lines\naxvline: vertical line across the axes\nNotes\n-----\n.. note::\n    In addition to the above described arguments, this function can take\n    a *data* keyword argument. If such a *data* argument is given,\n    the following arguments can also be string ``s``, which is\n    interpreted as ``data[s]`` (unless this raises an exception):\n    *x*, *ymin*, *ymax*, *colors*.\n    Objects passed as **data** must support item access (``data[s]``) and\n    membership test (``s in data``).\nFile:      ~/.local/opt/miniconda/lib/python3.8/site-packages/matplotlib/pyplot.py\nType:      function\n\n\n\n\n\na = 0.1\nb = 1.0\nind = 23\n\n\napprox =  a + b*xvec\n\n# Create figure and axes\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\nplt.plot(df['education'],df['income'],'o')\nplt.plot(xvec, approx, color='red')\n\nx, y = df['education'][ind], df['income'][ind]\nplt.plot(x, y, 'o', color='red' )\np = a+b*x\nplt.grid(True)\nh = abs(p-y)\nif p-y&gt;0:\n    # Create a Rectangle patch\n    rect = patches.Rectangle((x,y),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n    ax.add_patch(rect)\n    \nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.savefig(f\"errors_{1}.png\")\n\n\n\n\n\ndef L(a,b):\n    Δ = a + b*df['education'] - df['income']\n    return (Δ**2).sum()\n\n\na = 0.1\nb = 0.8\n\napprox =  a + b*xvec\n\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\n\n\nplt.plot(df['education'],df['income'],'o', label=f\"L({a,b})={L(a,b)}\")\nplt.plot(xvec, approx, color='red')\n\nplt.grid(True)\nfor ind in range(df.shape[0]):\n    \n    x, y = df['education'][ind], df['income'][ind]\n    p = a+b*x\n\n    h = abs(p-y)\n    if p-y&gt;0:\n        # Create a Rectangle patch\n        rect = patches.Rectangle((x,y),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\n    else:\n        rect = patches.Rectangle((x,y-h),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.legend(loc='upper right')\nplt.savefig(f\"errors_2.png\")\n\n\n\n\n\na = 90\nb = -0.6\n\napprox =  a + b*xvec\n\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\n\n\nplt.plot(df['education'],df['income'],'o', label=f\"L({a,b})={L(a,b)}\")\nplt.plot(xvec, approx, color='red')\n\nplt.grid(True)\nfor ind in range(df.shape[0]):\n    \n    x, y = df['education'][ind], df['income'][ind]\n    p = a+b*x\n\n    h = abs(p-y)\n    if p-y&gt;0:\n        # Create a Rectangle patch\n        rect = patches.Rectangle((x,y),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\n    else:\n        rect = patches.Rectangle((x,y-h),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.legend(loc='upper right')\nplt.savefig(f\"errors_3.png\")\n\n\n\n\n\nimport scipy.optimize\n\n\nscipy.optimize.minimize(lambda x: L(x[0], x[1]),np.array([0.5, 0.5]))\n\n      fun: 12480.970174488397\n hess_inv: array([[ 7.14169839e-09, -3.91281920e-09],\n       [-3.91281920e-09,  2.46663613e-09]])\n      jac: array([0.00024414, 0.00012207])\n  message: 'Desired error not necessarily achieved due to precision loss.'\n     nfev: 57\n      nit: 7\n     njev: 19\n   status: 2\n  success: False\n        x: array([10.60350224,  0.59485938])\n\n\n\na = 10\nb = 0.59\n\napprox =  a + b*xvec\n\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\n\n\nplt.plot(df['education'],df['income'],'o', label=f\"L({a,b})={L(a,b)}\")\nplt.plot(xvec, approx, color='red')\n\nplt.grid(True)\nfor ind in range(df.shape[0]):\n    \n    x, y = df['education'][ind], df['income'][ind]\n    p = a+b*x\n\n    h = abs(p-y)\n    if p-y&gt;0:\n        # Create a Rectangle patch\n        rect = patches.Rectangle((x,y),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\n    else:\n        rect = patches.Rectangle((x,y-h),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.legend(loc='upper right')\nplt.savefig(f\"errors_4.png\")\n\n\n\n\n\na = 10\nb = 0.59\n\napprox =  a + b*xvec\n\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\n\n\nplt.plot(df['education'],df['income'],'o', label=f\"L({a,b})={L(a,b)}\")\nplt.plot(xvec, approx, color='red', alpha=0.5)\n\nplt.plot(60, a + b*60, 'o', color='red',)\n\nprint(a+b*60)\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.legend(loc='upper right')\nplt.savefig(f\"prediction.png\")\n\n45.4\n\n\n\n\n\n\na = 10\nb = 0.59\n\napprox =  (a + b*df['education'] - df['income'])\n\nplt.figure(figsize=(12,6))\n\nplt.subplot(121)\nplt.plot(approx)\nplt.grid(False)\nplt.title(\"Residuals\")\n\n\nplt.subplot(122)\ndistplot(approx)\nplt.title(\"Distribution of residuals\")\nplt.grid()\n\nplt.savefig(\"residuals.png\")\n\n/home/pablo/.local/opt/miniconda/lib/python3.8/site-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n  warnings.warn(msg, FutureWarning)\n\n\n\n\n\n\n(a + b*df['education'] - df['income']).std()\n\n16.842782676352154\n\n\n\n\n\n/home/pablo/.local/opt/miniconda/lib/python3.8/site-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n  warnings.warn(msg, FutureWarning)\n\n\n&lt;AxesSubplot:ylabel='Density'&gt;\n\n\n\n\n\n\nfrom scipy.stats import f\n\n\nf(0.3)\n\nTypeError: _parse_args() missing 1 required positional argument: 'dfd'\n\n\n\nnp.rand\n\n\nK = 100\nxvec = np.linspace(0,1,K)\ne1 = np.random.randn(K)*0.1\nyvec = 0.1 + xvec*0.4 + e1\ne2 = np.random.randn(K)*0.05\nyvec2 = 0.1 + xvec*(xvec-1)/2 + e2\ne3 = np.random.randn(K)*xvec/2\nyvec3 = 0.1 + xvec + e3\n\nyvec4 = 0.1 + np.sin(xvec*6) + np.random.randn(K)*xvec/2\n\n\nfrom dolo.numeric.processes import VAR1\n\n\nsim = VAR1( ρ=0.8, Σ=0.001).simulate(N=1,T=100)\nyvec4 = 0.1 + xvec*0.4 + sim.ravel()\n\n\nplt.figure(figsize=(18,6))\nplt.subplot(241)\nplt.plot(xvec, yvec,'o')\nplt.plot(xvec, 0.1 + xvec*0.4 )\nplt.ylabel(\"Series\")\nplt.title(\"white noise\")\nplt.subplot(242)\nplt.plot(xvec, yvec2, 'o')\nplt.plot(xvec, yvec2*0)\nplt.title('nonlinear')\nplt.subplot(243)\nplt.plot(xvec, yvec3,'o')\nplt.plot(xvec, 0.1 + xvec)\nplt.title('heteroskedastic')\nplt.subplot(244)\nplt.plot(xvec, yvec4,'o')\nplt.plot(xvec, xvec*0.6)\n\nplt.title('correlated')\n\n\nplt.subplot(245)\nplt.plot(xvec, e1,'o')\nplt.ylabel(\"Residuals\")\nplt.subplot(246)\nplt.plot(xvec, yvec2-0.075, 'o')\n\nplt.subplot(247)\nplt.plot(xvec, e3,'o')\nplt.subplot(248)\nplt.plot(xvec, sim.ravel(),'o')\n\nplt.tight_layout()\n\nplt.savefig(\"residuals_circus.png\")"
  },
  {
    "objectID": "session_3/slides.html#descriptive-statistics",
    "href": "session_3/slides.html#descriptive-statistics",
    "title": "Linear Regression",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics"
  },
  {
    "objectID": "session_3/slides.html#fitting-the-data",
    "href": "session_3/slides.html#fitting-the-data",
    "title": "Linear Regression",
    "section": "Fitting the data",
    "text": "Fitting the data"
  },
  {
    "objectID": "session_3/slides.html#explained-variance",
    "href": "session_3/slides.html#explained-variance",
    "title": "Linear Regression",
    "section": "Explained Variance",
    "text": "Explained Variance"
  },
  {
    "objectID": "session_3/slides.html#statistical-inference",
    "href": "session_3/slides.html#statistical-inference",
    "title": "Linear Regression",
    "section": "Statistical inference",
    "text": "Statistical inference"
  },
  {
    "objectID": "session_3/slides.html#now-lets-practice",
    "href": "session_3/slides.html#now-lets-practice",
    "title": "Linear Regression",
    "section": "Now let’s practice",
    "text": "Now let’s practice"
  },
  {
    "objectID": "session_3/index.html#descriptive-statistics",
    "href": "session_3/index.html#descriptive-statistics",
    "title": "Linear Regression",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\n\n\nA Simple Dataset\n\nDuncan’s Occupational Prestige Data\nMany occupations in 1950.\n\\(x\\): education\n\nPercentage of occupational incumbents in 1950 who were high school graduates\n\n\\(y\\): income\n\nPercentage of occupational incumbents in the 1950 US Census who earned $3,500 or more per year\n\n\\(z\\): Percentage of respondents in a social survey who rated the occupation as “good” or better in prestige\n\n\n\n\nQuick look\n\n\nimport statsmodels.api as sm\ndataset = sm.datasets.get_rdataset(\"Duncan\", \"carData\")\ndf = dataset.data\ndf.head()\n\n\n\n\ntype\nincome\neducation\nprestige\n\n\n\n\naccountant\nprof\n62\n86\n82\n\n\npilot\nprof\n72\n76\n83\n\n\narchitect\nprof\n75\n92\n90\n\n\nauthor\nprof\n55\n90\n76\n\n\nchemist\nprof\n64\n86\n90\n\n\n\n\n\nplt.plot(df['education'],df['income'],'o')\nplt.grid()\nplt.xlabel(\"Education\")\nplt.ylabel(\"Income\")\nplt.savefig(\"data_description.png\")\n\n\n\n\n\n\nDescriptive Statistics\n\n\n\n\nFor any variable \\(v\\) with \\(N\\) observations:\n\nmean: \\(\\overline{v} = \\frac{1}{N} \\sum_{i=1}^N v_i\\)\nvariance \\(V({v}) = \\frac{1}{N} \\sum_{i=1}^N \\left(v_i - \\overline{v} \\right)^2\\)\nstandard deviation : \\(\\sigma(v)=\\sqrt{V(v)}\\)\n\n\n\n\ndf.describe()\n\n\n\n\nincome\neducation\nprestige\n\n\n\n\ncount\n45.000000\n45.000000\n45.000000\n\n\nmean\n41.866667\n52.555556\n47.688889\n\n\nstd\n24.435072\n29.760831\n31.510332\n\n\nmin\n7.000000\n7.000000\n3.000000\n\n\n25 %\n21.000000\n26.000000\n16.000000\n\n\n50 %\n42.000000\n45.000000\n41.000000\n\n\n75 %\n64.000000\n84.000000\n81.000000\n\n\nmax\n81.000000\n100.000000\n97.000000\n\n\n\n\n\n\n\nRelation between variables\n\nHow do we measure relations between two variables (with \\(N\\) observations)\n\nCovariance: \\(Cov(x,y) = \\frac{1}{N}\\sum_i (x_i-\\overline{x})(y_i-\\overline{y})\\)\nCorrelation: \\(Cor(x,y) = \\frac{Cov(x,y)}{\\sigma(x)\\sigma(y)}\\)\n\nBy construction, \\(Cor(x,y)\\in[-1,1]\\)\n\nif \\(Cor(x,y)&gt;0\\), x and y are positively correlated\nif \\(Cor(x,y)&lt;0\\), x and y are negatively correlated\n\nInterpretation:\n\n\nno interpretation!\ncorrelation is not causality\nalso: data can be correlated by pure chance (spurious correlation)\n\n\n\n\n\nExamples\n\n\ndf.cov()\n\n\n\n\nincome\neducation\nprestige\n\n\n\n\nincome\n597.072727\n526.871212\n645.071212\n\n\neducation\n526.871212\n885.707071\n798.904040\n\n\nprestige\n645.071212\n798.904040\n992.901010\n\n\n\n\n\n\ndf.corr()\n\n\n\n\nincome\neducation\nprestige\n\n\n\n\nincome\n1.000000\n0.724512\n0.837801\n\n\neducation\n0.724512\n1.000000\n0.851916\n\n\nprestige\n0.837801\n0.851916\n1.000000"
  },
  {
    "objectID": "session_3/index.html#fitting-the-data",
    "href": "session_3/index.html#fitting-the-data",
    "title": "Linear Regression",
    "section": "Fitting the data",
    "text": "Fitting the data\n\n\nA Linear Model\n\n\n\nConsider the line: \\[y = α + β x\\]\n\nSeveral possibilities.\n\nWhich one do we choose to represent the model?\n\nNeed some criterium.\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeast Square Criterium\n\n\n\n\n\nCompare the model to the data: \\[y_i = \\alpha + \\beta x_i + \\underbrace{e_i}_{\\text{prediction error}}\\] - Square Errors \\[{e_i}^2 = (y_i-\\alpha-\\beta x_i)^2\\]\n\nLoss Function: sum of squares \\[L(\\alpha,\\beta) = \\sum_{i=1}^N (e_i)^2\\]\n\n\n\n\n\n\n\n\n\n\n\n\nMinimizing Least Squares\n\n\n\n\n\nTry to chose \\(\\alpha, \\beta\\) so as to minimize the sum of the squares \\(L(α, β)\\)\n\nIt is a convex minimization problem: unique solution\n\nThis direct iterative procedure is used in machine learning\n\n\n\n\n  \n\n\n\n\n\nOrdinary Least Squares (1)\n\nThe mathematical problem \\(\\min_{\\alpha,\\beta} L(\\alpha,\\beta)\\) has one unique solution\n\nproof not important here\n\nSolution is given by the explicit formula: \\[\\hat{\\alpha} = \\overline{y} - \\hat{\\beta} \\overline{x}\\] \\[\\hat{\\beta} = \\frac{Cov({x,y})}{Var(x)} = Cor(x,y) \\frac{\\sigma(y)}{\\sigma({x})}\\]\n\\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) are estimators.\n\nHence the hats.\nMore on that later.\n\n\n\n\n\nConcrete Example\n\nIn our example: \\[\\underbrace{y}\\_{\\text{income}} = 10 + 0.59 \\underbrace{x}\\_{education}\\]\nWe can say that income and education are positively correlated\nWe can say that a unit increase in education is associated with a 0.59 increase in income\nWe can say that a unit increase in education explains a 0.59 increase in income\nBut:\n\nhere explains does not mean cause"
  },
  {
    "objectID": "session_3/index.html#explained-variance",
    "href": "session_3/index.html#explained-variance",
    "title": "Linear Regression",
    "section": "Explained Variance",
    "text": "Explained Variance\n\n\nPredictions\n\nIt is possible to make predictions with the model:\n\nHow much would an occupation which hires 60% high schoolers fare salary-wise?\n\n\n\n\nPrediction: salary measure is \\(45.4\\)\nOK, but that seems noisy, how much do I really predict ? Can I get a sense of the precision of my prediction ?\n\n\n\n\nLook at the residuals\n\n\n\nPlot the residuals: \n\n\n\n\nAny abnormal observation?\nTheory requires residuals to be:\n\nzero-mean\nnon-correlated\nnormally distributed\n\nThat looks like a normal distribution\n\nstandard deviation is \\(\\sigma(e_i) = 16.84\\)\n\nA more honnest prediction would be \\(45.6 ± 16.84\\)\n\n\n\n\n\n\nWhat could go wrong\n\n\na well specified model, residuals must look like white noise (i.i.d.: independent and identically distributed)\nwhen residuals are clearly abnormal, the model must be changed\n\n\n\n\nExplained Variance\n\n\n\n\nWhat is the share of the total variance explained by the variance of my prediction? \\[R^2 = \\frac{\\overbrace{Var(\\alpha + \\beta x_i)}^{ \\text{MSS} } } {\\underbrace{Var(y_i)}\\_{ \\text{TSS} } } = \\frac{MSS}{TSS} = (Cor(x,y))^2\\] \\[R^2 = 1-\\frac{\\overbrace{Var(y_i - \\alpha + \\beta x_i)}^{\\text{RSS}} } { \\underbrace{Var(y_i)}\\_{ {\\text{TSS}  }}} = 1 - \\frac{RSS}{TSS} \\]\n\nMSS: model sum of squares, explained variance\nRSS: residual sum of square, unexplained variance\nTSS: total sum of squares, total variance\n\n\n\n\n\n\nCoefficient of determination is a measure of the explanatory power of a regression\n\nbut not of the significance of a coefficient\nwe’ll get back to it when we see multivariate regressions\n\n\nIn one-dimensional case, it is possible to have small R2, yet a very precise regression coefficient.\n\n\n\n\n\nGraphical Representation"
  },
  {
    "objectID": "session_3/index.html#statistical-inference",
    "href": "session_3/index.html#statistical-inference",
    "title": "Linear Regression",
    "section": "Statistical inference",
    "text": "Statistical inference\n\n\nStatistical model\n\n\n\n\n\nImagine the true model is: \\[y = α + β x + \\epsilon\\] \\[\\epsilon\\_i  \\sim \\mathcal{N}\\left({0,\\sigma^{2}}\\right)\\]\n\nerrors are independent …\nand normallly distributed …\nwith constant variance (homoscedastic)\n\n\nUsing this data-generation process, I have drawns randomly \\(N\\) data points (a.k.a. gathered the data)\n\nmaybe an acual sample (for instance \\(N\\) patients)\nan abstract sample otherwise\n\n\nThen computed my estimate \\(\\hat{α}\\), \\(\\hat{β}\\)\n\nHow confident am I in these estimates ?\n\nI could have gotten a completely different one…\nclearly, the bigger \\(N\\), the more confident I am…\n\n\n\n\n\n  \n\n\n\n\n\nStatistical inference (2)\n\n\n\n\n\nAssume we have computed \\(\\hat{\\alpha}\\), \\(\\hat{\\beta}\\) from the data. Let’s make a thought experiment instead.\n\nImagine the actual data generating process was given by \\(\\hat{α} + \\hat{\\beta} x + \\epsilon\\) where \\(\\epsilon \\sim \\mathcal{N}(0,Var({e_i}))\\)\n\nIf I draw randomly \\(N\\) points using this D.G.P. I get new estimates.\n\nAnd if I make randomly many draws, I get a distribution for my estimate.\n\nI get an estimated \\(\\hat{\\sigma}(\\beta)\\)\nwere my initial estimates very likely ?\nor could they have taken any value with another draw from the data ?\nin the example, we see that estimates around of 0.7 or 0.9, would be compatible with the data\n\n\nHow do we formalize these ideas?\n\nStatistical tests.\n\n\n\n\n\n\n&lt;img src=\"graphs/random_estimates_1.png\" class=\"fragment current-visible\" data-fragment-index=2&gt;\n&lt;img src=\"graphs/random_estimates_2.png\" class=\"fragment current-visible\" data-fragment-index=3&gt;\n&lt;img src=\"graphs/random_estimates_3.png\" class=\"fragment current-visible\" data-fragment-index=4&gt;\n&lt;img src=\"graphs/random_estimates_4.png\" class=\"fragment current-visible\" data-fragment-index=5&gt;\n&lt;img src=\"graphs/random_estimates_5.png\" class=\"fragment current-visible\" data-fragment-index=6&gt;\n&lt;img src=\"graphs/random_estimates_6.png\" class=\"fragment current-visible\" data-fragment-index=7&gt;\n&lt;img src=\"graphs/random_estimates_7.png\" class=\"fragment current-visible\" data-fragment-index=8&gt;\n&lt;img src=\"graphs/random_estimates_8.png\" class=\"fragment current-visible\" data-fragment-index=9&gt;\n&lt;img src=\"graphs/random_estimates_9.png\" class=\"fragment current-visible\" data-fragment-index=10&gt;\n&lt;img src=\"graphs/random_estimates_10.png\" class=\"fragment current-visible\" data-fragment-index=11&gt;\n&lt;img src=\"graphs/random_estimates_100.png\" class=\"fragment\" data-fragment-index=12&gt;\n\n\n\n\n\n\n\n\nFirst estimates\n\n\nGiven the true model, all estimators are random variables of the data generating process\n\nGiven the values \\(\\alpha\\), \\(\\beta\\), \\(\\sigma\\) of the true model, we can model the distribution of the estimates.\n\nSome closed forms:\n\n\\(\\hat{\\sigma}^2 = Var(y_i - \\alpha -\\beta x_i)\\) estimated variance of the residuals\n$mean() = () $ (unbiased)\n\\(\\sigma(\\hat{\\beta}) = \\frac{\\sigma^2}{Var(x_i)}\\)\n\n\nThese statististics or any functon of them can be computed exactly, *given the data().\n\nTheir distribution depends, on the data-generating process\n\nCan we produce statistics whose distribution is known given mild assumptions on the data-generating process?\n\nif so, we can assess how likely are our observations\n\n\n\n\n\nFisher-Statistic\n\n\n\n\n\nTest\n\nHypothesis H0: \\(α=β=0\\) (model explains nothing)\nHypothesis H1: (model explains something)\nFisher Statistics: \\(\\boxed{F=\\frac{Explained Variance}{Unexplained Variance}}\\)\n\n\n\n\n\nDistribution of \\(F\\) is known theoretically.\n\nAssuming the model is actually linear and the shocks normal.\nIt depends on the number of degrees of Freedom. (Here \\(N-2=18\\))\nNot on the actual parameters of the model.\n\n\n\n\n\nIn our case, \\(Fstat=40.48\\). What was the probability it was that big, under the \\(H0\\) hypothesis?\n\nextremely small: \\(Prob(F&gt;Fstat|H0)=5.41e-6\\)\nwe can reject \\(H0\\) with \\(p-value=5e-6\\)\n\nIn social science, typical required p-value is 5%.\n\n\n\n\n\n\n\n\n\n\n\nStudent test\n\nSo our estimate is \\(y = \\underbrace{0.121}\\_{\\tilde{\\alpha}} + \\underbrace{0.794}\\_{\\tilde{\\beta}} x\\).\n\nwe know \\(\\tilde{\\beta}\\) is a bit random (it’s an estimator)\nare we even sure \\(\\tilde{\\beta}\\) could not have been zero?\n\nStudent Test:\n\nH0: \\(\\beta=0\\)\nH1: \\(\\beta \\neq 0\\)\nStatistics: \\(t=\\frac{\\hat{\\beta}}{\\sigma(\\hat{\\beta})}\\)\n\nintuitively: compare mean of estimator to its standard deviation\nalso a function of degrees of freedom\n\n\nSignificance levels (read in a table or use software):\n\nfor 18 degrees of freedom, \\(P(|t|&gt;t^{\\star})=0.05\\) with \\(t^{\\star}=1.734\\)\nif \\(t&gt;t^{\\star}\\) we are \\(95%\\) confident the coefficient is significant\n\n\n\n\n\nStudent tables\n\n\n\n\nConfidence intervals\n\nThe student test can also be used to construct confidence intervals.\nGiven estimate, \\(\\hat{\\beta}\\) with standard deviation \\(\\sigma(\\hat{\\beta})\\)\nGiven a probability threshold \\(\\alpha\\) (for instance \\(\\alpha=0.05\\)) we can compute \\(t^{\\star}\\) such that \\(P(|t|&gt;t*)=\\alpha\\)\nWe construct the confidence interval:\n\n\\[I^{\\alpha} = [\\hat{\\beta}-t\\sigma(\\hat{\\beta}), \\hat{\\beta}+t\\sigma(\\hat{\\beta})]\\]\n\nInterpretation: given the estimated value, one is 95 % sure (\\(1-\\alpha\\)) the estimated parameter falls in this interval"
  },
  {
    "objectID": "session_3/index.html#now-lets-practice",
    "href": "session_3/index.html#now-lets-practice",
    "title": "Linear Regression",
    "section": "Now let’s practice",
    "text": "Now let’s practice"
  },
  {
    "objectID": "session_3/Philips_curve.html",
    "href": "session_3/Philips_curve.html",
    "title": "Visualizing the Philips Curve using matplotlib and altair",
    "section": "",
    "text": "Our goal here is to empirically represent the Philips curve. We import the data first using dbnomics, then use two different plotting libraries to construct the plots."
  },
  {
    "objectID": "session_3/Philips_curve.html#importing-the-data",
    "href": "session_3/Philips_curve.html#importing-the-data",
    "title": "Visualizing the Philips Curve using matplotlib and altair",
    "section": "Importing the Data",
    "text": "Importing the Data\nWe start by loading library dbnomics. It is installed on the Nuvolos servers (just change the kernel to “ESCPython”)\n\nimport dbnomics\n\nThe following code imports data for from dbnomics for a few countries.\n\ntable_1 = dbnomics.fetch_series([\n    \"OECD/DP_LIVE/FRA.CPI.TOT.AGRWTH.Q\",\n    \"OECD/DP_LIVE/GBR.CPI.TOT.AGRWTH.Q\",\n    \"OECD/DP_LIVE/USA.CPI.TOT.AGRWTH.Q\",\n    \"OECD/DP_LIVE/DEU.CPI.TOT.AGRWTH.Q\"\n])\n\n\ntable_2 = dbnomics.fetch_series([\n    \"OECD/MEI/DEU.LRUNTTTT.STSA.Q\",\n    \"OECD/MEI/FRA.LRUNTTTT.STSA.Q\",\n    \"OECD/MEI/USA.LRUNTTTT.STSA.Q\",\n    \"OECD/MEI/GBR.LRUNTTTT.STSA.Q\"\n])\n\nDescribe concisely the data that has been imported (periodicity, type of measure, …). You can either check dbnomics website or look at the databases.\nShow the first rows of each database. Make a list of all columns.\nCompute averages and standard deviations for unemployment and inflation, per country.\n\n# option 1: by using pandas boolean selection \ntable_1.query(\"Country=='France'\")\n\n\n\n\n\n\n\n\n@frequency\nprovider_code\ndataset_code\ndataset_name\nseries_code\nseries_name\noriginal_period\nperiod\noriginal_value\nvalue\nLOCATION\nINDICATOR\nSUBJECT\nMEASURE\nFREQUENCY\nCountry\nIndicator\nSubject\nMeasure\nFrequency\n\n\n\n\n0\nquarterly\nOECD\nDP_LIVE\nOECD Data Live dataset\nFRA.CPI.TOT.AGRWTH.Q\nFrance – Inflation (CPI) – Total – Annual grow...\n1956-Q1\n1956-01-01\n1.746324\n1.746324\nFRA\nCPI\nTOT\nAGRWTH\nQ\nFrance\nInflation (CPI)\nTotal\nAnnual growth rate (%)\nQuarterly\n\n\n1\nquarterly\nOECD\nDP_LIVE\nOECD Data Live dataset\nFRA.CPI.TOT.AGRWTH.Q\nFrance – Inflation (CPI) – Total – Annual grow...\n1956-Q2\n1956-04-01\n1.838658\n1.838658\nFRA\nCPI\nTOT\nAGRWTH\nQ\nFrance\nInflation (CPI)\nTotal\nAnnual growth rate (%)\nQuarterly\n\n\n2\nquarterly\nOECD\nDP_LIVE\nOECD Data Live dataset\nFRA.CPI.TOT.AGRWTH.Q\nFrance – Inflation (CPI) – Total – Annual grow...\n1956-Q3\n1956-07-01\n2.670692\n2.670692\nFRA\nCPI\nTOT\nAGRWTH\nQ\nFrance\nInflation (CPI)\nTotal\nAnnual growth rate (%)\nQuarterly\n\n\n3\nquarterly\nOECD\nDP_LIVE\nOECD Data Live dataset\nFRA.CPI.TOT.AGRWTH.Q\nFrance – Inflation (CPI) – Total – Annual grow...\n1956-Q4\n1956-10-01\n1.345803\n1.345803\nFRA\nCPI\nTOT\nAGRWTH\nQ\nFrance\nInflation (CPI)\nTotal\nAnnual growth rate (%)\nQuarterly\n\n\n4\nquarterly\nOECD\nDP_LIVE\nOECD Data Live dataset\nFRA.CPI.TOT.AGRWTH.Q\nFrance – Inflation (CPI) – Total – Annual grow...\n1957-Q1\n1957-01-01\n0.543215\n0.543215\nFRA\nCPI\nTOT\nAGRWTH\nQ\nFrance\nInflation (CPI)\nTotal\nAnnual growth rate (%)\nQuarterly\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n263\nquarterly\nOECD\nDP_LIVE\nOECD Data Live dataset\nFRA.CPI.TOT.AGRWTH.Q\nFrance – Inflation (CPI) – Total – Annual grow...\n2021-Q4\n2021-10-01\n2.718014\n2.718014\nFRA\nCPI\nTOT\nAGRWTH\nQ\nFrance\nInflation (CPI)\nTotal\nAnnual growth rate (%)\nQuarterly\n\n\n264\nquarterly\nOECD\nDP_LIVE\nOECD Data Live dataset\nFRA.CPI.TOT.AGRWTH.Q\nFrance – Inflation (CPI) – Total – Annual grow...\n2022-Q1\n2022-01-01\n3.658344\n3.658344\nFRA\nCPI\nTOT\nAGRWTH\nQ\nFrance\nInflation (CPI)\nTotal\nAnnual growth rate (%)\nQuarterly\n\n\n265\nquarterly\nOECD\nDP_LIVE\nOECD Data Live dataset\nFRA.CPI.TOT.AGRWTH.Q\nFrance – Inflation (CPI) – Total – Annual grow...\n2022-Q2\n2022-04-01\n5.288945\n5.288945\nFRA\nCPI\nTOT\nAGRWTH\nQ\nFrance\nInflation (CPI)\nTotal\nAnnual growth rate (%)\nQuarterly\n\n\n266\nquarterly\nOECD\nDP_LIVE\nOECD Data Live dataset\nFRA.CPI.TOT.AGRWTH.Q\nFrance – Inflation (CPI) – Total – Annual grow...\n2022-Q3\n2022-07-01\n5.848373\n5.848373\nFRA\nCPI\nTOT\nAGRWTH\nQ\nFrance\nInflation (CPI)\nTotal\nAnnual growth rate (%)\nQuarterly\n\n\n267\nquarterly\nOECD\nDP_LIVE\nOECD Data Live dataset\nFRA.CPI.TOT.AGRWTH.Q\nFrance – Inflation (CPI) – Total – Annual grow...\n2022-Q4\n2022-10-01\n6.066803\n6.066803\nFRA\nCPI\nTOT\nAGRWTH\nQ\nFrance\nInflation (CPI)\nTotal\nAnnual growth rate (%)\nQuarterly\n\n\n\n\n268 rows × 20 columns\n\n\n\n\n# option 2: by using groupby\n\nThe following command merges the two databases together. Explain the role of argument on. What happened to the column names?\n\ntable = table_1.merge(table_2, on=[\"period\", 'Country']) \n\nThe on argument indicate, which columns identify a unique observation. Here the date and the country denote the same observation in both countries. However the column value of the first and the second database have a different meaning (resp unemployment and inflation). To distinguish them, they receive a suffix (_x and _y respectively).\nWe rename the new names for the sake of clarity and normalize everything with lower cases.\n\ntable = table.rename(columns={\n    'period': 'date',         # because it sounds more natural\n    'Country': 'country',\n    'value_x': 'inflation',\n    'value_y': 'unemployment'\n})\n\nOn the merged table, compute at once all the statistics computed before (use groupby and agg).\nBefore we process further, we should tidy the dataframe by keeping only what we need. - Keep only the columns date, country, inflation and unemployment - Drop all na values - Make a copy of the result\n\ndf = table[['date', 'country', 'inflation', 'unemployment']].dropna()\n\n\ndf = df.copy()\n# note: the copy() function is here to avoid keeping references to the original database\n\nWhat is the maximum availability interval for each country? How would you proceed to keep only those dates where all datas are available? In the following we keep the f\nOur DataFrame is now ready for further analysis !"
  },
  {
    "objectID": "session_3/Philips_curve.html#plotting-using-matplotlib",
    "href": "session_3/Philips_curve.html#plotting-using-matplotlib",
    "title": "Visualizing the Philips Curve using matplotlib and altair",
    "section": "Plotting using matplotlib",
    "text": "Plotting using matplotlib\nOur goal now consists in plotting inflation against unemployment to see whether a pattern emerges. We will first work on France.\n\nfrom matplotlib import pyplot as plt\n\nCreate a database df_fr which contains only the data for France.\nThe following command create a line plot for inflation against unemployment. Can you transform it into a scatterplot ?\n\nplt.plot(df_fr['unemployment'], df_fr['inflation']) # missing 'o'\n\n\n\n\nExpand the above command to make the plot nicer (label, title, grid, …)\nThe following piece of code regresses inflation on unemployment.\n\nfrom statsmodels.formula import api as sm\nmodel = sm.ols(formula='inflation ~ unemployment', data=df_fr)\nresult = model.fit()\n\nWe can use the resulting model to “predict” inflation from unemployment.\n\nresult.predict(df_fr['unemployment'])\n\n0     2.061373\n1     1.942585\n2     1.980572\n3     1.758155\n4     1.638049\n        ...   \n74    2.159664\n75    2.388158\n76    2.489185\n77    2.328699\n78    2.586670\nLength: 79, dtype: float64\n\n\nStore the result in df_fr as a new column reg_unemployment\nBy expanding again, the command above to make a plot, add the regression line to the scatter plot.\nNow we would like to compare all countries. Can you find a way to represent the data for all of them (all on one graph, using subplots…) ?\nAny comment on these results?"
  },
  {
    "objectID": "session_3/Philips_curve.html#visualizing-data-using-altair",
    "href": "session_3/Philips_curve.html#visualizing-data-using-altair",
    "title": "Visualizing the Philips Curve using matplotlib and altair",
    "section": "Visualizing data using altair",
    "text": "Visualizing data using altair\nAltair offers a different syntax to make plots. It is well adapted to the exploration phase, as it is able to operate on a full databse (without splitting it like we did for matplotlib). It also provides some data transformation tools like regressions, and ways to add some interactivity.\n\nimport altair as alt\n\nThe following command makes a basic plot from the dataframe df which contains all the countries. Can you enhance it by providing a title and encoding information to distinguish the various countries (for instance colors)?\n\nchart = alt.Chart(df).mark_point().encode(\n    x='unemployment',\n    y='inflation',\n    # add something here\n)\nchart\n\n/opt/conda/envs/escpython/lib/python3.10/site-packages/altair/utils/core.py:283: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\nThe following graph plots a regression line, but for all countries, it is rather meaningless. Can you restrict the data to France only?\n\n# modify the following code\nchart = alt.Chart(df).mark_point().encode(\n    x='unemployment',\n    y='inflation',\n)\nchart + chart.transform_regression('unemployment', 'inflation').mark_line()\n\n/opt/conda/envs/escpython/lib/python3.10/site-packages/altair/utils/core.py:283: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\nOne way to visualize data consists in adding some interactivity. Add some title and click on the legend\n\n#run first then modify the following code\n\nmulti = alt.selection_multi(fields=[\"country\"])\n\nlegend = alt.Chart(df).mark_point().encode(\n    y=alt.Y('country:N', axis=alt.Axis(orient='right')),\n    color=alt.condition(multi, 'country:N', alt.value('lightgray'), legend=None)\n).add_selection(multi)\n\nchart_2 = alt.Chart(df).mark_point().encode(\n    x='unemployment',\n    y='inflation',\n    color=alt.condition(multi, 'country:N', alt.value('lightgray')),\n    # find a way to separate on the graph data from France and US\n)\n\nchart_2 | legend\n\n\n\n\n\n\nBonus question: in the following graph you can select an interval in the left panel to select some subsample. Can you add the regression line(s) corresponding to the selected data to the last graph?\n\nbrush = alt.selection_interval(encodings=['x'],)\n\nhistorical_chart_1 = alt.Chart(df).mark_line().encode(\n    x='date',\n    y='unemployment',\n    color='country'\n).add_selection(\n    brush\n)\nhistorical_chart_2 = alt.Chart(df).mark_line().encode(\n    x='date',\n    y='inflation',\n    color='country'\n)\nchart = alt.Chart(df).mark_point().encode(\n    x='unemployment',\n    y='inflation',\n    # find a way to separate on the graph data from France and US\n    color=alt.condition(brush, 'country:N', alt.value('lightgray'))\n)\nalt.hconcat(historical_chart_1, historical_chart_2, chart,)\n\n\n\n\n\n\n\n# TODO"
  },
  {
    "objectID": "session_3/Philips_curve_correction.html",
    "href": "session_3/Philips_curve_correction.html",
    "title": "Visualizing the Philips Curve using matplotlib and altair",
    "section": "",
    "text": "Our goal here is to empirically represent the Philips curve. We import the data first using dbnomics, then use two different plotting libraries to construct the plots."
  },
  {
    "objectID": "session_3/Philips_curve_correction.html#importing-the-data",
    "href": "session_3/Philips_curve_correction.html#importing-the-data",
    "title": "Visualizing the Philips Curve using matplotlib and altair",
    "section": "Importing the Data",
    "text": "Importing the Data\nWe start by loading library dbnomics. It is installed on the Nuvolos servers (just change the kernel to “ESCPython”)\n\nimport dbnomics\n\nThe following code imports data for from dbnomics for a few countries.\n\ntable_1 = dbnomics.fetch_series([\n    \"OECD/DP_LIVE/FRA.CPI.TOT.AGRWTH.Q\",\n    \"OECD/DP_LIVE/GBR.CPI.TOT.AGRWTH.Q\",\n    \"OECD/DP_LIVE/USA.CPI.TOT.AGRWTH.Q\",\n    \"OECD/DP_LIVE/DEU.CPI.TOT.AGRWTH.Q\"\n])\n\n\ntable_2 = dbnomics.fetch_series([\n    \"OECD/MEI/DEU.LRUNTTTT.STSA.Q\",\n    \"OECD/MEI/FRA.LRUNTTTT.STSA.Q\",\n    \"OECD/MEI/USA.LRUNTTTT.STSA.Q\",\n    \"OECD/MEI/GBR.LRUNTTTT.STSA.Q\"\n])\n\nDescribe concisely the data that has been imported (periodicity, type of measure, …). You can either check dbnomics website or look at the databases.\n\ntable_1:\n\ngrowth of CPI index. The total of all goods. Quarterly data for France, UK, USA, Germany\n\ntable_2\n\nunemployment rate coming from LO\n\n\nShow the first rows of each database. Make a list of all columns.\nCompute averages and standard deviations for unemployment and inflation, per country.\n\n# option 1: by using pandas boolean selection \n\n\n# option 2: by using groupby\n\nThe following command merges the two databases together. Explain the role of argument on. What happened to the column names?\n\ntable = table_1.merge(table_2, on=[\"period\", 'Country']) \n\nThe on argument indicate, which columns identify a unique observation. Here the date and the country denote the same observation in both countries. However the column value of the first and the second database have a different meaning (resp unemployment and inflation). To distinguish them, they receive a suffix (_x and _y respectively).\nWe rename the new names for the sake of clarity and normalize everything with lower cases.\n\ntable = table.rename(columns={\n    'period': 'date',         # because it sounds more natural\n    'Country': 'country',\n    'value_x': 'inflation',\n    'value_y': 'unemployment'\n})\n\nOn the merged table, compute at once all the statistics computed before (use groupby and agg).\n\ntable[['country','inflation', 'unemployment']].groupby('country').agg( [ 'mean', 'std'])\n\n\n\n\n\n\n\n\ninflation\nunemployment\n\n\n\nmean\nstd\nmean\nstd\n\n\ncountry\n\n\n\n\n\n\n\n\nFrance\n1.471232\n1.097924\n8.753655\n0.951248\n\n\nGermany\n2.675258\n1.857526\n5.019525\n3.083088\n\n\nUnited Kingdom\n5.352624\n5.287619\n6.757433\n2.381173\n\n\nUnited States\n3.670127\n2.791956\n5.906965\n1.658766\n\n\n\n\n\n\n\nBefore we process further, we should tidy the dataframe by keeping only what we need. - Keep only the columns date, country, inflation and unemployment - Drop all na values - Make a copy of the result\n\ndf = table[['date', 'country', 'inflation', 'unemployment']].dropna()\n\n\ndf = df.copy()\n# note: the copy() function is here to avoid keeping references to the original database\n\nWhat is the maximum availability interval for each country? How would you proceed to keep only those dates where all datas are available? In the following we keep the f\nOur DataFrame is now ready for further analysis !"
  },
  {
    "objectID": "session_3/Philips_curve_correction.html#plotting-using-matplotlib",
    "href": "session_3/Philips_curve_correction.html#plotting-using-matplotlib",
    "title": "Visualizing the Philips Curve using matplotlib and altair",
    "section": "Plotting using matplotlib",
    "text": "Plotting using matplotlib\nOur goal now consists in plotting inflation against unemployment to see whether a pattern emerges. We will first work on France.\n\nfrom matplotlib import pyplot as plt\n\nCreate a database df_fr which contains only the data for France.\n\ndf_fr = df[ df['country'] == 'France' ].copy() # again, we copy the result, because we plan to modify it\n\nThe following command create a line plot for inflation against unemployment. Can you transform it into a scatterplot ?\n\nplt.plot(df_fr['unemployment'], df_fr['inflation']) # missing 'o'\n\n\n\n\nExpand the above command to make the plot nicer (label, title, grid, …)\nThe following piece of code regresses inflation on unemployment.\n\nfrom statsmodels.formula import api as sm\nmodel = sm.ols(formula='inflation ~ unemployment', data=df_fr)\nresult = model.fit()\n\nWe can use the resulting model to “predict” inflation from unemployment.\n\nresult.predict(df_fr['unemployment'])\n\n0     2.061373\n1     1.942585\n2     1.980572\n3     1.758155\n4     1.638049\n        ...   \n74    2.159664\n75    2.388158\n76    2.489185\n77    2.328699\n78    2.586670\nLength: 79, dtype: float64\n\n\nStore the result in df_fr as reg_unemployment\n\ndf_fr['pred_inflation'] = result.predict(df_fr['unemployment'])\n\nBy expanding again, the command above to make a plot, add the regression line to the scatter plot.\n\nplt.plot(df_fr['unemployment'], df_fr['inflation'],'o', alpha=0.5)\nplt.plot(df_fr['unemployment'], df_fr['pred_inflation'], color='C0')\nplt.title(\"Philips Curve\")\nplt.xlabel(\"Unemployment (%)\")\nplt.ylabel(\"Inflation\")\nplt.grid()\n\n\n\n\nNow we would like to compare all countries. Can you find a way to represent the data for all of them (all on one graph, using subplots…) ?\n\n# this solution uses loops and iterators but the same can be done manually\n\n\ncountries = df['country'].unique()\n\n\ndf_countries = []\nfor c in countries:\n    \n    tmp = df[df['country']==c].copy()\n    \n    model = sm.ols(formula='inflation ~ unemployment', data=tmp)\n    result = model.fit()\n    \n    tmp['pred_inflation'] = result.predict(tmp['unemployment'])\n\n    df_countries.append(tmp)\n\n\n# all on one graph\nfor i, (d, c) in enumerate(zip(df_countries, countries)):\n    plt.plot(d['unemployment'], d['inflation'], 'o', alpha=0.2, label=c, color=f\"C{i}\") # missing 'o'\n    plt.plot(d['unemployment'], d['pred_inflation'],   color=f\"C{i}\") # missing 'o'\nplt.legend(loc='upper right')\n\n&lt;matplotlib.legend.Legend at 0x7f15febfbee0&gt;\n\n\n\n\n\n\n# using subplots\n\n\n# all on one graph\nfig = plt.subplots(2,2)\nfor i, (d, c) in enumerate(zip(df_countries, countries)):\n    plt.subplot(2,2,i+1)\n    plt.plot(d['unemployment'], d['inflation'], 'o', alpha=0.2, color=f\"C{i}\") # missing 'o'\n    plt.plot(d['unemployment'], d['pred_inflation'],   color=f\"C{i}\") # missing 'o'\n    plt.xlabel(\"Unemployment\")\n    plt.ylabel(\"Inflation\")\n    plt.title(c)\n    plt.tight_layout()\n\n\n\n\nAny comment on these results?\n\nFirst it would be nice to break the period into many subperiods. We know the story about the disappearnce of the Philips curve in the US (and the Lucas Critique).\nSecond, there seems to be a strong contrast between US/UK and Germany/France. If one remembers the mechanisms behind the Philips curve it is not surprising: prices and wages are more rigid in Europe.\n\nIn the case of Germany, the conclusion is certainly not too strong: there has been a lot of volatility.\nIn the case of France, the time span of the time series is much smaller which also weakens the conclusion."
  },
  {
    "objectID": "session_3/Philips_curve_correction.html#visualizing-data-using-altair",
    "href": "session_3/Philips_curve_correction.html#visualizing-data-using-altair",
    "title": "Visualizing the Philips Curve using matplotlib and altair",
    "section": "Visualizing data using altair",
    "text": "Visualizing data using altair\nAltair offers a different syntax to make plots. It is well adapted to the exploration phase, as it is able to operate on a full databse (without splitting it like we did for matplotlib). It also provides some data transformation tools like regressions, and ways to add some interactivity.\n\nimport altair as alt\n\nThe following command makes a basic plot from the dataframe df which contains all the countries. Can you enhance it by providing a title and encoding information to distinguish the various countries?\n\nchart = alt.Chart(df).mark_point().encode(\n    x='unemployment',\n    y='inflation',\n    # find a way to separate on the graph data from France and US\n)\nchart\n\n/opt/conda/envs/escpython/lib/python3.10/site-packages/altair/utils/core.py:283: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\n# solution: \nchart = alt.Chart(df, title=\"Data for all countries\").mark_point().encode(\n    x='unemployment',\n    y='inflation',\n    color=\"country\",\n)\nchart\n\n\n\n\n\n\nThe following graph plots a regression line, but for all countries, it is rather meaningless. Can you restrict the data to France only?\n\nchart = alt.Chart(df).mark_point().encode(\n    x='unemployment',\n    y='inflation',\n    # find a way to separate on the graph data from France and US\n)\nchart + chart.transform_regression('unemployment', 'inflation').mark_line()\n\n\n\n\n\n\n\n# solution (it is also possible to replace df by df_fr...)\nchart = alt.Chart(df).transform_filter('datum.country==\"France\"').mark_point().encode(\n    x='unemployment',\n    y='inflation',\n    # find a way to separate on the graph data from France and US\n)\nchart + chart.transform_regression('unemployment', 'inflation').mark_line()\n\n\n\n\n\n\nOne way to visualize data consists in adding some interactivity. Add some title and click on the legend\n\nmulti = alt.selection_multi(fields=[\"country\"])\n\nlegend = alt.Chart(df).mark_point().encode(\n    y=alt.Y('country:N', axis=alt.Axis(orient='right')),\n    color=alt.condition(multi, 'country:N', alt.value('lightgray'), legend=None)\n).add_selection(multi)\n\nchart_2 = alt.Chart(df).mark_point().encode(\n    x='unemployment',\n    y='inflation',\n    color=alt.condition(multi, 'country:N', alt.value('lightgray')),\n    # find a way to separate on the graph data from France and US\n)\n\n# chart + chart.transform_regression('unemployment', 'inflation').mark_line()\n\n\n# Try to click on the legend\nchart_2 | legend\n\n\n\n\n\n\nBonus question: in the following graph you can select an interval in the left panel to select some subsample. Can you add the regression line(s) corresponding to the selected data to the last graph?\n\nbrush = alt.selection_interval(encodings=['x'],)\n\nhistorical_chart_1 = alt.Chart(df).mark_line().encode(\n    x='date',\n    y='unemployment',\n    color='country'\n).add_selection(\n    brush\n)\nhistorical_chart_2 = alt.Chart(df).mark_line().encode(\n    x='date',\n    y='inflation',\n    color='country'\n)\nchart = alt.Chart(df).mark_point().encode(\n    x='unemployment',\n    y='inflation',\n    # find a way to separate on the graph data from France and US\n    color=alt.condition(brush, 'country:N', alt.value('lightgray'))\n)\nalt.hconcat(historical_chart_1, historical_chart_2, chart,)\n\n\n\n\n\n\n\n# TODO"
  },
  {
    "objectID": "session_3/Regressions-correction.html",
    "href": "session_3/Regressions-correction.html",
    "title": "Regressions",
    "section": "",
    "text": "Import the Duncan/carData dataset\n\nimport statsmodels.api as sm\ndataset = sm.datasets.get_rdataset(\"Duncan\", \"carData\")\ndf = dataset.data\ndf.head()\n\n\n\n\n\n\n\n\ntype\nincome\neducation\nprestige\n\n\n\n\naccountant\nprof\n62\n86\n82\n\n\npilot\nprof\n72\n76\n83\n\n\narchitect\nprof\n75\n92\n90\n\n\nauthor\nprof\n55\n90\n76\n\n\nchemist\nprof\n64\n86\n90\n\n\n\n\n\n\n\nEstimate by hand the model \\(\\text{income} = \\alpha + \\beta \\times \\text{education}\\) . Plot.\nCompute total, explained, unexplained variance. Compute R^2 statistics\nUse statsmodels (formula API) to estimate \\(\\text{income} = \\alpha + \\beta \\times \\text{education}\\). Comment regression statistics.\n\n#https://www.statsmodels.org/stable/generated/statsmodels.formula.api.ols.html\n\nfrom statsmodels.formula import api as smf\n\nmodel_1 = smf.ols(\"income ~ education\", df)\nres_1 = model_1.fit()\n\n&lt;statsmodels.regression.linear_model.RegressionResultsWrapper at 0x7ffad5b135e0&gt;\n\n\n\nres_1.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nincome\nR-squared:\n0.525\n\n\nModel:\nOLS\nAdj. R-squared:\n0.514\n\n\nMethod:\nLeast Squares\nF-statistic:\n47.51\n\n\nDate:\nTue, 02 Feb 2021\nProb (F-statistic):\n1.84e-08\n\n\nTime:\n11:29:50\nLog-Likelihood:\n-190.42\n\n\nNo. Observations:\n45\nAIC:\n384.8\n\n\nDf Residuals:\n43\nBIC:\n388.5\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n10.6035\n5.198\n2.040\n0.048\n0.120\n21.087\n\n\neducation\n0.5949\n0.086\n6.893\n0.000\n0.421\n0.769\n\n\n\n\n\n\nOmnibus:\n9.841\nDurbin-Watson:\n1.736\n\n\nProb(Omnibus):\n0.007\nJarque-Bera (JB):\n10.609\n\n\nSkew:\n0.776\nProb(JB):\n0.00497\n\n\nKurtosis:\n4.802\nCond. No.\n123.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe estimated regresssion is income = 10.6 + 0.59 education. At a 5% p-value level both the intercept and the coefficient are significant. R-squared is 0.52: the model explains half of the variance.\nUse statsmodels to estimate \\(\\text{income} = \\alpha + \\beta \\times \\text{prestige}\\). Comment regression statistics.\n\nformula = \"income ~ education\"\n\n\ndf.head()\n\n\n\n\n\n\n\n\ntype\nincome\neducation\nprestige\n\n\n\n\naccountant\nprof\n62\n86\n82\n\n\npilot\nprof\n72\n76\n83\n\n\narchitect\nprof\n75\n92\n90\n\n\nauthor\nprof\n55\n90\n76\n\n\nchemist\nprof\n64\n86\n90\n\n\n\n\n\n\n\n\nmodel_2 = smf.ols(\"income ~ prestige\", df)\nres_2 = model_2.fit()\n\n\nres_2.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nincome\nR-squared:\n0.702\n\n\nModel:\nOLS\nAdj. R-squared:\n0.695\n\n\nMethod:\nLeast Squares\nF-statistic:\n101.3\n\n\nDate:\nTue, 02 Feb 2021\nProb (F-statistic):\n7.14e-13\n\n\nTime:\n11:55:59\nLog-Likelihood:\n-179.93\n\n\nNo. Observations:\n45\nAIC:\n363.9\n\n\nDf Residuals:\n43\nBIC:\n367.5\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n10.8840\n3.678\n2.959\n0.005\n3.467\n18.301\n\n\nprestige\n0.6497\n0.065\n10.062\n0.000\n0.519\n0.780\n\n\n\n\n\n\nOmnibus:\n8.893\nDurbin-Watson:\n2.048\n\n\nProb(Omnibus):\n0.012\nJarque-Bera (JB):\n19.848\n\n\nSkew:\n0.047\nProb(JB):\n4.90e-05\n\n\nKurtosis:\n6.252\nCond. No.\n104.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe estimated regresssion is income = 10.6 + 0.59 education. At a 0.5% p-value level both the intercept and the coefficient are significant. R-squared is 0.70: the model predicts income better than the former one.\n__Use statsmodels to estimate $ = + + _2 + $. Comment regression statistics.__\n\nmodel_3 = smf.ols(\"income ~ education + prestige\", df)\nres_3 = model_3.fit()\n\n\nres_3.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nincome\nR-squared:\n0.702\n\n\nModel:\nOLS\nAdj. R-squared:\n0.688\n\n\nMethod:\nLeast Squares\nF-statistic:\n49.55\n\n\nDate:\nTue, 02 Feb 2021\nProb (F-statistic):\n8.88e-12\n\n\nTime:\n11:56:03\nLog-Likelihood:\n-179.90\n\n\nNo. Observations:\n45\nAIC:\n365.8\n\n\nDf Residuals:\n42\nBIC:\n371.2\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n10.4264\n4.164\n2.504\n0.016\n2.024\n18.829\n\n\neducation\n0.0323\n0.132\n0.244\n0.808\n-0.234\n0.299\n\n\nprestige\n0.6237\n0.125\n5.003\n0.000\n0.372\n0.875\n\n\n\n\n\n\nOmnibus:\n9.200\nDurbin-Watson:\n2.053\n\n\nProb(Omnibus):\n0.010\nJarque-Bera (JB):\n21.265\n\n\nSkew:\n0.075\nProb(JB):\n2.41e-05\n\n\nKurtosis:\n6.364\nCond. No.\n168.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nThe \\(R^2\\) is only slightly higher than last model, but adjusted \\(R^2\\) is actually lower: the model has less predictive power.\nThe coefficient for education is not significant. It should be dropped from the regresssion.\nThis might happen, because education and prestige are correlated. Let’s check it:\n\ndf.corr()\n\n\n\n\n\n\n\n\nincome\neducation\nprestige\n\n\n\n\nincome\n1.000000\n0.724512\n0.837801\n\n\neducation\n0.724512\n1.000000\n0.851916\n\n\nprestige\n0.837801\n0.851916\n1.000000\n\n\n\n\n\n\n\nEducation and prestige are correlated at 83%. It makes no sense keeping the two in the same regression.\nWHich model would you recommend? For which purpose?\nIf the goal is to predict income, the one with prestige only, has the highest prediction power. If we are interested in the effect of education, we keep only education.\nPlot the regression with prestige\n\na = res_2.params.Intercept\nb = res_2.params.prestige\n\n\nx = df['prestige']\n\n\ny = a + b*x\n\n\nfrom matplotlib import pyplot as plt\nplt.plot(x, df['income'],'o')\nplt.plot(x, y)\nplt.xlabel(\"prestige\")\nplt.xlabel(\"income\")\n\nText(0.5, 0, 'income')\n\n\n\n\n\nCheck visually normality of residuals\n\npred = a + b*x\nactual = df['income']\nresid = actual - pred  # same as res_2.resid\n\n\nplt.plot(x,resid, 'o')\n\n\n\n\n\nplt.hist(resid)\n\n(array([0.00255915, 0.        , 0.        , 0.00511829, 0.0486238 ,\n        0.02815062, 0.01535488, 0.01023659, 0.00255915, 0.00255915]),\n array([-46.40643935, -37.72299114, -29.03954294, -20.35609473,\n        -11.67264653,  -2.98919832,   5.69424989,  14.37769809,\n         23.0611463 ,  31.74459451,  40.42804271]),\n &lt;BarContainer object of 10 artists&gt;)"
  },
  {
    "objectID": "session_3/Regressions-correction.html#linear-regressions",
    "href": "session_3/Regressions-correction.html#linear-regressions",
    "title": "Regressions",
    "section": "",
    "text": "Import the Duncan/carData dataset\n\nimport statsmodels.api as sm\ndataset = sm.datasets.get_rdataset(\"Duncan\", \"carData\")\ndf = dataset.data\ndf.head()\n\n\n\n\n\n\n\n\ntype\nincome\neducation\nprestige\n\n\n\n\naccountant\nprof\n62\n86\n82\n\n\npilot\nprof\n72\n76\n83\n\n\narchitect\nprof\n75\n92\n90\n\n\nauthor\nprof\n55\n90\n76\n\n\nchemist\nprof\n64\n86\n90\n\n\n\n\n\n\n\nEstimate by hand the model \\(\\text{income} = \\alpha + \\beta \\times \\text{education}\\) . Plot.\nCompute total, explained, unexplained variance. Compute R^2 statistics\nUse statsmodels (formula API) to estimate \\(\\text{income} = \\alpha + \\beta \\times \\text{education}\\). Comment regression statistics.\n\n#https://www.statsmodels.org/stable/generated/statsmodels.formula.api.ols.html\n\nfrom statsmodels.formula import api as smf\n\nmodel_1 = smf.ols(\"income ~ education\", df)\nres_1 = model_1.fit()\n\n&lt;statsmodels.regression.linear_model.RegressionResultsWrapper at 0x7ffad5b135e0&gt;\n\n\n\nres_1.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nincome\nR-squared:\n0.525\n\n\nModel:\nOLS\nAdj. R-squared:\n0.514\n\n\nMethod:\nLeast Squares\nF-statistic:\n47.51\n\n\nDate:\nTue, 02 Feb 2021\nProb (F-statistic):\n1.84e-08\n\n\nTime:\n11:29:50\nLog-Likelihood:\n-190.42\n\n\nNo. Observations:\n45\nAIC:\n384.8\n\n\nDf Residuals:\n43\nBIC:\n388.5\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n10.6035\n5.198\n2.040\n0.048\n0.120\n21.087\n\n\neducation\n0.5949\n0.086\n6.893\n0.000\n0.421\n0.769\n\n\n\n\n\n\nOmnibus:\n9.841\nDurbin-Watson:\n1.736\n\n\nProb(Omnibus):\n0.007\nJarque-Bera (JB):\n10.609\n\n\nSkew:\n0.776\nProb(JB):\n0.00497\n\n\nKurtosis:\n4.802\nCond. No.\n123.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe estimated regresssion is income = 10.6 + 0.59 education. At a 5% p-value level both the intercept and the coefficient are significant. R-squared is 0.52: the model explains half of the variance.\nUse statsmodels to estimate \\(\\text{income} = \\alpha + \\beta \\times \\text{prestige}\\). Comment regression statistics.\n\nformula = \"income ~ education\"\n\n\ndf.head()\n\n\n\n\n\n\n\n\ntype\nincome\neducation\nprestige\n\n\n\n\naccountant\nprof\n62\n86\n82\n\n\npilot\nprof\n72\n76\n83\n\n\narchitect\nprof\n75\n92\n90\n\n\nauthor\nprof\n55\n90\n76\n\n\nchemist\nprof\n64\n86\n90\n\n\n\n\n\n\n\n\nmodel_2 = smf.ols(\"income ~ prestige\", df)\nres_2 = model_2.fit()\n\n\nres_2.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nincome\nR-squared:\n0.702\n\n\nModel:\nOLS\nAdj. R-squared:\n0.695\n\n\nMethod:\nLeast Squares\nF-statistic:\n101.3\n\n\nDate:\nTue, 02 Feb 2021\nProb (F-statistic):\n7.14e-13\n\n\nTime:\n11:55:59\nLog-Likelihood:\n-179.93\n\n\nNo. Observations:\n45\nAIC:\n363.9\n\n\nDf Residuals:\n43\nBIC:\n367.5\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n10.8840\n3.678\n2.959\n0.005\n3.467\n18.301\n\n\nprestige\n0.6497\n0.065\n10.062\n0.000\n0.519\n0.780\n\n\n\n\n\n\nOmnibus:\n8.893\nDurbin-Watson:\n2.048\n\n\nProb(Omnibus):\n0.012\nJarque-Bera (JB):\n19.848\n\n\nSkew:\n0.047\nProb(JB):\n4.90e-05\n\n\nKurtosis:\n6.252\nCond. No.\n104.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe estimated regresssion is income = 10.6 + 0.59 education. At a 0.5% p-value level both the intercept and the coefficient are significant. R-squared is 0.70: the model predicts income better than the former one.\n__Use statsmodels to estimate $ = + + _2 + $. Comment regression statistics.__\n\nmodel_3 = smf.ols(\"income ~ education + prestige\", df)\nres_3 = model_3.fit()\n\n\nres_3.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nincome\nR-squared:\n0.702\n\n\nModel:\nOLS\nAdj. R-squared:\n0.688\n\n\nMethod:\nLeast Squares\nF-statistic:\n49.55\n\n\nDate:\nTue, 02 Feb 2021\nProb (F-statistic):\n8.88e-12\n\n\nTime:\n11:56:03\nLog-Likelihood:\n-179.90\n\n\nNo. Observations:\n45\nAIC:\n365.8\n\n\nDf Residuals:\n42\nBIC:\n371.2\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n10.4264\n4.164\n2.504\n0.016\n2.024\n18.829\n\n\neducation\n0.0323\n0.132\n0.244\n0.808\n-0.234\n0.299\n\n\nprestige\n0.6237\n0.125\n5.003\n0.000\n0.372\n0.875\n\n\n\n\n\n\nOmnibus:\n9.200\nDurbin-Watson:\n2.053\n\n\nProb(Omnibus):\n0.010\nJarque-Bera (JB):\n21.265\n\n\nSkew:\n0.075\nProb(JB):\n2.41e-05\n\n\nKurtosis:\n6.364\nCond. No.\n168.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nThe \\(R^2\\) is only slightly higher than last model, but adjusted \\(R^2\\) is actually lower: the model has less predictive power.\nThe coefficient for education is not significant. It should be dropped from the regresssion.\nThis might happen, because education and prestige are correlated. Let’s check it:\n\ndf.corr()\n\n\n\n\n\n\n\n\nincome\neducation\nprestige\n\n\n\n\nincome\n1.000000\n0.724512\n0.837801\n\n\neducation\n0.724512\n1.000000\n0.851916\n\n\nprestige\n0.837801\n0.851916\n1.000000\n\n\n\n\n\n\n\nEducation and prestige are correlated at 83%. It makes no sense keeping the two in the same regression.\nWHich model would you recommend? For which purpose?\nIf the goal is to predict income, the one with prestige only, has the highest prediction power. If we are interested in the effect of education, we keep only education.\nPlot the regression with prestige\n\na = res_2.params.Intercept\nb = res_2.params.prestige\n\n\nx = df['prestige']\n\n\ny = a + b*x\n\n\nfrom matplotlib import pyplot as plt\nplt.plot(x, df['income'],'o')\nplt.plot(x, y)\nplt.xlabel(\"prestige\")\nplt.xlabel(\"income\")\n\nText(0.5, 0, 'income')\n\n\n\n\n\nCheck visually normality of residuals\n\npred = a + b*x\nactual = df['income']\nresid = actual - pred  # same as res_2.resid\n\n\nplt.plot(x,resid, 'o')\n\n\n\n\n\nplt.hist(resid)\n\n(array([0.00255915, 0.        , 0.        , 0.00511829, 0.0486238 ,\n        0.02815062, 0.01535488, 0.01023659, 0.00255915, 0.00255915]),\n array([-46.40643935, -37.72299114, -29.03954294, -20.35609473,\n        -11.67264653,  -2.98919832,   5.69424989,  14.37769809,\n         23.0611463 ,  31.74459451,  40.42804271]),\n &lt;BarContainer object of 10 artists&gt;)"
  },
  {
    "objectID": "session_3/Regressions-correction.html#finding-the-right-model",
    "href": "session_3/Regressions-correction.html#finding-the-right-model",
    "title": "Regressions",
    "section": "Finding the right model",
    "text": "Finding the right model\nImport dataset from data.dta. Explore dataset (statistics, plots)\n\nimport pandas\n\n\ndf = pandas.read_stata('data.dta')\ndf.head()\n\n\n\n\n\n\n\n\nindex\nx\ny\nz\n\n\n\n\n0\n0\n1.504053\n0.543556\n1.917895\n\n\n1\n1\n43.619758\n0.543113\n4.058487\n\n\n2\n2\n1.226398\n0.736955\n1.785403\n\n\n3\n3\n89.103260\n0.996219\n6.321152\n\n\n4\n4\n32.117073\n0.140142\n3.445228\n\n\n\n\n\n\n\nOur goal is to explain z by x and y. Run a regression.\n\nfrom statsmodels.formula import api as smf\n\n\nmodel = smf.ols('z ~ x + y', data=df)\nres = model.fit()\n\n\nres.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nz\nR-squared:\n0.800\n\n\nModel:\nOLS\nAdj. R-squared:\n0.791\n\n\nMethod:\nLeast Squares\nF-statistic:\n93.90\n\n\nDate:\nTue, 23 Feb 2021\nProb (F-statistic):\n3.82e-17\n\n\nTime:\n10:41:10\nLog-Likelihood:\n-57.244\n\n\nNo. Observations:\n50\nAIC:\n120.5\n\n\nDf Residuals:\n47\nBIC:\n126.2\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n1.2177\n0.243\n5.019\n0.000\n0.730\n1.706\n\n\nx\n0.0356\n0.003\n12.235\n0.000\n0.030\n0.041\n\n\ny\n1.9128\n0.369\n5.177\n0.000\n1.169\n2.656\n\n\n\n\n\n\nOmnibus:\n3.205\nDurbin-Watson:\n1.859\n\n\nProb(Omnibus):\n0.201\nJarque-Bera (JB):\n2.349\n\n\nSkew:\n0.277\nProb(JB):\n0.309\n\n\nKurtosis:\n3.906\nCond. No.\n187.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nComments: regression looks significant. \\(R^2\\) looks good. Fisher statistics, is conclusive (the hypothesis H0 that all coefficients are zero is rejected at a 3.82e-17 confidence level.. The student statistics are also quite high. For each coefficient, the hypothesis H0 that coefficient is zero is rejected at a 0.001 confidence level.\nExamine the residuals of the regression. What’s wrong? Remedy?\n\nfrom matplotlib import pyplot as plt\n\n\nplt.plot(res.resid, 'o')\n\n\n\n\n\nplt.subplot(131)\nplt.plot(df['x'], df['y'],'o')\nplt.subplot(132)\nplt.plot(df['y'], df['z'],'o')\nplt.subplot(133)\nplt.plot(df['x'], df['z'],'o')\nplt.xlabel(\"x\")\nplt.ylabel(\"z\")\n\nplt.tight_layout()\n\n\n\n\n\nimport numpy as np\nplt.plot( np.log(df['z']), np.log(df['x']), 'o' )\n\n\n\n\nApparently, there is a linear relationship between ‘log(x)’ and log(y)\n\nfrom numpy import log\n\n\nmodel = smf.ols('log(z) ~ log(x) + y', data=df)\nres = model.fit()\n\n\nres.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nlog(z)\nR-squared:\n0.957\n\n\nModel:\nOLS\nAdj. R-squared:\n0.955\n\n\nMethod:\nLeast Squares\nF-statistic:\n525.6\n\n\nDate:\nTue, 23 Feb 2021\nProb (F-statistic):\n6.89e-33\n\n\nTime:\n10:41:56\nLog-Likelihood:\n44.223\n\n\nNo. Observations:\n50\nAIC:\n-82.45\n\n\nDf Residuals:\n47\nBIC:\n-76.71\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n0.0128\n0.039\n0.326\n0.746\n-0.066\n0.092\n\n\nlog(x)\n0.2957\n0.010\n29.878\n0.000\n0.276\n0.316\n\n\ny\n0.6164\n0.048\n12.735\n0.000\n0.519\n0.714\n\n\n\n\n\n\nOmnibus:\n4.351\nDurbin-Watson:\n2.490\n\n\nProb(Omnibus):\n0.114\nJarque-Bera (JB):\n1.936\n\n\nSkew:\n0.089\nProb(JB):\n0.380\n\n\nKurtosis:\n2.052\nCond. No.\n12.1\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nplt.plot(res.resid, 'o')"
  },
  {
    "objectID": "session_3/Regressions-correction.html#taylor-rule",
    "href": "session_3/Regressions-correction.html#taylor-rule",
    "title": "Regressions",
    "section": "Taylor Rule",
    "text": "Taylor Rule\nIn 1993, John taylor, estimated, using US data the regression: \\(i_t = i^{\\star} + \\alpha_{\\pi} \\pi_t + \\alpha_{\\pi} y_t\\) where \\(\\pi_t\\) is inflation and \\(y_t\\) the output gap (let’s say deviation from real gdp from the trend). He found that both coefficients were not significantly different from \\(0.5\\). Our goal, is to replicate the same analysis.\nImport macro data from statsmodels (https://www.statsmodels.org/devel/datasets/generated/macrodata.html)\n\nimport statsmodels\n\n## google: stats models macrodata\n## google: statsmodels datasets  -&gt; example in the tutorial\n\n# https://www.statsmodels.org/0.6.1/datasets/index.html\n# example about how to use lengley database\n\n\nimport statsmodels.api as sm\n\n\nsm.datasets.macrodata\n\n&lt;module 'statsmodels.datasets.macrodata' from '/home/pablo/.local/opt/miniconda3/lib/python3.8/site-packages/statsmodels/datasets/macrodata/__init__.py'&gt;\n\n\n\nds = sm.datasets.macrodata.load_pandas()\n\n\ndf = ds.raw_data\ndf.head()\n\n\n\n\n\n\n\n\nyear\nquarter\nrealgdp\nrealcons\nrealinv\nrealgovt\nrealdpi\ncpi\nm1\ntbilrate\nunemp\npop\ninfl\nrealint\n\n\n\n\n0\n1959.0\n1.0\n2710.349\n1707.4\n286.898\n470.045\n1886.9\n28.98\n139.7\n2.82\n5.8\n177.146\n0.00\n0.00\n\n\n1\n1959.0\n2.0\n2778.801\n1733.7\n310.859\n481.301\n1919.7\n29.15\n141.7\n3.08\n5.1\n177.830\n2.34\n0.74\n\n\n2\n1959.0\n3.0\n2775.488\n1751.8\n289.226\n491.260\n1916.4\n29.35\n140.5\n3.82\n5.3\n178.657\n2.74\n1.09\n\n\n3\n1959.0\n4.0\n2785.204\n1753.7\n299.356\n484.052\n1931.3\n29.37\n140.0\n4.33\n5.6\n179.386\n0.27\n4.06\n\n\n4\n1960.0\n1.0\n2847.699\n1770.5\n331.722\n462.199\n1955.5\n29.54\n139.6\n3.50\n5.2\n180.007\n2.31\n1.19\n\n\n\n\n\n\n\nCreate a database with all variables of interest including detrended gdp\n\ngdp = df['realgdp']\ninflation = df['infl']\nrealint = df['realint']\n\n\nddf = df # \n\n\nddf.head()\n\n\n\n\n\n\n\n\nyear\nquarter\nrealgdp\nrealcons\nrealinv\nrealgovt\nrealdpi\ncpi\nm1\ntbilrate\nunemp\npop\ninfl\nrealint\n\n\n\n\n0\n1959.0\n1.0\n2710.349\n1707.4\n286.898\n470.045\n1886.9\n28.98\n139.7\n2.82\n5.8\n177.146\n0.00\n0.00\n\n\n1\n1959.0\n2.0\n2778.801\n1733.7\n310.859\n481.301\n1919.7\n29.15\n141.7\n3.08\n5.1\n177.830\n2.34\n0.74\n\n\n2\n1959.0\n3.0\n2775.488\n1751.8\n289.226\n491.260\n1916.4\n29.35\n140.5\n3.82\n5.3\n178.657\n2.74\n1.09\n\n\n3\n1959.0\n4.0\n2785.204\n1753.7\n299.356\n484.052\n1931.3\n29.37\n140.0\n4.33\n5.6\n179.386\n0.27\n4.06\n\n\n4\n1960.0\n1.0\n2847.699\n1770.5\n331.722\n462.199\n1955.5\n29.54\n139.6\n3.50\n5.2\n180.007\n2.31\n1.19\n\n\n\n\n\n\n\nWe use the fisher relation: \\(r_t = i_t - \\pi_t\\)\n\nddf['ir'] = ddf['realint'] + ddf['infl']\n\nto detrend the gdp, we use hp-filter function from scipy google: hpfilter scipy\n\nfrom statsmodels.tsa.filters.hp_filter import hpfilter\n\n\ncycle, trend = hpfilter(ddf['realgdp'])\n\n\nddf['gdp'] = cycle/trend*100 # nominal interest rate and inflation are in percent\n\n\nddf.head()\n\n\n\n\n\n\n\n\nyear\nquarter\nrealgdp\nrealcons\nrealinv\nrealgovt\nrealdpi\ncpi\nm1\ntbilrate\nunemp\npop\ninfl\nrealint\nir\ngdp\n\n\n\n\n0\n1959.0\n1.0\n2710.349\n1707.4\n286.898\n470.045\n1886.9\n28.98\n139.7\n2.82\n5.8\n177.146\n0.00\n0.00\n0.00\n1.479383\n\n\n1\n1959.0\n2.0\n2778.801\n1733.7\n310.859\n481.301\n1919.7\n29.15\n141.7\n3.08\n5.1\n177.830\n2.34\n0.74\n3.08\n2.967657\n\n\n2\n1959.0\n3.0\n2775.488\n1751.8\n289.226\n491.260\n1916.4\n29.35\n140.5\n3.82\n5.3\n178.657\n2.74\n1.09\n3.83\n1.792534\n\n\n3\n1959.0\n4.0\n2785.204\n1753.7\n299.356\n484.052\n1931.3\n29.37\n140.0\n4.33\n5.6\n179.386\n0.27\n4.06\n4.33\n1.110571\n\n\n4\n1960.0\n1.0\n2847.699\n1770.5\n331.722\n462.199\n1955.5\n29.54\n139.6\n3.50\n5.2\n180.007\n2.31\n1.19\n3.50\n2.331547\n\n\n\n\n\n\n\nRun the basic regression\n\nfrom statsmodels.formula import api as sm\n\n\nmodel = sm.ols(\"ir ~ infl + gdp\", data=ddf) # no intercept\nresults = model.fit()\nresults.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nir\nR-squared:\n0.389\n\n\nModel:\nOLS\nAdj. R-squared:\n0.383\n\n\nMethod:\nLeast Squares\nF-statistic:\n63.65\n\n\nDate:\nTue, 02 Mar 2021\nProb (F-statistic):\n4.06e-22\n\n\nTime:\n11:54:15\nLog-Likelihood:\n-448.17\n\n\nNo. Observations:\n203\nAIC:\n902.3\n\n\nDf Residuals:\n200\nBIC:\n912.3\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n3.2035\n0.252\n12.696\n0.000\n2.706\n3.701\n\n\ninfl\n0.5288\n0.050\n10.557\n0.000\n0.430\n0.628\n\n\ngdp\n0.0795\n0.105\n0.759\n0.449\n-0.127\n0.286\n\n\n\n\n\n\nOmnibus:\n30.222\nDurbin-Watson:\n0.417\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n50.662\n\n\nSkew:\n0.796\nProb(JB):\n9.98e-12\n\n\nKurtosis:\n4.858\nCond. No.\n8.56\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nWhich control variables would you propose to add? Does it increase prediction power? How do you interpret that?\n\nddf.head()\n\n\n\n\n\n\n\n\nyear\nquarter\nrealgdp\nrealcons\nrealinv\nrealgovt\nrealdpi\ncpi\nm1\ntbilrate\nunemp\npop\ninfl\nrealint\nir\ngdp\n\n\n\n\n0\n1959.0\n1.0\n2710.349\n1707.4\n286.898\n470.045\n1886.9\n28.98\n139.7\n2.82\n5.8\n177.146\n0.00\n0.00\n0.00\n1.479383\n\n\n1\n1959.0\n2.0\n2778.801\n1733.7\n310.859\n481.301\n1919.7\n29.15\n141.7\n3.08\n5.1\n177.830\n2.34\n0.74\n3.08\n2.967657\n\n\n2\n1959.0\n3.0\n2775.488\n1751.8\n289.226\n491.260\n1916.4\n29.35\n140.5\n3.82\n5.3\n178.657\n2.74\n1.09\n3.83\n1.792534\n\n\n3\n1959.0\n4.0\n2785.204\n1753.7\n299.356\n484.052\n1931.3\n29.37\n140.0\n4.33\n5.6\n179.386\n0.27\n4.06\n4.33\n1.110571\n\n\n4\n1960.0\n1.0\n2847.699\n1770.5\n331.722\n462.199\n1955.5\n29.54\n139.6\n3.50\n5.2\n180.007\n2.31\n1.19\n3.50\n2.331547\n\n\n\n\n\n\n\n\nmodel = sm.ols(\"ir ~ infl + gdp + pop + unemp -1\", data=ddf) # no intercept\nresults = model.fit()\nresults.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nir\nR-squared (uncentered):\n0.884\n\n\nModel:\nOLS\nAdj. R-squared (uncentered):\n0.882\n\n\nMethod:\nLeast Squares\nF-statistic:\n380.2\n\n\nDate:\nTue, 02 Mar 2021\nProb (F-statistic):\n5.64e-92\n\n\nTime:\n11:58:05\nLog-Likelihood:\n-432.84\n\n\nNo. Observations:\n203\nAIC:\n873.7\n\n\nDf Residuals:\n199\nBIC:\n886.9\n\n\nDf Model:\n4\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\ninfl\n0.4380\n0.049\n8.895\n0.000\n0.341\n0.535\n\n\ngdp\n0.5710\n0.120\n4.739\n0.000\n0.333\n0.809\n\n\npop\n-0.0050\n0.002\n-2.068\n0.040\n-0.010\n-0.000\n\n\nunemp\n0.8064\n0.108\n7.458\n0.000\n0.593\n1.020\n\n\n\n\n\n\nOmnibus:\n5.307\nDurbin-Watson:\n0.391\n\n\nProb(Omnibus):\n0.070\nJarque-Bera (JB):\n7.501\n\n\nSkew:\n0.070\nProb(JB):\n0.0235\n\n\nKurtosis:\n3.931\nCond. No.\n247.\n\n\n\nNotes:[1] R² is computed without centering (uncentered) since the model does not contain a constant.[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nAt confidence level 2.5% gdp is between 0.333 and 0.809.\nAt confidence level 2.5% infl is between 0.341 and 0.535.\nThe coefficients would be significantly different from 0.5 if 0.5 was not in the condifence interval."
  },
  {
    "objectID": "session_3/Regressions.html",
    "href": "session_3/Regressions.html",
    "title": "Regressions",
    "section": "",
    "text": "Import the Duncan/carData dataset\n\nimport statsmodels.api as sm\ndataset = sm.datasets.get_rdataset(\"Duncan\", \"carData\")\ndf = dataset.data\ndf.head()\n\n\n\n\n\n\n\n\ntype\nincome\neducation\nprestige\n\n\n\n\naccountant\nprof\n62\n86\n82\n\n\npilot\nprof\n72\n76\n83\n\n\narchitect\nprof\n75\n92\n90\n\n\nauthor\nprof\n55\n90\n76\n\n\nchemist\nprof\n64\n86\n90\n\n\n\n\n\n\n\nEstimate by hand the model \\(\\text{income} = \\alpha + \\beta \\times \\text{education}\\) . Plot.\nCompute total, explained, unexplained variance. Compute R^2 statistics\nUse statsmodels (formula API) to estimate \\(\\text{income} = \\alpha + \\beta \\times \\text{education}\\). Comment regression statistics.\n\n#https://www.statsmodels.org/stable/generated/statsmodels.formula.api.ols.html\n\nfrom statsmodels.formula import api as smf\n\nmodel_1 = smf.ols(\"income ~ education\", df)\nres_1 = model_1.fit()\n\n&lt;statsmodels.regression.linear_model.RegressionResultsWrapper at 0x7ffad5b135e0&gt;\n\n\n\nres_1.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nincome\nR-squared:\n0.525\n\n\nModel:\nOLS\nAdj. R-squared:\n0.514\n\n\nMethod:\nLeast Squares\nF-statistic:\n47.51\n\n\nDate:\nTue, 02 Feb 2021\nProb (F-statistic):\n1.84e-08\n\n\nTime:\n11:29:50\nLog-Likelihood:\n-190.42\n\n\nNo. Observations:\n45\nAIC:\n384.8\n\n\nDf Residuals:\n43\nBIC:\n388.5\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n10.6035\n5.198\n2.040\n0.048\n0.120\n21.087\n\n\neducation\n0.5949\n0.086\n6.893\n0.000\n0.421\n0.769\n\n\n\n\n\n\nOmnibus:\n9.841\nDurbin-Watson:\n1.736\n\n\nProb(Omnibus):\n0.007\nJarque-Bera (JB):\n10.609\n\n\nSkew:\n0.776\nProb(JB):\n0.00497\n\n\nKurtosis:\n4.802\nCond. No.\n123.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe estimated regresssion is income = 10.6 + 0.59 education. At a 5% p-value level both the intercept and the coefficient are significant. R-squared is 0.52: the model explains half of the variance.\nUse statsmodels to estimate \\(\\text{income} = \\alpha + \\beta \\times \\text{prestige}\\). Comment regression statistics.\n\nformula = \"income ~ education\"\n\n\ndf.head()\n\n\n\n\n\n\n\n\ntype\nincome\neducation\nprestige\n\n\n\n\naccountant\nprof\n62\n86\n82\n\n\npilot\nprof\n72\n76\n83\n\n\narchitect\nprof\n75\n92\n90\n\n\nauthor\nprof\n55\n90\n76\n\n\nchemist\nprof\n64\n86\n90\n\n\n\n\n\n\n\n\nmodel_2 = smf.ols(\"income ~ prestige\", df)\nres_2 = model_2.fit()\n\n\nres_2.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nincome\nR-squared:\n0.702\n\n\nModel:\nOLS\nAdj. R-squared:\n0.695\n\n\nMethod:\nLeast Squares\nF-statistic:\n101.3\n\n\nDate:\nTue, 02 Feb 2021\nProb (F-statistic):\n7.14e-13\n\n\nTime:\n11:55:59\nLog-Likelihood:\n-179.93\n\n\nNo. Observations:\n45\nAIC:\n363.9\n\n\nDf Residuals:\n43\nBIC:\n367.5\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n10.8840\n3.678\n2.959\n0.005\n3.467\n18.301\n\n\nprestige\n0.6497\n0.065\n10.062\n0.000\n0.519\n0.780\n\n\n\n\n\n\nOmnibus:\n8.893\nDurbin-Watson:\n2.048\n\n\nProb(Omnibus):\n0.012\nJarque-Bera (JB):\n19.848\n\n\nSkew:\n0.047\nProb(JB):\n4.90e-05\n\n\nKurtosis:\n6.252\nCond. No.\n104.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe estimated regresssion is income = 10.6 + 0.59 education. At a 0.5% p-value level both the intercept and the coefficient are significant. R-squared is 0.70: the model predicts income better than the former one.\n__Use statsmodels to estimate $ = + + _2 + $. Comment regression statistics.__\n\nmodel_3 = smf.ols(\"income ~ education + prestige\", df)\nres_3 = model_3.fit()\n\n\nres_3.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nincome\nR-squared:\n0.702\n\n\nModel:\nOLS\nAdj. R-squared:\n0.688\n\n\nMethod:\nLeast Squares\nF-statistic:\n49.55\n\n\nDate:\nTue, 02 Feb 2021\nProb (F-statistic):\n8.88e-12\n\n\nTime:\n11:56:03\nLog-Likelihood:\n-179.90\n\n\nNo. Observations:\n45\nAIC:\n365.8\n\n\nDf Residuals:\n42\nBIC:\n371.2\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n10.4264\n4.164\n2.504\n0.016\n2.024\n18.829\n\n\neducation\n0.0323\n0.132\n0.244\n0.808\n-0.234\n0.299\n\n\nprestige\n0.6237\n0.125\n5.003\n0.000\n0.372\n0.875\n\n\n\n\n\n\nOmnibus:\n9.200\nDurbin-Watson:\n2.053\n\n\nProb(Omnibus):\n0.010\nJarque-Bera (JB):\n21.265\n\n\nSkew:\n0.075\nProb(JB):\n2.41e-05\n\n\nKurtosis:\n6.364\nCond. No.\n168.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nThe \\(R^2\\) is only slightly higher than last model, but adjusted \\(R^2\\) is actually lower: the model has less predictive power.\nThe coefficient for education is not significant. It should be dropped from the regresssion.\nThis might happen, because education and prestige are correlated. Let’s check it:\n\ndf.corr()\n\n\n\n\n\n\n\n\nincome\neducation\nprestige\n\n\n\n\nincome\n1.000000\n0.724512\n0.837801\n\n\neducation\n0.724512\n1.000000\n0.851916\n\n\nprestige\n0.837801\n0.851916\n1.000000\n\n\n\n\n\n\n\nEducation and prestige are correlated at 83%. It makes no sense keeping the two in the same regression.\nWHich model would you recommend? For which purpose?\nIf the goal is to predict income, the one with prestige only, has the highest prediction power. If we are interested in the effect of education, we keep only education.\nPlot the regression with prestige\n\na = res_2.params.Intercept\nb = res_2.params.prestige\n\n\nx = df['prestige']\n\n\ny = a + b*x\n\n\nfrom matplotlib import pyplot as plt\nplt.plot(x, df['income'],'o')\nplt.plot(x, y)\nplt.xlabel(\"prestige\")\nplt.xlabel(\"income\")\n\nText(0.5, 0, 'income')\n\n\n\n\n\nCheck visually normality of residuals\n\npred = a + b*x\nactual = df['income']\nresid = actual - pred  # same as res_2.resid\n\n\nplt.plot(x,resid, 'o')\n\n\n\n\n\nplt.hist(resid)\n\n(array([0.00255915, 0.        , 0.        , 0.00511829, 0.0486238 ,\n        0.02815062, 0.01535488, 0.01023659, 0.00255915, 0.00255915]),\n array([-46.40643935, -37.72299114, -29.03954294, -20.35609473,\n        -11.67264653,  -2.98919832,   5.69424989,  14.37769809,\n         23.0611463 ,  31.74459451,  40.42804271]),\n &lt;BarContainer object of 10 artists&gt;)"
  },
  {
    "objectID": "session_3/Regressions.html#linear-regressions",
    "href": "session_3/Regressions.html#linear-regressions",
    "title": "Regressions",
    "section": "",
    "text": "Import the Duncan/carData dataset\n\nimport statsmodels.api as sm\ndataset = sm.datasets.get_rdataset(\"Duncan\", \"carData\")\ndf = dataset.data\ndf.head()\n\n\n\n\n\n\n\n\ntype\nincome\neducation\nprestige\n\n\n\n\naccountant\nprof\n62\n86\n82\n\n\npilot\nprof\n72\n76\n83\n\n\narchitect\nprof\n75\n92\n90\n\n\nauthor\nprof\n55\n90\n76\n\n\nchemist\nprof\n64\n86\n90\n\n\n\n\n\n\n\nEstimate by hand the model \\(\\text{income} = \\alpha + \\beta \\times \\text{education}\\) . Plot.\nCompute total, explained, unexplained variance. Compute R^2 statistics\nUse statsmodels (formula API) to estimate \\(\\text{income} = \\alpha + \\beta \\times \\text{education}\\). Comment regression statistics.\n\n#https://www.statsmodels.org/stable/generated/statsmodels.formula.api.ols.html\n\nfrom statsmodels.formula import api as smf\n\nmodel_1 = smf.ols(\"income ~ education\", df)\nres_1 = model_1.fit()\n\n&lt;statsmodels.regression.linear_model.RegressionResultsWrapper at 0x7ffad5b135e0&gt;\n\n\n\nres_1.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nincome\nR-squared:\n0.525\n\n\nModel:\nOLS\nAdj. R-squared:\n0.514\n\n\nMethod:\nLeast Squares\nF-statistic:\n47.51\n\n\nDate:\nTue, 02 Feb 2021\nProb (F-statistic):\n1.84e-08\n\n\nTime:\n11:29:50\nLog-Likelihood:\n-190.42\n\n\nNo. Observations:\n45\nAIC:\n384.8\n\n\nDf Residuals:\n43\nBIC:\n388.5\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n10.6035\n5.198\n2.040\n0.048\n0.120\n21.087\n\n\neducation\n0.5949\n0.086\n6.893\n0.000\n0.421\n0.769\n\n\n\n\n\n\nOmnibus:\n9.841\nDurbin-Watson:\n1.736\n\n\nProb(Omnibus):\n0.007\nJarque-Bera (JB):\n10.609\n\n\nSkew:\n0.776\nProb(JB):\n0.00497\n\n\nKurtosis:\n4.802\nCond. No.\n123.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe estimated regresssion is income = 10.6 + 0.59 education. At a 5% p-value level both the intercept and the coefficient are significant. R-squared is 0.52: the model explains half of the variance.\nUse statsmodels to estimate \\(\\text{income} = \\alpha + \\beta \\times \\text{prestige}\\). Comment regression statistics.\n\nformula = \"income ~ education\"\n\n\ndf.head()\n\n\n\n\n\n\n\n\ntype\nincome\neducation\nprestige\n\n\n\n\naccountant\nprof\n62\n86\n82\n\n\npilot\nprof\n72\n76\n83\n\n\narchitect\nprof\n75\n92\n90\n\n\nauthor\nprof\n55\n90\n76\n\n\nchemist\nprof\n64\n86\n90\n\n\n\n\n\n\n\n\nmodel_2 = smf.ols(\"income ~ prestige\", df)\nres_2 = model_2.fit()\n\n\nres_2.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nincome\nR-squared:\n0.702\n\n\nModel:\nOLS\nAdj. R-squared:\n0.695\n\n\nMethod:\nLeast Squares\nF-statistic:\n101.3\n\n\nDate:\nTue, 02 Feb 2021\nProb (F-statistic):\n7.14e-13\n\n\nTime:\n11:55:59\nLog-Likelihood:\n-179.93\n\n\nNo. Observations:\n45\nAIC:\n363.9\n\n\nDf Residuals:\n43\nBIC:\n367.5\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n10.8840\n3.678\n2.959\n0.005\n3.467\n18.301\n\n\nprestige\n0.6497\n0.065\n10.062\n0.000\n0.519\n0.780\n\n\n\n\n\n\nOmnibus:\n8.893\nDurbin-Watson:\n2.048\n\n\nProb(Omnibus):\n0.012\nJarque-Bera (JB):\n19.848\n\n\nSkew:\n0.047\nProb(JB):\n4.90e-05\n\n\nKurtosis:\n6.252\nCond. No.\n104.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe estimated regresssion is income = 10.6 + 0.59 education. At a 0.5% p-value level both the intercept and the coefficient are significant. R-squared is 0.70: the model predicts income better than the former one.\n__Use statsmodels to estimate $ = + + _2 + $. Comment regression statistics.__\n\nmodel_3 = smf.ols(\"income ~ education + prestige\", df)\nres_3 = model_3.fit()\n\n\nres_3.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nincome\nR-squared:\n0.702\n\n\nModel:\nOLS\nAdj. R-squared:\n0.688\n\n\nMethod:\nLeast Squares\nF-statistic:\n49.55\n\n\nDate:\nTue, 02 Feb 2021\nProb (F-statistic):\n8.88e-12\n\n\nTime:\n11:56:03\nLog-Likelihood:\n-179.90\n\n\nNo. Observations:\n45\nAIC:\n365.8\n\n\nDf Residuals:\n42\nBIC:\n371.2\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n10.4264\n4.164\n2.504\n0.016\n2.024\n18.829\n\n\neducation\n0.0323\n0.132\n0.244\n0.808\n-0.234\n0.299\n\n\nprestige\n0.6237\n0.125\n5.003\n0.000\n0.372\n0.875\n\n\n\n\n\n\nOmnibus:\n9.200\nDurbin-Watson:\n2.053\n\n\nProb(Omnibus):\n0.010\nJarque-Bera (JB):\n21.265\n\n\nSkew:\n0.075\nProb(JB):\n2.41e-05\n\n\nKurtosis:\n6.364\nCond. No.\n168.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nThe \\(R^2\\) is only slightly higher than last model, but adjusted \\(R^2\\) is actually lower: the model has less predictive power.\nThe coefficient for education is not significant. It should be dropped from the regresssion.\nThis might happen, because education and prestige are correlated. Let’s check it:\n\ndf.corr()\n\n\n\n\n\n\n\n\nincome\neducation\nprestige\n\n\n\n\nincome\n1.000000\n0.724512\n0.837801\n\n\neducation\n0.724512\n1.000000\n0.851916\n\n\nprestige\n0.837801\n0.851916\n1.000000\n\n\n\n\n\n\n\nEducation and prestige are correlated at 83%. It makes no sense keeping the two in the same regression.\nWHich model would you recommend? For which purpose?\nIf the goal is to predict income, the one with prestige only, has the highest prediction power. If we are interested in the effect of education, we keep only education.\nPlot the regression with prestige\n\na = res_2.params.Intercept\nb = res_2.params.prestige\n\n\nx = df['prestige']\n\n\ny = a + b*x\n\n\nfrom matplotlib import pyplot as plt\nplt.plot(x, df['income'],'o')\nplt.plot(x, y)\nplt.xlabel(\"prestige\")\nplt.xlabel(\"income\")\n\nText(0.5, 0, 'income')\n\n\n\n\n\nCheck visually normality of residuals\n\npred = a + b*x\nactual = df['income']\nresid = actual - pred  # same as res_2.resid\n\n\nplt.plot(x,resid, 'o')\n\n\n\n\n\nplt.hist(resid)\n\n(array([0.00255915, 0.        , 0.        , 0.00511829, 0.0486238 ,\n        0.02815062, 0.01535488, 0.01023659, 0.00255915, 0.00255915]),\n array([-46.40643935, -37.72299114, -29.03954294, -20.35609473,\n        -11.67264653,  -2.98919832,   5.69424989,  14.37769809,\n         23.0611463 ,  31.74459451,  40.42804271]),\n &lt;BarContainer object of 10 artists&gt;)"
  },
  {
    "objectID": "session_3/Regressions.html#finding-the-right-model",
    "href": "session_3/Regressions.html#finding-the-right-model",
    "title": "Regressions",
    "section": "Finding the right model",
    "text": "Finding the right model\nImport dataset from data.dta. Explore dataset (statistics, plots)\nOur goal is to explain z by x and y. Run a regression.\nExamine the residuals of the regression. What’s wrong? Remedy?"
  },
  {
    "objectID": "session_3/Regressions.html#taylor-rule",
    "href": "session_3/Regressions.html#taylor-rule",
    "title": "Regressions",
    "section": "Taylor Rule",
    "text": "Taylor Rule\nIn 1993, John taylor, estimated, using US data the regression: \\(i_t = i^{\\star} + \\alpha_{\\pi} \\pi_t + \\alpha_{\\pi} y_t\\) where \\(\\pi_t\\) is inflation and \\(y_t\\) the output gap (let’s say deviation from real gdp from the trend). He found that both coefficients were not significantly different from \\(0.5\\). Our goal, is to replicate the same analysis.\nImport macro data from statsmodels (https://www.statsmodels.org/devel/datasets/generated/macrodata.html)\nCreate a database with all variables of interest including detrended gdp\nRun the basic regression\nWhich control variables would you propose to add? Does it increase prediction power? How do you interpret that?"
  },
  {
    "objectID": "session_3/Untitled.html",
    "href": "session_3/Untitled.html",
    "title": "Data-Based Economics",
    "section": "",
    "text": "!pip install statsmodels\n\nCollecting statsmodels\n  Downloading statsmodels-0.13.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.9 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.9/9.9 MB 5.1 MB/s eta 0:00:00m eta 0:00:01[36m0:00:01\nRequirement already satisfied: pandas&gt;=0.25 in /home/pablo/.local/opt/mambaforge/lib/python3.10/site-packages (from statsmodels) (1.4.4)\nRequirement already satisfied: numpy&gt;=1.17 in /home/pablo/.local/opt/mambaforge/lib/python3.10/site-packages (from statsmodels) (1.22.4)\nRequirement already satisfied: scipy&gt;=1.3 in /home/pablo/.local/opt/mambaforge/lib/python3.10/site-packages (from statsmodels) (1.9.1)\nCollecting patsy&gt;=0.5.2\n  Downloading patsy-0.5.3-py2.py3-none-any.whl (233 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 233.8/233.8 kB 4.7 MB/s eta 0:00:00 MB/s eta 0:00:01\nRequirement already satisfied: packaging&gt;=21.3 in /home/pablo/.local/opt/mambaforge/lib/python3.10/site-packages (from statsmodels) (21.3)\nRequirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /home/pablo/.local/opt/mambaforge/lib/python3.10/site-packages (from packaging&gt;=21.3-&gt;statsmodels) (3.0.9)\nRequirement already satisfied: python-dateutil&gt;=2.8.1 in /home/pablo/.local/opt/mambaforge/lib/python3.10/site-packages (from pandas&gt;=0.25-&gt;statsmodels) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /home/pablo/.local/opt/mambaforge/lib/python3.10/site-packages (from pandas&gt;=0.25-&gt;statsmodels) (2022.2.1)\nRequirement already satisfied: six in /home/pablo/.local/opt/mambaforge/lib/python3.10/site-packages (from patsy&gt;=0.5.2-&gt;statsmodels) (1.16.0)\nInstalling collected packages: patsy, statsmodels\nSuccessfully installed patsy-0.5.3 statsmodels-0.13.5\n\n\n\nimport statsmodels.api as sm\ndataset = sm.datasets.get_rdataset(\"Duncan\", \"carData\")\ndf = dataset.data\n\n\ndf.head()\n\n\n\n\n\n\n\n\ntype\nincome\neducation\nprestige\n\n\n\n\naccountant\nprof\n62\n86\n82\n\n\npilot\nprof\n72\n76\n83\n\n\narchitect\nprof\n75\n92\n90\n\n\nauthor\nprof\n55\n90\n76\n\n\nchemist\nprof\n64\n86\n90\n\n\n\n\n\n\n\n\ndfs = df.sort_values('income', ascending=False).head()\n\n\ndf.mean()\n\n/tmp/ipykernel_20796/3698961737.py:1: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n  df.mean()\n\n\nincome       41.866667\neducation    52.555556\nprestige     47.688889\ndtype: float64\n\n\n\ndf.median()\n\n/tmp/ipykernel_20796/530051474.py:1: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n  df.median()\n\n\nincome       42.0\neducation    45.0\nprestige     41.0\ndtype: float64\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nincome\neducation\nprestige\n\n\n\n\ncount\n45.000000\n45.000000\n45.000000\n\n\nmean\n41.866667\n52.555556\n47.688889\n\n\nstd\n24.435072\n29.760831\n31.510332\n\n\nmin\n7.000000\n7.000000\n3.000000\n\n\n25%\n21.000000\n26.000000\n16.000000\n\n\n50%\n42.000000\n45.000000\n41.000000\n\n\n75%\n64.000000\n84.000000\n81.000000\n\n\nmax\n81.000000\n100.000000\n97.000000\n\n\n\n\n\n\n\n\ndfs['income'].plot()\n\n&lt;AxesSubplot: &gt;\n\n\n\n\n\n\ndf['income'].hist()\n\n&lt;AxesSubplot: &gt;\n\n\n\n\n\n\ndf['education'].hist()\n\n&lt;AxesSubplot: &gt;\n\n\n\n\n\n\ndf['prestige'].hist()\n\n&lt;AxesSubplot: &gt;\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\n\nplt.plot(df['education'], df['income'],'o')\n\n\n\n\n\nplt.plot(df['prestige'], df['income'],'o')\n\n\n\n\n\ndf.corr()\n\n\n\n\n\n\n\n\nincome\neducation\nprestige\n\n\n\n\nincome\n1.000000\n0.724512\n0.837801\n\n\neducation\n0.724512\n1.000000\n0.851916\n\n\nprestige\n0.837801\n0.851916\n1.000000\n\n\n\n\n\n\n\n\nimport statsmodels.formula.api as sm\n\n\nmodel = sm.ols(formula=\"income ~ education\", data=df)\n\n\nresult = model.fit()\n\n\nresult.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nincome\nR-squared:\n0.525\n\n\nModel:\nOLS\nAdj. R-squared:\n0.514\n\n\nMethod:\nLeast Squares\nF-statistic:\n47.51\n\n\nDate:\nWed, 01 Feb 2023\nProb (F-statistic):\n1.84e-08\n\n\nTime:\n09:38:54\nLog-Likelihood:\n-190.42\n\n\nNo. Observations:\n45\nAIC:\n384.8\n\n\nDf Residuals:\n43\nBIC:\n388.5\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n10.6035\n5.198\n2.040\n0.048\n0.120\n21.087\n\n\neducation\n0.5949\n0.086\n6.893\n0.000\n0.421\n0.769\n\n\n\n\n\n\nOmnibus:\n9.841\nDurbin-Watson:\n1.736\n\n\nProb(Omnibus):\n0.007\nJarque-Bera (JB):\n10.609\n\n\nSkew:\n0.776\nProb(JB):\n0.00497\n\n\nKurtosis:\n4.802\nCond. No.\n123.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\"income ~ education\"   ~~   income = α + β * education"
  },
  {
    "objectID": "session_4/graphs/index.html",
    "href": "session_4/graphs/index.html",
    "title": "Multiple Regressions",
    "section": "",
    "text": "type\nincome\neducation\nprestige\n\n\n\n\naccountant\nprof\n62\n86\n82\n\n\npilot\nprof\n72\n76\n83\n\n\narchitect\nprof\n75\n92\n90\n\n\nauthor\nprof\n55\n90\n76\n\n\nchemist\nprof\n64\n86\n90\n\n\n\n\n\n\nLast week we “ran” a linear regression: \\(y = \\alpha + \\beta x\\). Result: \\[\\text{income} = xx + 0.72 \\text{education}\\]\nShould we have looked at “prestige” instead ? \\[\\text{income} = xx + 0.83 \\text{prestige}\\]\nWhich one is better?\n\n\n\n\n\n\n\n\n\nif the goal is to predict: the one with higher explained variance\n\nprestige has higher \\(R^2\\) (\\(0.83^2\\))\n\nunless we are interested in the effect of education\n\n\n\n\n\n\nWhat about using both?\n\n2 variables model: \\[\\text{income} = \\alpha + \\beta_1 \\text{education} + \\beta_2 \\text{prestige}\\]\nwill probably improve prediction power (explained variance)\n\\(\\beta_1\\) might not be meaningful on its own anymore (education and prestige are correlated)\n\n\n\n\n\n\nNow we are trying to fit a plane to a cloud of points.\n \n\n\n\n\n\nTake all observations: \\((\\text{income}\\_n,\\text{education}\\_n,\\text{prestige}\\_n)\\_{n\\in[0,N]}\\)\nObjective: sum of squares \\[ L(\\alpha, \\beta_1, \\beta_2) = \\sum_i \\left( \\underbrace{ \\alpha + \\beta_1 \\text{education}\\_n + \\beta_2 \\text{prestige}\\_n - \\text{income}\\_n }\\_{e_n=\\text{prediction error} }\\right)^2 \\]\nMinimize loss function in \\(\\alpha\\), \\(\\beta_1\\), \\(\\beta_2\\)\nAgain, we can perform numerical optimization (machine learning approach)\n\n… but there is an explicit formula\n\n\n\n\n\n\n\n\n\n\\[Y = \\begin{bmatrix}\n\\text{income}_1 \\\\\\\\\n\\vdots \\\\\\\\\n\\text{income}_N\n\\end{bmatrix}\\] \\[X = \\begin{bmatrix}\n1 & \\text{education}_1 & \\text{prestige}_1 \\\\\\\\\n\\vdots & \\vdots & \\vdots \\\\\\\\\n1 &\\text{education}_N & \\text{prestige}_N\n\\end{bmatrix}\\]\n\n\n\nMatrix Version (look for \\(B = \\left( \\alpha, \\beta_1 , \\beta_2 \\right)\\)): \\[Y =  X B + E\\]\nNote that constant can be interpreted as a “variable”\nLoss function \\[L(A,B) = (Y - X B)' (Y - X B)\\]\nResult of minimization \\(\\min_{(A,B)} L(A,B)\\) : \\[\\begin{bmatrix}\\alpha & \\beta_1 & \\beta_2 \\end{bmatrix} = (X'X)^{-1} X' Y \\]\n\n\n\n\n\n\n\n\nResult: \\[\\text{income} = 10.43  + 0.03 \\times \\text{education} + 0.62 \\times \\text{prestige}\\]\nQuestions:\n\nis it a better regression than the other?\nis the coefficient in front of education significant?\nhow do we interpret it?\ncan we build confidence intervals?\n\n\n\n\n\n\n\n\nAs in the 1d case we can compare:\n\nthe variability of the model predictions (\\(MSS\\))\nthe variance of the data (\\(TSS\\), T for total)\n\nCoefficient of determination: \\[R^2 = \\frac{MSS}{TSS}\\]\nOr: \\[R^2 = 1-\\frac{RSS}{SST}\\] where \\(RSS\\) is the non explained variance\n\n\n\n\n\n\n\nIn our example:\n\n\n\n\n\n\n\n\n\nRegression\n\\(R^2\\)\n \\(R^2_{adj}\\) \n\n\n\n\neducation\n0.525\n 0.514 \n\n\nprestige\n0.702\n 0.695 \n\n\neducation + prestige\n0.7022\n 0.688 \n\n\n\n\n\n\n\nFact:\n\nadding more regressors always improve \\(R^2\\)\nwhy not throw everything in? (kitchen sink regressions)\n\ntwo many regressors: overfitting the data\n\n\nPenalise additional regressors: adjusted R^2\n\nexample formula:\n\n\\(N\\): number of observations\n\\(p\\) number of variables \\[R^2_{adj} = 1-(1-R^2)\\frac{N-1}{N-p-1}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport statsmodels\nWe use a special API inspired by R:\nimport statsmodels.formula.api as smf\n\n\n\n\nRunning a regression with statsmodels\nmodel = smf.ols('income ~ education',  df)  # model\nres = model.fit()  # perform the regression\nres.describe()\n\n‘income ~ education’ is the model formula\n\nResult:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 income   R-squared:                       0.525\nModel:                            OLS   Adj. R-squared:                  0.514\nMethod:                 Least Squares   F-statistic:                     47.51\nDate:                Tue, 02 Feb 2021   Prob (F-statistic):           1.84e-08\nTime:                        05:21:25   Log-Likelihood:                -190.42\nNo. Observations:                  45   AIC:                             384.8\nDf Residuals:                      43   BIC:                             388.5\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n==============================================================================\nIntercept     10.6035      5.198      2.040      0.048       0.120      21.087\neducation      0.5949      0.086      6.893      0.000       0.421       0.769\n==============================================================================\nOmnibus:                        9.841   Durbin-Watson:                   1.736\nProb(Omnibus):                  0.007   Jarque-Bera (JB):               10.609\nSkew:                           0.776   Prob(JB):                      0.00497\nKurtosis:                       4.802   Cond. No.                         123.\n==============================================================================\n\n\n\n\n\nWith statsmodels formulas, can be supplied with R-style syntax\nExamples:\n\n\n\n\n\n\n\n\nFormula\nModel\n\n\n\n\nincome ~ education\n\\(\\text{income}_i = \\alpha + \\beta \\text{education}_i\\)\n\n\nincome ~ prestige\n\\(\\text{income}_i = \\alpha + \\beta \\text{prestige}_i\\)\n\n\nincome ~ prestige - 1\n\\(\\text{income}_i = \\beta \\text{prestige}_i\\) (no intercept)\n\n\nincome ~ education + prestige\n\\(\\text{income}_i = \\alpha + \\beta_1 \\text{education}_i + \\beta_2 \\text{prestige}_i\\)\n\n\n\n\n\n\n\n\nOne can use formulas to apply transformations to variables\n\n\n\n\n\n\n\n\nFormula\nModel\n\n\n\n\nlog(P) ~ log(M) + log(Y)\n\\(\\log(P_i) = \\alpha + \\alpha_1 \\log(M_i) + \\alpha_2 \\log(Y_i)\\) (log-log)\n\n\nlog(Y) ~ i\n\\(\\log(P_i) = \\alpha + i_i\\) (semi-logs)\n\n\n\n\nThis is useful if the true relationship is nonlinear\nAlso useful, to interpret the coefficients\n\n\n\n\n\n\nExample:\n\n(police_spending and prevention_policies in million dollars) \\[ \\text{number_or_crimes} = 0.005\\\\% - 0.001 \\text{pol_spend} - 0.005 \\text{prev_pol} + 0.002 \\text{population density}\\]\n\nreads: when holding other variables constant a 0.1 million increase in police spending reduces crime rate by 0.001%\ninterpretation?\n\nproblematic because variables have different units\nwe can say that prevention policies are more efficient than police spending ceteris paribus\n\nTake logs: \\[ \\log(\\text{number_or_crimes}) = 0.005\\\\% - 0.15 \\log(\\text{pol_spend}) - 0.4 \\log(\\text{prev_pol}) + 0.2 \\log(\\text{population density})\\]\n\nnow we have an estimate of elasticities\na \\(1\\%\\) increase in police spending leads to a \\(0.15\\%\\) decrease in the number of crimes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecall what we do:\n\nwe have the data \\(X,Y\\)\nwe choose a model: \\[ Y = \\alpha + X \\beta \\]\nfrom the data we compute estimates: \\[\\hat{\\beta}  = (X'X)^{-1} X' Y \\] \\[\\hat{\\alpha} = Y- X \\beta \\]\nestimates are a precise function of data\n\nexact formula not important here\n\n\n\n\n\n\n\nWe make some hypotheses on the data generation process:\n\n\\(Y = X \\beta + \\epsilon\\)\n\\(\\mathbb{E}\\left[ \\epsilon \\right] = 0\\)\n\\(\\epsilon\\) multivariate normal with covariance matrix \\(\\sigma^2 I_n\\)\n\n\\(\\forall i, \\sigma(\\epsilon_i) = \\sigma\\)\n\\(\\forall i,j, cov(\\epsilon_i, \\epsilon_j) = 0\\)\n\n\n\nUnder these hypotheses:\n\n\\(\\hat{\\beta}\\) is an unbiased estimate of true parameter \\(\\beta\\)\n\ni.e. \\(\\mathbb{E} [\\hat{\\beta}] = \\beta\\)\n\none can prove \\(Var(\\hat{\\beta}) = \\sigma^2 I_n\\)\n\\(\\sigma\\) can be estimated by \\(\\hat{\\sigma}=S\\frac{\\sum_i (y_i-{pred}_i)^2}{N-p}\\)\n\n\\(N-p\\): degrees of freedoms\n\none can estimate: \\(\\sigma(\\hat{\\beta_k})\\)\n\nit is the \\(i\\)-th diagonal element of \\(\\hat{\\sigma}^2 X'X\\)\n\n\n\n\n\n\n\n\n\n\n\nApproach is very similar to the one-dimensional case\n\nFisher criterium (F-test):\n\n\n\\(H0\\): all coeficients are 0\n\ni.e. true model is \\(y=\\alpha + \\epsilon\\)\n\n\n\\(H1\\): some coefficients are not 0\n\nstatistics: \\[F=\\frac{MSR}{MSE}\\]\n\n\\(MSR\\): mean-squared error of constant model\n\\(MSE\\): mean-squared error of full model\n\n\n\nUnder the model assumptions, distribution of \\(F\\) is known\n\nit is remarkable that it doesn’t depend on \\(\\sigma\\) !\n\n\nOne can produce a p-value.\n\nprobability to obtain this statistics given hypotheses 0\nif very low, H0 is rejected\n\n\n\n\n\n\n\n\nStudent test. Given a an coefficient \\(\\beta_k\\):\n\n\n\\(H0\\): coefficient is 0\n\n\\(H1\\): coefficient is not zero\n\nstatistics: \\(t = \\frac{\\hat{\\beta_k}}{\\hat{\\sigma}(\\hat{\\beta_k})}\\)\n\nwhere \\(\\hat{\\sigma}(\\beta_k)\\) is \\(i\\)-th diagonal element of \\(\\hat{\\sigma}^2 X'X\\)\n\n\n\nUnder the inference hypotheses, distribution of \\(t\\) is known.\n\nit is a student distribution\n\n\nProcedure:\n\n\nCompute \\(t\\). Check acceptance threshold \\(t*\\) at probability \\(\\alpha\\).\n\nCoefficient is significant with probability \\(1-\\alpha\\) if \\(t&gt;t*\\)\n\nOr compute implied acceptance rate \\(\\alpha\\) for \\(t\\).\n\nif \\(t\\) is high enough, null hypothesis is rejected\n\n\n\n\n\n\n\n\nSame as in the 1d case.\nTake estimate \\(\\color{red}{\\beta_i}\\) with an estimate of its standard deviation \\(\\color{red}{\\hat{\\sigma}(\\beta_i)}\\)\nCompute student \\(\\color{red}{t^{\\star}}\\) at \\(\\color{red}{\\alpha}\\) confidence level (ex: \\(\\alpha=5\\\\%\\)) such that:\n\n\\(P(|t|&gt;t^{\\star})&lt;\\alpha\\)\n\nProduce confidence intervals at \\(\\alpha\\) confidence level:\n\n\\([\\color{red}{\\beta_i} - t^{\\star} \\color{red}{\\hat{\\sigma}(\\beta_i)}, \\color{red}{\\beta_i} + t^{\\star} \\color{red}{\\hat{\\sigma}(\\beta_i)}]\\)\n\n\n\n\n\n\n\nThe tests seen so far rely on strong statistical assumptions (normality, homoscedasticity, etc..)\nSome tests can be used to test these assumptions:\n\nJarque-Bera: is the distribution of data truly normal\nDurbin-Watson: are residuals autocorrelated (makes sense for time-series)\n…\n\nIn case assumptions are not met…\n\n… still possible to do econometrics\n… but beyond the scope of this course\n\n\n\n\n\n\n\n\n\n\n\n\nI’ve got plenty of data:\n\n\\(y\\): gdp\n\\(x_1\\): investment\n\\(x_2\\): inflation\n\\(x_3\\): education\n\\(x_4\\): unemployment\n…\n\n\nMany possible regressions:\n\n\\(y = α + \\beta_1 x_1\\)\n\\(y = α + \\beta_2 x_2 + \\beta_3 x_4\\)\n…\n\n\nWhich one do I choose ?\n\nputting everything together is not an option (kitchen sink regression)\n\n\n\n\n\n\n\nSuppose you run a regression: \\[y = \\alpha + \\beta_1 x_1 + \\epsilon\\]\nBut unknowingly to you, the actual model is \\[y = \\alpha + \\beta_1 x_1 + \\beta_2 x_2 + \\eta\\]\nThe residual \\(y - \\alpha - \\beta_1 x_1\\) is not white noise\n\nspecification hypotheses are violated\nestimate \\(\\hat{\\beta_1}\\) will have a bias (omitted variable bias)\nto correct the bias we add \\(x_2\\) (“control” for \\(x_2\\))\n\n\n\n\n\n\n\nSuppose I want to check Okun’s law. I consider the following model: \\[\\text{gdp_growth} = \\alpha + \\beta \\times \\text{unemployment}\\]\nI obtain: \\[\\text{gdp_growth} = 0.01 - 0.1 \\times \\text{unemployment} + e_i\\]\nThen I inspect visually the residuals: not normal at all!\nConclusion: my regression is misspecified, \\(0.1\\) is a biased (useless) estimate\nI need to control for additional variables. For instance: \\[\\text{gdp_growth} = \\alpha + \\beta_1 \\text{unemployment} + \\beta_2 \\text{interest rate}\\]\nUntil the residuals are actually white noise\n\n\n\n\n\n\nWhat happens if two regressors are (almost) colinear? \\[y = \\alpha + \\beta_1 x_1 + \\beta_2 x_2\\] where \\(x_2 = \\kappa x_1\\)\nIntuitively: parameters are not unique\n\nif \\(y = \\alpha + \\beta_1 x_1\\) is the right model…\nthen \\(y = \\alpha + \\beta_1 \\lambda x_1 + \\beta_2 (1-\\lambda) \\frac{1}{\\kappa} x_2\\) is exactly as good…\n\nMathematically: \\((X'X)\\) is not invertible.\nWhen regressors are almost colinear, coefficients can have a lot of variability.\nTest: correlation plot, correlation statistics\n\n\n\n\n\n\\[y = \\alpha + \\beta_1 x_1 + ... \\beta_n x_n\\]\nWhich regressors to choose ?\n\nMethod 1 : remove coefficients with lowest t (less significant) to maximize adjusted R-squared\n\nremove regressors with lowest t\nregress again\nsee if adjusted \\(R^2\\) is decreasing\n\nif so continue\notherwise cancel last step and stop\n\n\nMethod 2 : choose combination to maximize Akaike Information Criterium\n\nAIC: \\(p - log(L)\\)\n\\(L\\) is likelihood\ncomputed by all good econometric softwares\n\n\n\n\n\n\n\n\n\nIntro to causality"
  },
  {
    "objectID": "session_4/graphs/index.html#the-problem",
    "href": "session_4/graphs/index.html#the-problem",
    "title": "Multiple Regressions",
    "section": "",
    "text": "type\nincome\neducation\nprestige\n\n\n\n\naccountant\nprof\n62\n86\n82\n\n\npilot\nprof\n72\n76\n83\n\n\narchitect\nprof\n75\n92\n90\n\n\nauthor\nprof\n55\n90\n76\n\n\nchemist\nprof\n64\n86\n90\n\n\n\n\n\n\nLast week we “ran” a linear regression: \\(y = \\alpha + \\beta x\\). Result: \\[\\text{income} = xx + 0.72 \\text{education}\\]\nShould we have looked at “prestige” instead ? \\[\\text{income} = xx + 0.83 \\text{prestige}\\]\nWhich one is better?\n\n\n\n\n\n\n\n\n\nif the goal is to predict: the one with higher explained variance\n\nprestige has higher \\(R^2\\) (\\(0.83^2\\))\n\nunless we are interested in the effect of education\n\n\n\n\n\n\nWhat about using both?\n\n2 variables model: \\[\\text{income} = \\alpha + \\beta_1 \\text{education} + \\beta_2 \\text{prestige}\\]\nwill probably improve prediction power (explained variance)\n\\(\\beta_1\\) might not be meaningful on its own anymore (education and prestige are correlated)\n\n\n\n\n\n\nNow we are trying to fit a plane to a cloud of points.\n \n\n\n\n\n\nTake all observations: \\((\\text{income}\\_n,\\text{education}\\_n,\\text{prestige}\\_n)\\_{n\\in[0,N]}\\)\nObjective: sum of squares \\[ L(\\alpha, \\beta_1, \\beta_2) = \\sum_i \\left( \\underbrace{ \\alpha + \\beta_1 \\text{education}\\_n + \\beta_2 \\text{prestige}\\_n - \\text{income}\\_n }\\_{e_n=\\text{prediction error} }\\right)^2 \\]\nMinimize loss function in \\(\\alpha\\), \\(\\beta_1\\), \\(\\beta_2\\)\nAgain, we can perform numerical optimization (machine learning approach)\n\n… but there is an explicit formula\n\n\n\n\n\n\n\n\n\n\\[Y = \\begin{bmatrix}\n\\text{income}_1 \\\\\\\\\n\\vdots \\\\\\\\\n\\text{income}_N\n\\end{bmatrix}\\] \\[X = \\begin{bmatrix}\n1 & \\text{education}_1 & \\text{prestige}_1 \\\\\\\\\n\\vdots & \\vdots & \\vdots \\\\\\\\\n1 &\\text{education}_N & \\text{prestige}_N\n\\end{bmatrix}\\]\n\n\n\nMatrix Version (look for \\(B = \\left( \\alpha, \\beta_1 , \\beta_2 \\right)\\)): \\[Y =  X B + E\\]\nNote that constant can be interpreted as a “variable”\nLoss function \\[L(A,B) = (Y - X B)' (Y - X B)\\]\nResult of minimization \\(\\min_{(A,B)} L(A,B)\\) : \\[\\begin{bmatrix}\\alpha & \\beta_1 & \\beta_2 \\end{bmatrix} = (X'X)^{-1} X' Y \\]\n\n\n\n\n\n\n\n\nResult: \\[\\text{income} = 10.43  + 0.03 \\times \\text{education} + 0.62 \\times \\text{prestige}\\]\nQuestions:\n\nis it a better regression than the other?\nis the coefficient in front of education significant?\nhow do we interpret it?\ncan we build confidence intervals?"
  },
  {
    "objectID": "session_4/graphs/index.html#explained-variance",
    "href": "session_4/graphs/index.html#explained-variance",
    "title": "Multiple Regressions",
    "section": "",
    "text": "As in the 1d case we can compare:\n\nthe variability of the model predictions (\\(MSS\\))\nthe variance of the data (\\(TSS\\), T for total)\n\nCoefficient of determination: \\[R^2 = \\frac{MSS}{TSS}\\]\nOr: \\[R^2 = 1-\\frac{RSS}{SST}\\] where \\(RSS\\) is the non explained variance\n\n\n\n\n\n\n\nIn our example:\n\n\n\n\n\n\n\n\n\nRegression\n\\(R^2\\)\n \\(R^2_{adj}\\) \n\n\n\n\neducation\n0.525\n 0.514 \n\n\nprestige\n0.702\n 0.695 \n\n\neducation + prestige\n0.7022\n 0.688 \n\n\n\n\n\n\n\nFact:\n\nadding more regressors always improve \\(R^2\\)\nwhy not throw everything in? (kitchen sink regressions)\n\ntwo many regressors: overfitting the data\n\n\nPenalise additional regressors: adjusted R^2\n\nexample formula:\n\n\\(N\\): number of observations\n\\(p\\) number of variables \\[R^2_{adj} = 1-(1-R^2)\\frac{N-1}{N-p-1}\\]"
  },
  {
    "objectID": "session_4/graphs/index.html#interpretation-and-variable-change",
    "href": "session_4/graphs/index.html#interpretation-and-variable-change",
    "title": "Multiple Regressions",
    "section": "",
    "text": "import statsmodels\nWe use a special API inspired by R:\nimport statsmodels.formula.api as smf\n\n\n\n\nRunning a regression with statsmodels\nmodel = smf.ols('income ~ education',  df)  # model\nres = model.fit()  # perform the regression\nres.describe()\n\n‘income ~ education’ is the model formula\n\nResult:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 income   R-squared:                       0.525\nModel:                            OLS   Adj. R-squared:                  0.514\nMethod:                 Least Squares   F-statistic:                     47.51\nDate:                Tue, 02 Feb 2021   Prob (F-statistic):           1.84e-08\nTime:                        05:21:25   Log-Likelihood:                -190.42\nNo. Observations:                  45   AIC:                             384.8\nDf Residuals:                      43   BIC:                             388.5\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n==============================================================================\nIntercept     10.6035      5.198      2.040      0.048       0.120      21.087\neducation      0.5949      0.086      6.893      0.000       0.421       0.769\n==============================================================================\nOmnibus:                        9.841   Durbin-Watson:                   1.736\nProb(Omnibus):                  0.007   Jarque-Bera (JB):               10.609\nSkew:                           0.776   Prob(JB):                      0.00497\nKurtosis:                       4.802   Cond. No.                         123.\n==============================================================================\n\n\n\n\n\nWith statsmodels formulas, can be supplied with R-style syntax\nExamples:\n\n\n\n\n\n\n\n\nFormula\nModel\n\n\n\n\nincome ~ education\n\\(\\text{income}_i = \\alpha + \\beta \\text{education}_i\\)\n\n\nincome ~ prestige\n\\(\\text{income}_i = \\alpha + \\beta \\text{prestige}_i\\)\n\n\nincome ~ prestige - 1\n\\(\\text{income}_i = \\beta \\text{prestige}_i\\) (no intercept)\n\n\nincome ~ education + prestige\n\\(\\text{income}_i = \\alpha + \\beta_1 \\text{education}_i + \\beta_2 \\text{prestige}_i\\)\n\n\n\n\n\n\n\n\nOne can use formulas to apply transformations to variables\n\n\n\n\n\n\n\n\nFormula\nModel\n\n\n\n\nlog(P) ~ log(M) + log(Y)\n\\(\\log(P_i) = \\alpha + \\alpha_1 \\log(M_i) + \\alpha_2 \\log(Y_i)\\) (log-log)\n\n\nlog(Y) ~ i\n\\(\\log(P_i) = \\alpha + i_i\\) (semi-logs)\n\n\n\n\nThis is useful if the true relationship is nonlinear\nAlso useful, to interpret the coefficients\n\n\n\n\n\n\nExample:\n\n(police_spending and prevention_policies in million dollars) \\[ \\text{number_or_crimes} = 0.005\\\\% - 0.001 \\text{pol_spend} - 0.005 \\text{prev_pol} + 0.002 \\text{population density}\\]\n\nreads: when holding other variables constant a 0.1 million increase in police spending reduces crime rate by 0.001%\ninterpretation?\n\nproblematic because variables have different units\nwe can say that prevention policies are more efficient than police spending ceteris paribus\n\nTake logs: \\[ \\log(\\text{number_or_crimes}) = 0.005\\\\% - 0.15 \\log(\\text{pol_spend}) - 0.4 \\log(\\text{prev_pol}) + 0.2 \\log(\\text{population density})\\]\n\nnow we have an estimate of elasticities\na \\(1\\%\\) increase in police spending leads to a \\(0.15\\%\\) decrease in the number of crimes"
  },
  {
    "objectID": "session_4/graphs/index.html#statistical-inference",
    "href": "session_4/graphs/index.html#statistical-inference",
    "title": "Multiple Regressions",
    "section": "",
    "text": "Recall what we do:\n\nwe have the data \\(X,Y\\)\nwe choose a model: \\[ Y = \\alpha + X \\beta \\]\nfrom the data we compute estimates: \\[\\hat{\\beta}  = (X'X)^{-1} X' Y \\] \\[\\hat{\\alpha} = Y- X \\beta \\]\nestimates are a precise function of data\n\nexact formula not important here\n\n\n\n\n\n\n\nWe make some hypotheses on the data generation process:\n\n\\(Y = X \\beta + \\epsilon\\)\n\\(\\mathbb{E}\\left[ \\epsilon \\right] = 0\\)\n\\(\\epsilon\\) multivariate normal with covariance matrix \\(\\sigma^2 I_n\\)\n\n\\(\\forall i, \\sigma(\\epsilon_i) = \\sigma\\)\n\\(\\forall i,j, cov(\\epsilon_i, \\epsilon_j) = 0\\)\n\n\n\nUnder these hypotheses:\n\n\\(\\hat{\\beta}\\) is an unbiased estimate of true parameter \\(\\beta\\)\n\ni.e. \\(\\mathbb{E} [\\hat{\\beta}] = \\beta\\)\n\none can prove \\(Var(\\hat{\\beta}) = \\sigma^2 I_n\\)\n\\(\\sigma\\) can be estimated by \\(\\hat{\\sigma}=S\\frac{\\sum_i (y_i-{pred}_i)^2}{N-p}\\)\n\n\\(N-p\\): degrees of freedoms\n\none can estimate: \\(\\sigma(\\hat{\\beta_k})\\)\n\nit is the \\(i\\)-th diagonal element of \\(\\hat{\\sigma}^2 X'X\\)\n\n\n\n\n\n\n\n\n\n\n\nApproach is very similar to the one-dimensional case\n\nFisher criterium (F-test):\n\n\n\\(H0\\): all coeficients are 0\n\ni.e. true model is \\(y=\\alpha + \\epsilon\\)\n\n\n\\(H1\\): some coefficients are not 0\n\nstatistics: \\[F=\\frac{MSR}{MSE}\\]\n\n\\(MSR\\): mean-squared error of constant model\n\\(MSE\\): mean-squared error of full model\n\n\n\nUnder the model assumptions, distribution of \\(F\\) is known\n\nit is remarkable that it doesn’t depend on \\(\\sigma\\) !\n\n\nOne can produce a p-value.\n\nprobability to obtain this statistics given hypotheses 0\nif very low, H0 is rejected\n\n\n\n\n\n\n\n\nStudent test. Given a an coefficient \\(\\beta_k\\):\n\n\n\\(H0\\): coefficient is 0\n\n\\(H1\\): coefficient is not zero\n\nstatistics: \\(t = \\frac{\\hat{\\beta_k}}{\\hat{\\sigma}(\\hat{\\beta_k})}\\)\n\nwhere \\(\\hat{\\sigma}(\\beta_k)\\) is \\(i\\)-th diagonal element of \\(\\hat{\\sigma}^2 X'X\\)\n\n\n\nUnder the inference hypotheses, distribution of \\(t\\) is known.\n\nit is a student distribution\n\n\nProcedure:\n\n\nCompute \\(t\\). Check acceptance threshold \\(t*\\) at probability \\(\\alpha\\).\n\nCoefficient is significant with probability \\(1-\\alpha\\) if \\(t&gt;t*\\)\n\nOr compute implied acceptance rate \\(\\alpha\\) for \\(t\\).\n\nif \\(t\\) is high enough, null hypothesis is rejected\n\n\n\n\n\n\n\n\nSame as in the 1d case.\nTake estimate \\(\\color{red}{\\beta_i}\\) with an estimate of its standard deviation \\(\\color{red}{\\hat{\\sigma}(\\beta_i)}\\)\nCompute student \\(\\color{red}{t^{\\star}}\\) at \\(\\color{red}{\\alpha}\\) confidence level (ex: \\(\\alpha=5\\\\%\\)) such that:\n\n\\(P(|t|&gt;t^{\\star})&lt;\\alpha\\)\n\nProduce confidence intervals at \\(\\alpha\\) confidence level:\n\n\\([\\color{red}{\\beta_i} - t^{\\star} \\color{red}{\\hat{\\sigma}(\\beta_i)}, \\color{red}{\\beta_i} + t^{\\star} \\color{red}{\\hat{\\sigma}(\\beta_i)}]\\)\n\n\n\n\n\n\n\nThe tests seen so far rely on strong statistical assumptions (normality, homoscedasticity, etc..)\nSome tests can be used to test these assumptions:\n\nJarque-Bera: is the distribution of data truly normal\nDurbin-Watson: are residuals autocorrelated (makes sense for time-series)\n…\n\nIn case assumptions are not met…\n\n… still possible to do econometrics\n… but beyond the scope of this course"
  },
  {
    "objectID": "session_4/graphs/index.html#variable-selection",
    "href": "session_4/graphs/index.html#variable-selection",
    "title": "Multiple Regressions",
    "section": "",
    "text": "I’ve got plenty of data:\n\n\\(y\\): gdp\n\\(x_1\\): investment\n\\(x_2\\): inflation\n\\(x_3\\): education\n\\(x_4\\): unemployment\n…\n\n\nMany possible regressions:\n\n\\(y = α + \\beta_1 x_1\\)\n\\(y = α + \\beta_2 x_2 + \\beta_3 x_4\\)\n…\n\n\nWhich one do I choose ?\n\nputting everything together is not an option (kitchen sink regression)\n\n\n\n\n\n\n\nSuppose you run a regression: \\[y = \\alpha + \\beta_1 x_1 + \\epsilon\\]\nBut unknowingly to you, the actual model is \\[y = \\alpha + \\beta_1 x_1 + \\beta_2 x_2 + \\eta\\]\nThe residual \\(y - \\alpha - \\beta_1 x_1\\) is not white noise\n\nspecification hypotheses are violated\nestimate \\(\\hat{\\beta_1}\\) will have a bias (omitted variable bias)\nto correct the bias we add \\(x_2\\) (“control” for \\(x_2\\))\n\n\n\n\n\n\n\nSuppose I want to check Okun’s law. I consider the following model: \\[\\text{gdp_growth} = \\alpha + \\beta \\times \\text{unemployment}\\]\nI obtain: \\[\\text{gdp_growth} = 0.01 - 0.1 \\times \\text{unemployment} + e_i\\]\nThen I inspect visually the residuals: not normal at all!\nConclusion: my regression is misspecified, \\(0.1\\) is a biased (useless) estimate\nI need to control for additional variables. For instance: \\[\\text{gdp_growth} = \\alpha + \\beta_1 \\text{unemployment} + \\beta_2 \\text{interest rate}\\]\nUntil the residuals are actually white noise\n\n\n\n\n\n\nWhat happens if two regressors are (almost) colinear? \\[y = \\alpha + \\beta_1 x_1 + \\beta_2 x_2\\] where \\(x_2 = \\kappa x_1\\)\nIntuitively: parameters are not unique\n\nif \\(y = \\alpha + \\beta_1 x_1\\) is the right model…\nthen \\(y = \\alpha + \\beta_1 \\lambda x_1 + \\beta_2 (1-\\lambda) \\frac{1}{\\kappa} x_2\\) is exactly as good…\n\nMathematically: \\((X'X)\\) is not invertible.\nWhen regressors are almost colinear, coefficients can have a lot of variability.\nTest: correlation plot, correlation statistics\n\n\n\n\n\n\\[y = \\alpha + \\beta_1 x_1 + ... \\beta_n x_n\\]\nWhich regressors to choose ?\n\nMethod 1 : remove coefficients with lowest t (less significant) to maximize adjusted R-squared\n\nremove regressors with lowest t\nregress again\nsee if adjusted \\(R^2\\) is decreasing\n\nif so continue\notherwise cancel last step and stop\n\n\nMethod 2 : choose combination to maximize Akaike Information Criterium\n\nAIC: \\(p - log(L)\\)\n\\(L\\) is likelihood\ncomputed by all good econometric softwares"
  },
  {
    "objectID": "session_4/graphs/index.html#coming-next",
    "href": "session_4/graphs/index.html#coming-next",
    "title": "Multiple Regressions",
    "section": "",
    "text": "Intro to causality"
  },
  {
    "objectID": "session_4/graphs/inference.html",
    "href": "session_4/graphs/inference.html",
    "title": "Data-Based Economics",
    "section": "",
    "text": "from matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n\ndef generate_dataset(μ1, μ2, α, β, σ, N=10):\n    xvec = np.random.uniform(μ1, μ2, N)\n    yvec = α + β*xvec + np.random.normal(size=N)*σ\n    return pd.DataFrame({'x': xvec, 'y': yvec})\n\n\ndf = generate_dataset(0.0, 1.0, 0.1, 0.8, 0.1)\n\n\nplt.plot(df['x'], df['y'], 'o')\nplt.grid()\n\n\n\n\n\ndef plot_distribution(α, β, σ, N=100000, μ1=0.0, μ2=1.0):\n    xvec = np.random.uniform(μ1, μ2, N)\n    yvec = α + β*xvec + np.random.normal(size=N)*σ\n    plt.plot(xvec, yvec, '.r', alpha=0.005)\n    plt.plot(xvec, α + β*xvec, color='black')\n\n# missing ridge line\n\n\nimport statsmodels\n\n\nμ1 = 0\nμ2 = 1.0\nα = 0.1\nβ = 0.8\nσ = 0.2\nN = 20\nK = 1000\n\n\nimport statsmodels.formula.api as smf\n\n\ndf = generate_dataset(μ1, μ2, α, β, σ, N=N)\n\n\nres = smf.ols(formula='y ~ x + 1', data=df).fit()\nparams = res.params\nαhat = params['Intercept']\nβhat = params['x']\nσhat = res.resid.std()\n\n\nres.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\ny\nR-squared:\n0.692\n\n\nModel:\nOLS\nAdj. R-squared:\n0.675\n\n\nMethod:\nLeast Squares\nF-statistic:\n40.48\n\n\nDate:\nTue, 26 Jan 2021\nProb (F-statistic):\n5.41e-06\n\n\nTime:\n04:02:36\nLog-Likelihood:\n7.6662\n\n\nNo. Observations:\n20\nAIC:\n-11.33\n\n\nDf Residuals:\n18\nBIC:\n-9.341\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n0.1210\n0.077\n1.565\n0.135\n-0.041\n0.283\n\n\nx\n0.7941\n0.125\n6.362\n0.000\n0.532\n1.056\n\n\n\n\n\n\nOmnibus:\n1.410\nDurbin-Watson:\n1.507\n\n\nProb(Omnibus):\n0.494\nJarque-Bera (JB):\n0.890\n\n\nSkew:\n-0.081\nProb(JB):\n0.641\n\n\nKurtosis:\n1.979\nCond. No.\n4.20\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nres.predict(df['x'])\n\n0     0.326200\n1     0.211704\n2     0.798819\n3     0.603306\n4     0.573319\n5     0.823919\n6     0.740622\n7     0.503227\n8     0.292622\n9     0.489566\n10    0.138720\n11    0.355157\n12    0.594171\n13    0.883917\n14    0.266229\n15    0.827021\n16    0.912376\n17    0.163088\n18    0.684858\n19    0.732782\ndtype: float64\n\n\n\nfor i in [1,2,3]:\n    \n    fig = plt.figure(figsize=(10,14))\n    plt.subplot(311)\n    plot_distribution(0.1, 0.8, 0.2)\n    plt.grid()\n    plt.title(f\"True Distribution: $y = {α:.2f} + {β:.2f} x + {σ:.2f} u$\")\n    plt.xlim(0,1)\n    plt.ylim(-0.5, 1.5)\n\n    plt.subplot(312)\n    plt.xlim(0,1)\n    plt.ylim(-0.5, 1.5)\n    if i&gt;=2:\n        plt.plot(df['x'], df['y'], 'o')\n    if i&gt;=3:\n        plt.plot(df['x'], res.predict(), label=f'$\\hat{{α}}={αhat:.2f}; \\hat{{β}}={βhat:.2f}$')\n        plt.legend(loc='lower right')\n    plt.title(\"Random Draw\")\n    plt.grid()\n    \n    plt.savefig(f\"regression_uncertainty_{i}.png\")\n\n\n\n\n\n\n\n\n\n\n\nimport scipy.stats\n\n\ndatasets = [generate_dataset(μ1, μ2, αhat, βhat, σhat, N=N) for i in range(K)]\nall_params = [smf.ols(formula='x ~ y + 1', data=df).fit() for df in datasets]\nαvec = np.array( [e.params['Intercept'] for e in all_params] )\nβvec = np.array( [e.params['y'] for e in all_params] )\n\n\ngkd = scipy.stats.kde.gaussian_kde(βvec)\n\n\nfor i in [1,2,3,4,5,6,7,8,9,10,100]:\n\n    fig = plt.figure(figsize=(10,14))\n    plt.subplot(311)\n    plot_distribution(0.1, 0.8, 0.2)\n    plt.grid()\n    plt.title(f\"True Distribution: $y = {αhat:.2f} + {βhat:.2f} x + {σhat:.2f} u$\")\n    plt.xlim(0,1)\n    plt.ylim(-0.5, 1.5)\n    \n    plt.subplot(312)\n    plt.xlim(0,1)\n    plt.ylim(-0.5, 1.5)\n    df = datasets[i]\n    if i&gt;=2:\n        plt.plot(df['x'], df['y'], 'o')\n    plt.title(\"Random Draw\")\n    plt.grid()\n\n    plt.subplot(313)\n    if i==3:\n        plt.plot(βvec[i], βvec[i]*0, 'o')\n    if i&gt;4:\n        plt.plot(βvec[3:i], βvec[3:i]*0, 'o')\n    if i&gt;10:\n        xx = np.linspace(0.2, 1.4, 10000)\n        plt.plot( βvec, gkd.pdf(βvec), '.')\n    plt.title(\"Distribution of β\")\n    plt.xlim(0.2, 1.4)\n    plt.ylim(-0.1, 4)\n    plt.grid()\n\n    plt.tight_layout()\n\n    plt.savefig(f\"random_estimates_{i}.png\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplt.plot( βvec, βvec*0, 'o')"
  },
  {
    "objectID": "session_4/graphs/Untitled1.html",
    "href": "session_4/graphs/Untitled1.html",
    "title": "Data-Based Economics",
    "section": "",
    "text": "from matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n\nimport statsmodels.api as sm\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nincome\neducation\nprestige\n\n\n\n\ncount\n45.000000\n45.000000\n45.000000\n\n\nmean\n41.866667\n52.555556\n47.688889\n\n\nstd\n24.435072\n29.760831\n31.510332\n\n\nmin\n7.000000\n7.000000\n3.000000\n\n\n25%\n21.000000\n26.000000\n16.000000\n\n\n50%\n42.000000\n45.000000\n41.000000\n\n\n75%\n64.000000\n84.000000\n81.000000\n\n\nmax\n81.000000\n100.000000\n97.000000\n\n\n\n\n\n\n\n\ndf.cov()\n\n\n\n\n\n\n\n\nincome\neducation\nprestige\n\n\n\n\nincome\n597.072727\n526.871212\n645.071212\n\n\neducation\n526.871212\n885.707071\n798.904040\n\n\nprestige\n645.071212\n798.904040\n992.901010\n\n\n\n\n\n\n\n\nfrom matplotlib import pyplot as plt\n\n\nplt.figure(figsize=(8,6))\nplt.plot(df['education'],df['income'],'o')\nplt.grid()\nplt.xlabel(\"x (Education)\")\nplt.ylabel(\"y (Income)\")\nplt.savefig(\"data_description.png\")\n\n\n\n\n\nfor i in [1,2,3]:\n    xvec = np.linspace(10,100)\n\n    plt.figure(figsize=(12,8))\n    plt.plot(df['education'],df['income'],'o')\n\n    plt.plot(xvec, xvec * 0 + 50)\n    if i&gt;=2:\n        plt.plot(xvec, xvec )\n    if i&gt;=3:\n        plt.plot(xvec,  90- 0.6*xvec )\n\n    plt.grid()\n    plt.xlabel(\"x (Education)\")\n    plt.ylabel(\"y (Income)\")\n    plt.savefig(f\"which_line_{i}.png\")\n\n\n\n\n\n\n\n\n\n\n\nfrom ipywidgets import interact\n\n\nimport matplotlib.patches as patches\n\n\na = 0.1\nb = 1.0\nind = 23\n\n\napprox =  a + b*xvec\n\n# Create figure and axes\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\nplt.plot(df['education'],df['income'],'o')\nplt.plot(xvec, approx, color='red')\n\nx, y = df['education'][ind], df['income'][ind]\nplt.plot(x, y, 'o', color='red' )\np = a+b*x\nplt.grid(True)\nh = abs(p-y)\nplt.vlines(x, y+h, y, color='red')\n\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.savefig(f\"error_0.png\")\n\n\n\n\n\nplt.vlines?\n\n\nSignature:\nplt.vlines(\n    x,\n    ymin,\n    ymax,\n    colors=None,\n    linestyles='solid',\n    label='',\n    *,\n    data=None,\n    **kwargs,\n)\nDocstring:\nPlot vertical lines.\nPlot vertical lines at each *x* from *ymin* to *ymax*.\nParameters\n----------\nx : float or array-like\n    x-indexes where to plot the lines.\nymin, ymax : float or array-like\n    Respective beginning and end of each line. If scalars are\n    provided, all lines will have same length.\ncolors : list of colors, default: :rc:`lines.color`\nlinestyles : {'solid', 'dashed', 'dashdot', 'dotted'}, optional\nlabel : str, default: ''\nReturns\n-------\n`~matplotlib.collections.LineCollection`\nOther Parameters\n----------------\n**kwargs : `~matplotlib.collections.LineCollection` properties.\nSee Also\n--------\nhlines : horizontal lines\naxvline: vertical line across the axes\nNotes\n-----\n.. note::\n    In addition to the above described arguments, this function can take\n    a *data* keyword argument. If such a *data* argument is given,\n    the following arguments can also be string ``s``, which is\n    interpreted as ``data[s]`` (unless this raises an exception):\n    *x*, *ymin*, *ymax*, *colors*.\n    Objects passed as **data** must support item access (``data[s]``) and\n    membership test (``s in data``).\nFile:      ~/.local/opt/miniconda/lib/python3.8/site-packages/matplotlib/pyplot.py\nType:      function\n\n\n\n\n\na = 0.1\nb = 1.0\nind = 23\n\n\napprox =  a + b*xvec\n\n# Create figure and axes\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\nplt.plot(df['education'],df['income'],'o')\nplt.plot(xvec, approx, color='red')\n\nx, y = df['education'][ind], df['income'][ind]\nplt.plot(x, y, 'o', color='red' )\np = a+b*x\nplt.grid(True)\nh = abs(p-y)\nif p-y&gt;0:\n    # Create a Rectangle patch\n    rect = patches.Rectangle((x,y),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n    ax.add_patch(rect)\n    \nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.savefig(f\"errors_{1}.png\")\n\n\n\n\n\ndef L(a,b):\n    Δ = a + b*df['education'] - df['income']\n    return (Δ**2).sum()\n\n\na = 0.1\nb = 0.8\n\napprox =  a + b*xvec\n\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\n\n\nplt.plot(df['education'],df['income'],'o', label=f\"L({a,b})={L(a,b)}\")\nplt.plot(xvec, approx, color='red')\n\nplt.grid(True)\nfor ind in range(df.shape[0]):\n    \n    x, y = df['education'][ind], df['income'][ind]\n    p = a+b*x\n\n    h = abs(p-y)\n    if p-y&gt;0:\n        # Create a Rectangle patch\n        rect = patches.Rectangle((x,y),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\n    else:\n        rect = patches.Rectangle((x,y-h),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.legend(loc='upper right')\nplt.savefig(f\"errors_2.png\")\n\n\n\n\n\na = 90\nb = -0.6\n\napprox =  a + b*xvec\n\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\n\n\nplt.plot(df['education'],df['income'],'o', label=f\"L({a,b})={L(a,b)}\")\nplt.plot(xvec, approx, color='red')\n\nplt.grid(True)\nfor ind in range(df.shape[0]):\n    \n    x, y = df['education'][ind], df['income'][ind]\n    p = a+b*x\n\n    h = abs(p-y)\n    if p-y&gt;0:\n        # Create a Rectangle patch\n        rect = patches.Rectangle((x,y),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\n    else:\n        rect = patches.Rectangle((x,y-h),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.legend(loc='upper right')\nplt.savefig(f\"errors_3.png\")\n\n\n\n\n\nimport scipy.optimize\n\n\nscipy.optimize.minimize(lambda x: L(x[0], x[1]),np.array([0.5, 0.5]))\n\n      fun: 12480.970174488397\n hess_inv: array([[ 7.14169839e-09, -3.91281920e-09],\n       [-3.91281920e-09,  2.46663613e-09]])\n      jac: array([0.00024414, 0.00012207])\n  message: 'Desired error not necessarily achieved due to precision loss.'\n     nfev: 57\n      nit: 7\n     njev: 19\n   status: 2\n  success: False\n        x: array([10.60350224,  0.59485938])\n\n\n\na = 10\nb = 0.59\n\napprox =  a + b*xvec\n\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\n\n\nplt.plot(df['education'],df['income'],'o', label=f\"L({a,b})={L(a,b)}\")\nplt.plot(xvec, approx, color='red')\n\nplt.grid(True)\nfor ind in range(df.shape[0]):\n    \n    x, y = df['education'][ind], df['income'][ind]\n    p = a+b*x\n\n    h = abs(p-y)\n    if p-y&gt;0:\n        # Create a Rectangle patch\n        rect = patches.Rectangle((x,y),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\n    else:\n        rect = patches.Rectangle((x,y-h),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.legend(loc='upper right')\nplt.savefig(f\"errors_4.png\")\n\n\n\n\n\na = 10\nb = 0.59\n\napprox =  a + b*xvec\n\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\n\n\nplt.plot(df['education'],df['income'],'o', label=f\"L({a,b})={L(a,b)}\")\nplt.plot(xvec, approx, color='red', alpha=0.5)\n\nplt.plot(60, a + b*60, 'o', color='red',)\n\nprint(a+b*60)\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.legend(loc='upper right')\nplt.savefig(f\"prediction.png\")\n\n45.4\n\n\n\n\n\n\na = 10\nb = 0.59\n\napprox =  (a + b*df['education'] - df['income'])\n\nplt.figure(figsize=(12,6))\n\nplt.subplot(121)\nplt.plot(approx)\nplt.grid(False)\nplt.title(\"Residuals\")\n\n\nplt.subplot(122)\ndistplot(approx)\nplt.title(\"Distribution of residuals\")\nplt.grid()\n\nplt.savefig(\"residuals.png\")\n\n/home/pablo/.local/opt/miniconda/lib/python3.8/site-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n  warnings.warn(msg, FutureWarning)\n\n\n\n\n\n\n(a + b*df['education'] - df['income']).std()\n\n16.842782676352154\n\n\n\n\n\n/home/pablo/.local/opt/miniconda/lib/python3.8/site-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n  warnings.warn(msg, FutureWarning)\n\n\n&lt;AxesSubplot:ylabel='Density'&gt;\n\n\n\n\n\n\nfrom scipy.stats import f\n\n\nf(0.3)\n\nTypeError: _parse_args() missing 1 required positional argument: 'dfd'\n\n\n\nnp.rand\n\n\nK = 100\nxvec = np.linspace(0,1,K)\ne1 = np.random.randn(K)*0.1\nyvec = 0.1 + xvec*0.4 + e1\ne2 = np.random.randn(K)*0.05\nyvec2 = 0.1 + xvec*(xvec-1)/2 + e2\ne3 = np.random.randn(K)*xvec/2\nyvec3 = 0.1 + xvec + e3\n\nyvec4 = 0.1 + np.sin(xvec*6) + np.random.randn(K)*xvec/2\n\n\nfrom dolo.numeric.processes import VAR1\n\n\nsim = VAR1( ρ=0.8, Σ=0.001).simulate(N=1,T=100)\nyvec4 = 0.1 + xvec*0.4 + sim.ravel()\n\n\nplt.figure(figsize=(18,6))\nplt.subplot(241)\nplt.plot(xvec, yvec,'o')\nplt.plot(xvec, 0.1 + xvec*0.4 )\nplt.ylabel(\"Series\")\nplt.title(\"white noise\")\nplt.subplot(242)\nplt.plot(xvec, yvec2, 'o')\nplt.plot(xvec, yvec2*0)\nplt.title('nonlinear')\nplt.subplot(243)\nplt.plot(xvec, yvec3,'o')\nplt.plot(xvec, 0.1 + xvec)\nplt.title('heteroskedastic')\nplt.subplot(244)\nplt.plot(xvec, yvec4,'o')\nplt.plot(xvec, xvec*0.6)\n\nplt.title('correlated')\n\n\nplt.subplot(245)\nplt.plot(xvec, e1,'o')\nplt.ylabel(\"Residuals\")\nplt.subplot(246)\nplt.plot(xvec, yvec2-0.075, 'o')\n\nplt.subplot(247)\nplt.plot(xvec, e3,'o')\nplt.subplot(248)\nplt.plot(xvec, sim.ravel(),'o')\n\nplt.tight_layout()\n\nplt.savefig(\"residuals_circus.png\")"
  },
  {
    "objectID": "session_4/slides.html#remember-dataset-from-last-time",
    "href": "session_4/slides.html#remember-dataset-from-last-time",
    "title": "Multiple Regressions",
    "section": "Remember dataset from last time",
    "text": "Remember dataset from last time\n\n\n\n\n\n\ntype\nincome\neducation\nprestige\n\n\n\n\naccountant\nprof\n62\n86\n82\n\n\npilot\nprof\n72\n76\n83\n\n\narchitect\nprof\n75\n92\n90\n\n\nauthor\nprof\n55\n90\n76\n\n\nchemist\nprof\n64\n86\n90\n\n\n\n\n\nLast week we “ran” a linear regression: \\(y = \\alpha + \\beta x\\). Result: \\[\\text{income} = xx + 0.72 \\text{education}\\]\nShould we have looked at “prestige” instead ? \\[\\text{income} = xx + 0.83 \\text{prestige}\\]\nWhich one is better?"
  },
  {
    "objectID": "session_4/slides.html#prestige-or-education",
    "href": "session_4/slides.html#prestige-or-education",
    "title": "Multiple Regressions",
    "section": "Prestige or Education",
    "text": "Prestige or Education\n\n\nif the goal is to predict: the one with higher explained variance\n\nprestige has higher \\(R^2\\) (\\(0.83^2\\))\n\nunless we are interested in the effect of education"
  },
  {
    "objectID": "session_4/slides.html#multiple-regression",
    "href": "session_4/slides.html#multiple-regression",
    "title": "Multiple Regressions",
    "section": "Multiple regression",
    "text": "Multiple regression\n\nWhat about using both?\n\n2 variables model: \\[\\text{income} = \\alpha + \\beta_1 \\text{education} + \\beta_2 \\text{prestige}\\]\nwill probably improve prediction power (explained variance)\n\\(\\beta_1\\) might not be meaningful on its own anymore (education and prestige are correlated)"
  },
  {
    "objectID": "session_4/slides.html#fitting-a-model",
    "href": "session_4/slides.html#fitting-a-model",
    "title": "Multiple Regressions",
    "section": "Fitting a model",
    "text": "Fitting a model\nNow we are trying to fit a plane to a cloud of points."
  },
  {
    "objectID": "session_4/slides.html#minimization-criterium",
    "href": "session_4/slides.html#minimization-criterium",
    "title": "Multiple Regressions",
    "section": "Minimization Criterium",
    "text": "Minimization Criterium\n\nTake all observations: \\((\\text{income}\\_n,\\text{education}\\_n,\\text{prestige}\\_n)\\_{n\\in[0,N]}\\)\nObjective: sum of squares \\[ L(\\alpha, \\beta_1, \\beta_2) = \\sum_i \\left( \\underbrace{ \\alpha + \\beta_1 \\text{education}\\_n + \\beta_2 \\text{prestige}\\_n - \\text{income}\\_n }\\_{e_n=\\text{prediction error} }\\right)^2 \\]\nMinimize loss function in \\(\\alpha\\), \\(\\beta_1\\), \\(\\beta_2\\)\nAgain, we can perform numerical optimization (machine learning approach)\n\n… but there is an explicit formula"
  },
  {
    "objectID": "session_4/slides.html#ordinary-least-square",
    "href": "session_4/slides.html#ordinary-least-square",
    "title": "Multiple Regressions",
    "section": "Ordinary Least Square",
    "text": "Ordinary Least Square\n\n\n\n\\[Y = \\begin{bmatrix}\n\\text{income}_1 \\\\\\\\\n\\vdots \\\\\\\\\n\\text{income}_N\n\\end{bmatrix}\\] \\[X = \\begin{bmatrix}\n1 & \\text{education}_1 & \\text{prestige}_1 \\\\\\\\\n\\vdots & \\vdots & \\vdots \\\\\\\\\n1 &\\text{education}_N & \\text{prestige}_N\n\\end{bmatrix}\\]\n\n\nMatrix Version (look for \\(B = \\left( \\alpha, \\beta_1 , \\beta_2 \\right)\\)): \\[Y =  X B + E\\]\nNote that constant can be interpreted as a “variable”\nLoss function \\[L(A,B) = (Y - X B)' (Y - X B)\\]\nResult of minimization \\(\\min_{(A,B)} L(A,B)\\) : \\[\\begin{bmatrix}\\alpha & \\beta_1 & \\beta_2 \\end{bmatrix} = (X'X)^{-1} X' Y \\]"
  },
  {
    "objectID": "session_4/slides.html#solution",
    "href": "session_4/slides.html#solution",
    "title": "Multiple Regressions",
    "section": "Solution",
    "text": "Solution\n\nResult: \\[\\text{income} = 10.43  + 0.03 \\times \\text{education} + 0.62 \\times \\text{prestige}\\]\nQuestions:\n\nis it a better regression than the other?\nis the coefficient in front of education significant?\nhow do we interpret it?\ncan we build confidence intervals?"
  },
  {
    "objectID": "session_4/slides.html#explained-variance-1",
    "href": "session_4/slides.html#explained-variance-1",
    "title": "Multiple Regressions",
    "section": "Explained Variance",
    "text": "Explained Variance\n\nAs in the 1d case we can compare:\n\nthe variability of the model predictions (\\(MSS\\))\nthe variance of the data (\\(TSS\\), T for total)\n\nCoefficient of determination: \\[R^2 = \\frac{MSS}{TSS}\\]\nOr: \\[R^2 = 1-\\frac{RSS}{SST}\\] where \\(RSS\\) is the non explained variance"
  },
  {
    "objectID": "session_4/slides.html#adjusted-r-squared",
    "href": "session_4/slides.html#adjusted-r-squared",
    "title": "Multiple Regressions",
    "section": "Adjusted R squared",
    "text": "Adjusted R squared\n\n\n\nIn our example:\n\n\n\n\n\n\n\n\n\nRegression\n\\(R^2\\)\n \\(R^2_{adj}\\) \n\n\n\n\neducation\n0.525\n 0.514 \n\n\nprestige\n0.702\n 0.695 \n\n\neducation + prestige\n0.7022\n 0.688 \n\n\n\n\n\n\nFact:\n\nadding more regressors always improve \\(R^2\\)\nwhy not throw everything in? (kitchen sink regressions)\n\ntwo many regressors: overfitting the data\n\n\nPenalise additional regressors: adjusted R^2\n\nexample formula:\n\n\\(N\\): number of observations\n\\(p\\) number of variables \\[R^2_{adj} = 1-(1-R^2)\\frac{N-1}{N-p-1}\\]"
  },
  {
    "objectID": "session_4/slides.html#making-a-regression-with-statsmodels",
    "href": "session_4/slides.html#making-a-regression-with-statsmodels",
    "title": "Multiple Regressions",
    "section": "Making a regression with statsmodels",
    "text": "Making a regression with statsmodels\nimport statsmodels\nWe use a special API inspired by R:\nimport statsmodels.formula.api as smf"
  },
  {
    "objectID": "session_4/slides.html#performing-a-regression",
    "href": "session_4/slides.html#performing-a-regression",
    "title": "Multiple Regressions",
    "section": "Performing a regression",
    "text": "Performing a regression\n\nRunning a regression with statsmodels\n\nmodel = smf.ols('income ~ education',  df)  # model\nres = model.fit()  # perform the regression\nres.describe()\n\n‘income ~ education’ is the model formula\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 income   R-squared:                       0.525\nModel:                            OLS   Adj. R-squared:                  0.514\nMethod:                 Least Squares   F-statistic:                     47.51\nDate:                Tue, 02 Feb 2021   Prob (F-statistic):           1.84e-08\nTime:                        05:21:25   Log-Likelihood:                -190.42\nNo. Observations:                  45   AIC:                             384.8\nDf Residuals:                      43   BIC:                             388.5\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n==============================================================================\nIntercept     10.6035      5.198      2.040      0.048       0.120      21.087\neducation      0.5949      0.086      6.893      0.000       0.421       0.769\n==============================================================================\nOmnibus:                        9.841   Durbin-Watson:                   1.736\nProb(Omnibus):                  0.007   Jarque-Bera (JB):               10.609\nSkew:                           0.776   Prob(JB):                      0.00497\nKurtosis:                       4.802   Cond. No.                         123.\n=============================================================================="
  },
  {
    "objectID": "session_4/slides.html#formula-mini-language",
    "href": "session_4/slides.html#formula-mini-language",
    "title": "Multiple Regressions",
    "section": "Formula mini-language",
    "text": "Formula mini-language\n\nWith statsmodels formulas, can be supplied with R-style syntax\nExamples:\n\n\n\n\n\n\n\n\nFormula\nModel\n\n\n\n\nincome ~ education\n\\(\\text{income}_i = \\alpha + \\beta \\text{education}_i\\)\n\n\nincome ~ prestige\n\\(\\text{income}_i = \\alpha + \\beta \\text{prestige}_i\\)\n\n\nincome ~ prestige - 1\n\\(\\text{income}_i = \\beta \\text{prestige}_i\\) (no intercept)\n\n\nincome ~ education + prestige\n\\(\\text{income}_i = \\alpha + \\beta_1 \\text{education}_i + \\beta_2 \\text{prestige}_i\\)"
  },
  {
    "objectID": "session_4/slides.html#formula-mini-language-1",
    "href": "session_4/slides.html#formula-mini-language-1",
    "title": "Multiple Regressions",
    "section": "Formula mini-language",
    "text": "Formula mini-language\n\nOne can use formulas to apply transformations to variables\n\n\n\n\n\n\n\n\nFormula\nModel\n\n\n\n\nlog(P) ~ log(M) + log(Y)\n\\(\\log(P_i) = \\alpha + \\alpha_1 \\log(M_i) + \\alpha_2 \\log(Y_i)\\) (log-log)\n\n\nlog(Y) ~ i\n\\(\\log(P_i) = \\alpha + i_i\\) (semi-logs)\n\n\n\n\nThis is useful if the true relationship is nonlinear\nAlso useful, to interpret the coefficients"
  },
  {
    "objectID": "session_4/slides.html#coefficients-interpetation",
    "href": "session_4/slides.html#coefficients-interpetation",
    "title": "Multiple Regressions",
    "section": "Coefficients interpetation",
    "text": "Coefficients interpetation\n\nExample:\n\n(police_spending and prevention_policies in million dollars) \\[\\text{number_or_crimes} = 0.005\\% - 0.001 \\text{pol_spend} - 0.005 \\text{prev_pol} + 0.002 \\text{population density}\\]\n\nreads: when holding other variables constant a 0.1 million increase in police spending reduces crime rate by 0.001%\ninterpretation?\n\nproblematic because variables have different units\nwe can say that prevention policies are more efficient than police spending ceteris paribus\n\nTake logs: \\[\\log(\\text{number_or_crimes}) = 0.005\\% - 0.15 \\log(\\text{pol_spend}) - 0.4 \\log(\\text{prev_pol}) + 0.2 \\log(\\text{population density})\\]\n\nnow we have an estimate of elasticities\na \\(1\\%\\) increase in police spending leads to a \\(0.15\\%\\) decrease in the number of crimes"
  },
  {
    "objectID": "session_4/slides.html#hypotheses",
    "href": "session_4/slides.html#hypotheses",
    "title": "Multiple Regressions",
    "section": "Hypotheses",
    "text": "Hypotheses\n\n\n\n\nRecall what we do:\n\nwe have the data \\(X,Y\\)\nwe choose a model: \\[ Y = \\alpha + X \\beta \\]\nfrom the data we compute estimates: \\[\\hat{\\beta}  = (X'X)^{-1} X' Y \\] \\[\\hat{\\alpha} = Y- X \\beta \\]\nestimates are a precise function of data\n\nexact formula not important here\n\n\n\n\n\n\nWe make some hypotheses on the data generation process:\n\n\\(Y = X \\beta + \\epsilon\\)\n\\(\\mathbb{E}\\left[ \\epsilon \\right] = 0\\)\n\\(\\epsilon\\) multivariate normal with covariance matrix \\(\\sigma^2 I_n\\)\n\n\\(\\forall i, \\sigma(\\epsilon_i) = \\sigma\\)\n\\(\\forall i,j, cov(\\epsilon_i, \\epsilon_j) = 0\\)\n\n\n\nUnder these hypotheses:\n\n\\(\\hat{\\beta}\\) is an unbiased estimate of true parameter \\(\\beta\\)\n\ni.e. \\(\\mathbb{E} [\\hat{\\beta}] = \\beta\\)\n\none can prove \\(Var(\\hat{\\beta}) = \\sigma^2 I_n\\)\n\\(\\sigma\\) can be estimated by \\(\\hat{\\sigma}=S\\frac{\\sum_i (y_i-{pred}_i)^2}{N-p}\\)\n\n\\(N-p\\): degrees of freedoms\n\none can estimate: \\(\\sigma(\\hat{\\beta_k})\\)\n\nit is the \\(i\\)-th diagonal element of \\(\\hat{\\sigma}^2 X'X\\)"
  },
  {
    "objectID": "session_4/slides.html#is-the-regression-significant",
    "href": "session_4/slides.html#is-the-regression-significant",
    "title": "Multiple Regressions",
    "section": "Is the regression significant?",
    "text": "Is the regression significant?\n\n\n\nApproach is very similar to the one-dimensional case\nFisher criterium (F-test):\n\n\\(H0\\): all coeficients are 0\n\ni.e. true model is \\(y=\\alpha + \\epsilon\\)\n\n\\(H1\\): some coefficients are not 0\n\nStatistics: \\[F=\\frac{MSR}{MSE}\\]\n\n\\(MSR\\): mean-squared error of constant model\n\\(MSE\\): mean-squared error of full model\n\n\n\n\n\nUnder:\n\nthe model assumptions about the data generation process\nthe H0 hypothesis\n\n… the distribution of \\(F\\) is known\nIt is remarkable that it doesn’t depend on \\(\\sigma\\) !\n\nOne can produce a p-value.\n\nprobability to obtain this statistics given hypothesis H0\nif very low, H0 is rejected"
  },
  {
    "objectID": "session_4/slides.html#is-each-coefficient-significant",
    "href": "session_4/slides.html#is-each-coefficient-significant",
    "title": "Multiple Regressions",
    "section": "Is each coefficient significant ?",
    "text": "Is each coefficient significant ?\n\nStudent test. Given a coefficient \\(\\beta_k\\):\n\n\\(H0\\): coefficient is 0\n\\(H1\\): coefficient is not zero\n\nStatistics: \\(t = \\frac{\\hat{\\beta_k}}{\\hat{\\sigma}(\\hat{\\beta_k})}\\)\n\nwhere \\(\\hat{\\sigma}(\\beta_k)\\) is \\(i\\)-th diagonal element of \\(\\hat{\\sigma}^2 X'X\\)\nit compares the estimated value of a coefficient to its estimated standard deviation\n\nUnder the inference hypotheses, distribution of \\(t\\) is known.\n\nit is a student distribution\n\nProcedure:\n\nCompute \\(t\\). Check acceptance threshold \\(t*\\) at probability \\(\\alpha\\) (ex 5%)\nCoefficient is significant with probability \\(1-\\alpha\\) if \\(t&gt;t*\\)\nOr just look at the \\(p-value\\): probability that \\(t\\) would be as high as it is, assuming \\(H0\\)"
  },
  {
    "objectID": "session_4/slides.html#confidence-intervals",
    "href": "session_4/slides.html#confidence-intervals",
    "title": "Multiple Regressions",
    "section": "Confidence intervals",
    "text": "Confidence intervals\n\nSame as in the 1d case.\nTake estimate \\(\\color{red}{\\beta_i}\\) with an estimate of its standard deviation \\(\\color{red}{\\hat{\\sigma}(\\beta_i)}\\)\nCompute student \\(\\color{red}{t^{\\star}}\\) at \\(\\color{red}{\\alpha}\\) confidence level (ex: \\(\\alpha=5\\\\%\\)) such that:\n\n\\(P(|t|&gt;t^{\\star})&lt;\\alpha\\)\n\nProduce confidence intervals at \\(\\alpha\\) confidence level:\n\n\\([\\color{red}{\\beta_i} - t^{\\star} \\color{red}{\\hat{\\sigma}(\\beta_i)}, \\color{red}{\\beta_i} + t^{\\star} \\color{red}{\\hat{\\sigma}(\\beta_i)}]\\)\n\nInterpretation:\n\nfor a given confidence interval at confidence level \\(\\alpha\\)…\nthe probability that our coefficient was obtained, if the true coefficient were outside of it, is smaller than \\(\\alpha\\)"
  },
  {
    "objectID": "session_4/slides.html#other-tests",
    "href": "session_4/slides.html#other-tests",
    "title": "Multiple Regressions",
    "section": "Other tests",
    "text": "Other tests\n\nThe tests seen so far rely on strong statistical assumptions (normality, homoscedasticity, etc..)\nSome tests can be used to test these assumptions:\n\nJarque-Bera: is the distribution of data truly normal\nDurbin-Watson: are residuals autocorrelated (makes sense for time-series)\n…\n\nIn case assumptions are not met…\n\n… still possible to do econometrics\n… but beyond the scope of this course"
  },
  {
    "objectID": "session_4/slides.html#variable-selection-1",
    "href": "session_4/slides.html#variable-selection-1",
    "title": "Multiple Regressions",
    "section": "Variable selection",
    "text": "Variable selection\n\n\n\nI’ve got plenty of data:\n\n\\(y\\): gdp\n\\(x_1\\): investment\n\\(x_2\\): inflation\n\\(x_3\\): education\n\\(x_4\\): unemployment\n…\n\n\n\n\nMany possible regressions:\n\n\\(y = α + \\beta_1 x_1\\)\n\\(y = α + \\beta_2 x_2 + \\beta_3 x_4\\)\n…\n\n\n\n\n\n\nWhich one do I choose ?\n\nputting everything together is not an option (kitchen sink regression)"
  },
  {
    "objectID": "session_4/slides.html#not-enough-coefficients",
    "href": "session_4/slides.html#not-enough-coefficients",
    "title": "Multiple Regressions",
    "section": "Not enough coefficients",
    "text": "Not enough coefficients\n\nSuppose you run a regression: \\[y = \\alpha + \\beta_1 x_1 + \\epsilon\\] and are genuinely interested in coefficient \\(\\beta_1\\)\nBut unknowingly to you, the actual model is \\[y = \\alpha + \\beta_1 x_1 + \\beta_2 x_2 + \\eta\\]\nThe residual \\(y - \\alpha - \\beta_1 x_1\\) is not white noise\n\nspecification hypotheses are violated\nestimate \\(\\hat{\\beta_1}\\) will have a bias (omitted variable bias)\nto correct the bias we add \\(x_2\\)\n\neven though we are not interested in \\(x_2\\) by itself\nwe control for \\(x_2\\))"
  },
  {
    "objectID": "session_4/slides.html#example",
    "href": "session_4/slides.html#example",
    "title": "Multiple Regressions",
    "section": "Example",
    "text": "Example\n\n\nSuppose I want to check Okun’s law. I consider the following model: \\[\\text{gdp_growth} = \\alpha + \\beta \\times \\text{unemployment}\\]\nI obtain: \\[\\text{gdp_growth} = 0.01 - 0.1 \\times \\text{unemployment} + e_i\\]\nThen I inspect visually the residuals: not normal at all!\nConclusion: my regression is misspecified, \\(0.1\\) is a biased (useless) estimate\nI need to control for additional variables. For instance: \\[\\text{gdp_growth} = \\alpha + \\beta_1 \\text{unemployment} + \\beta_2 \\text{interest rate}\\]\nUntil the residuals are actually white noise"
  },
  {
    "objectID": "session_4/slides.html#colinear-regressors",
    "href": "session_4/slides.html#colinear-regressors",
    "title": "Multiple Regressions",
    "section": "Colinear regressors",
    "text": "Colinear regressors\n\n\nWhat happens if two regressors are (almost) colinear? \\[y = \\alpha + \\beta_1 x_1 + \\beta_2 x_2\\] where \\(x_2 = \\kappa x_1\\)\nIntuitively: parameters are not unique\n\nif \\(y = \\alpha + \\beta_1 x_1\\) is the right model…\nthen \\(y = \\alpha + \\beta_1 \\lambda x_1 + \\beta_2 (1-\\lambda) \\frac{1}{\\kappa} x_2\\) is exactly as good…\n\nMathematically: \\((X'X)\\) is not invertible.\nWhen regressors are almost colinear, coefficients can have a lot of variability.\nTest:\n\ncorrelation statistics\ncorrelation plot"
  },
  {
    "objectID": "session_4/slides.html#choosing-regressors",
    "href": "session_4/slides.html#choosing-regressors",
    "title": "Multiple Regressions",
    "section": "Choosing regressors",
    "text": "Choosing regressors\n\\[y = \\alpha + \\beta_1 x_1 + ... \\beta_n x_n\\]\nWhich regressors to choose ?\n\nMethod 1 : remove coefficients with lowest t (less significant) to maximize adjusted R-squared\n\nremove regressors with lowest t\n\nnot the one you are interested in ;)\n\nregress again\nsee if adjusted \\(R^2\\) is decreasing\n\nif so continue\notherwise cancel last step and stop\n\n\nMethod 2 : choose combination to maximize Akaike Information Criterium\n\nAIC: \\(p - log(L)\\)\n\\(L\\) is likelihood\ncomputed by all good econometric softwares"
  },
  {
    "objectID": "session_4/index.html",
    "href": "session_4/index.html",
    "title": "Multiple Regressions",
    "section": "",
    "text": "type\nincome\neducation\nprestige\n\n\n\n\naccountant\nprof\n62\n86\n82\n\n\npilot\nprof\n72\n76\n83\n\n\narchitect\nprof\n75\n92\n90\n\n\nauthor\nprof\n55\n90\n76\n\n\nchemist\nprof\n64\n86\n90\n\n\n\n\n\nLast week we “ran” a linear regression: \\(y = \\alpha + \\beta x\\). Result: \\[\\text{income} = xx + 0.72 \\text{education}\\]\nShould we have looked at “prestige” instead ? \\[\\text{income} = xx + 0.83 \\text{prestige}\\]\nWhich one is better?\n\n\n\n\n\n\n\n\n\nif the goal is to predict: the one with higher explained variance\n\nprestige has higher \\(R^2\\) (\\(0.83^2\\))\n\nunless we are interested in the effect of education\n\n\n\n\n\n\nWhat about using both?\n\n2 variables model: \\[\\text{income} = \\alpha + \\beta_1 \\text{education} + \\beta_2 \\text{prestige}\\]\nwill probably improve prediction power (explained variance)\n\\(\\beta_1\\) might not be meaningful on its own anymore (education and prestige are correlated)\n\n\n\n\n\n\nNow we are trying to fit a plane to a cloud of points.\n \n\n\n\n\n\nTake all observations: \\((\\text{income}\\_n,\\text{education}\\_n,\\text{prestige}\\_n)\\_{n\\in[0,N]}\\)\nObjective: sum of squares \\[ L(\\alpha, \\beta_1, \\beta_2) = \\sum_i \\left( \\underbrace{ \\alpha + \\beta_1 \\text{education}\\_n + \\beta_2 \\text{prestige}\\_n - \\text{income}\\_n }\\_{e_n=\\text{prediction error} }\\right)^2 \\]\nMinimize loss function in \\(\\alpha\\), \\(\\beta_1\\), \\(\\beta_2\\)\nAgain, we can perform numerical optimization (machine learning approach)\n\n… but there is an explicit formula\n\n\n\n\n\n\n\n\n\n\\[Y = \\begin{bmatrix}\n\\text{income}_1 \\\\\\\\\n\\vdots \\\\\\\\\n\\text{income}_N\n\\end{bmatrix}\\] \\[X = \\begin{bmatrix}\n1 & \\text{education}_1 & \\text{prestige}_1 \\\\\\\\\n\\vdots & \\vdots & \\vdots \\\\\\\\\n1 &\\text{education}_N & \\text{prestige}_N\n\\end{bmatrix}\\]\n\n\nMatrix Version (look for \\(B = \\left( \\alpha, \\beta_1 , \\beta_2 \\right)\\)): \\[Y =  X B + E\\]\nNote that constant can be interpreted as a “variable”\nLoss function \\[L(A,B) = (Y - X B)' (Y - X B)\\]\nResult of minimization \\(\\min_{(A,B)} L(A,B)\\) : \\[\\begin{bmatrix}\\alpha & \\beta_1 & \\beta_2 \\end{bmatrix} = (X'X)^{-1} X' Y \\]\n\n\n\n\n\n\n\nResult: \\[\\text{income} = 10.43  + 0.03 \\times \\text{education} + 0.62 \\times \\text{prestige}\\]\nQuestions:\n\nis it a better regression than the other?\nis the coefficient in front of education significant?\nhow do we interpret it?\ncan we build confidence intervals?"
  },
  {
    "objectID": "session_4/index.html#remember-dataset-from-last-time",
    "href": "session_4/index.html#remember-dataset-from-last-time",
    "title": "Multiple Regressions",
    "section": "",
    "text": "type\nincome\neducation\nprestige\n\n\n\n\naccountant\nprof\n62\n86\n82\n\n\npilot\nprof\n72\n76\n83\n\n\narchitect\nprof\n75\n92\n90\n\n\nauthor\nprof\n55\n90\n76\n\n\nchemist\nprof\n64\n86\n90\n\n\n\n\n\nLast week we “ran” a linear regression: \\(y = \\alpha + \\beta x\\). Result: \\[\\text{income} = xx + 0.72 \\text{education}\\]\nShould we have looked at “prestige” instead ? \\[\\text{income} = xx + 0.83 \\text{prestige}\\]\nWhich one is better?"
  },
  {
    "objectID": "session_4/index.html#prestige-or-education",
    "href": "session_4/index.html#prestige-or-education",
    "title": "Multiple Regressions",
    "section": "",
    "text": "if the goal is to predict: the one with higher explained variance\n\nprestige has higher \\(R^2\\) (\\(0.83^2\\))\n\nunless we are interested in the effect of education"
  },
  {
    "objectID": "session_4/index.html#multiple-regression",
    "href": "session_4/index.html#multiple-regression",
    "title": "Multiple Regressions",
    "section": "",
    "text": "What about using both?\n\n2 variables model: \\[\\text{income} = \\alpha + \\beta_1 \\text{education} + \\beta_2 \\text{prestige}\\]\nwill probably improve prediction power (explained variance)\n\\(\\beta_1\\) might not be meaningful on its own anymore (education and prestige are correlated)"
  },
  {
    "objectID": "session_4/index.html#fitting-a-model",
    "href": "session_4/index.html#fitting-a-model",
    "title": "Multiple Regressions",
    "section": "",
    "text": "Now we are trying to fit a plane to a cloud of points."
  },
  {
    "objectID": "session_4/index.html#minimization-criterium",
    "href": "session_4/index.html#minimization-criterium",
    "title": "Multiple Regressions",
    "section": "",
    "text": "Take all observations: \\((\\text{income}\\_n,\\text{education}\\_n,\\text{prestige}\\_n)\\_{n\\in[0,N]}\\)\nObjective: sum of squares \\[ L(\\alpha, \\beta_1, \\beta_2) = \\sum_i \\left( \\underbrace{ \\alpha + \\beta_1 \\text{education}\\_n + \\beta_2 \\text{prestige}\\_n - \\text{income}\\_n }\\_{e_n=\\text{prediction error} }\\right)^2 \\]\nMinimize loss function in \\(\\alpha\\), \\(\\beta_1\\), \\(\\beta_2\\)\nAgain, we can perform numerical optimization (machine learning approach)\n\n… but there is an explicit formula"
  },
  {
    "objectID": "session_4/index.html#ordinary-least-square",
    "href": "session_4/index.html#ordinary-least-square",
    "title": "Multiple Regressions",
    "section": "",
    "text": "\\[Y = \\begin{bmatrix}\n\\text{income}_1 \\\\\\\\\n\\vdots \\\\\\\\\n\\text{income}_N\n\\end{bmatrix}\\] \\[X = \\begin{bmatrix}\n1 & \\text{education}_1 & \\text{prestige}_1 \\\\\\\\\n\\vdots & \\vdots & \\vdots \\\\\\\\\n1 &\\text{education}_N & \\text{prestige}_N\n\\end{bmatrix}\\]\n\n\nMatrix Version (look for \\(B = \\left( \\alpha, \\beta_1 , \\beta_2 \\right)\\)): \\[Y =  X B + E\\]\nNote that constant can be interpreted as a “variable”\nLoss function \\[L(A,B) = (Y - X B)' (Y - X B)\\]\nResult of minimization \\(\\min_{(A,B)} L(A,B)\\) : \\[\\begin{bmatrix}\\alpha & \\beta_1 & \\beta_2 \\end{bmatrix} = (X'X)^{-1} X' Y \\]"
  },
  {
    "objectID": "session_4/index.html#solution",
    "href": "session_4/index.html#solution",
    "title": "Multiple Regressions",
    "section": "",
    "text": "Result: \\[\\text{income} = 10.43  + 0.03 \\times \\text{education} + 0.62 \\times \\text{prestige}\\]\nQuestions:\n\nis it a better regression than the other?\nis the coefficient in front of education significant?\nhow do we interpret it?\ncan we build confidence intervals?"
  },
  {
    "objectID": "session_4/index.html#explained-variance-1",
    "href": "session_4/index.html#explained-variance-1",
    "title": "Multiple Regressions",
    "section": "Explained Variance",
    "text": "Explained Variance\n\nAs in the 1d case we can compare:\n\nthe variability of the model predictions (\\(MSS\\))\nthe variance of the data (\\(TSS\\), T for total)\n\nCoefficient of determination: \\[R^2 = \\frac{MSS}{TSS}\\]\nOr: \\[R^2 = 1-\\frac{RSS}{SST}\\] where \\(RSS\\) is the non explained variance"
  },
  {
    "objectID": "session_4/index.html#adjusted-r-squared",
    "href": "session_4/index.html#adjusted-r-squared",
    "title": "Multiple Regressions",
    "section": "Adjusted R squared",
    "text": "Adjusted R squared\n\n\n\nIn our example:\n\n\n\n\n\n\n\n\n\nRegression\n\\(R^2\\)\n \\(R^2_{adj}\\) \n\n\n\n\neducation\n0.525\n 0.514 \n\n\nprestige\n0.702\n 0.695 \n\n\neducation + prestige\n0.7022\n 0.688 \n\n\n\n\n\n\nFact:\n\nadding more regressors always improve \\(R^2\\)\nwhy not throw everything in? (kitchen sink regressions)\n\ntwo many regressors: overfitting the data\n\n\nPenalise additional regressors: adjusted R^2\n\nexample formula:\n\n\\(N\\): number of observations\n\\(p\\) number of variables \\[R^2_{adj} = 1-(1-R^2)\\frac{N-1}{N-p-1}\\]"
  },
  {
    "objectID": "session_4/index.html#making-a-regression-with-statsmodels",
    "href": "session_4/index.html#making-a-regression-with-statsmodels",
    "title": "Multiple Regressions",
    "section": "Making a regression with statsmodels",
    "text": "Making a regression with statsmodels\nimport statsmodels\nWe use a special API inspired by R:\nimport statsmodels.formula.api as smf"
  },
  {
    "objectID": "session_4/index.html#performing-a-regression",
    "href": "session_4/index.html#performing-a-regression",
    "title": "Multiple Regressions",
    "section": "Performing a regression",
    "text": "Performing a regression\n\nRunning a regression with statsmodels\n\nmodel = smf.ols('income ~ education',  df)  # model\nres = model.fit()  # perform the regression\nres.describe()\n\n‘income ~ education’ is the model formula\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 income   R-squared:                       0.525\nModel:                            OLS   Adj. R-squared:                  0.514\nMethod:                 Least Squares   F-statistic:                     47.51\nDate:                Tue, 02 Feb 2021   Prob (F-statistic):           1.84e-08\nTime:                        05:21:25   Log-Likelihood:                -190.42\nNo. Observations:                  45   AIC:                             384.8\nDf Residuals:                      43   BIC:                             388.5\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n==============================================================================\nIntercept     10.6035      5.198      2.040      0.048       0.120      21.087\neducation      0.5949      0.086      6.893      0.000       0.421       0.769\n==============================================================================\nOmnibus:                        9.841   Durbin-Watson:                   1.736\nProb(Omnibus):                  0.007   Jarque-Bera (JB):               10.609\nSkew:                           0.776   Prob(JB):                      0.00497\nKurtosis:                       4.802   Cond. No.                         123.\n=============================================================================="
  },
  {
    "objectID": "session_4/index.html#formula-mini-language",
    "href": "session_4/index.html#formula-mini-language",
    "title": "Multiple Regressions",
    "section": "Formula mini-language",
    "text": "Formula mini-language\n\nWith statsmodels formulas, can be supplied with R-style syntax\nExamples:\n\n\n\n\n\n\n\n\nFormula\nModel\n\n\n\n\nincome ~ education\n\\(\\text{income}_i = \\alpha + \\beta \\text{education}_i\\)\n\n\nincome ~ prestige\n\\(\\text{income}_i = \\alpha + \\beta \\text{prestige}_i\\)\n\n\nincome ~ prestige - 1\n\\(\\text{income}_i = \\beta \\text{prestige}_i\\) (no intercept)\n\n\nincome ~ education + prestige\n\\(\\text{income}_i = \\alpha + \\beta_1 \\text{education}_i + \\beta_2 \\text{prestige}_i\\)"
  },
  {
    "objectID": "session_4/index.html#formula-mini-language-1",
    "href": "session_4/index.html#formula-mini-language-1",
    "title": "Multiple Regressions",
    "section": "Formula mini-language",
    "text": "Formula mini-language\n\nOne can use formulas to apply transformations to variables\n\n\n\n\n\n\n\n\nFormula\nModel\n\n\n\n\nlog(P) ~ log(M) + log(Y)\n\\(\\log(P_i) = \\alpha + \\alpha_1 \\log(M_i) + \\alpha_2 \\log(Y_i)\\) (log-log)\n\n\nlog(Y) ~ i\n\\(\\log(P_i) = \\alpha + i_i\\) (semi-logs)\n\n\n\n\nThis is useful if the true relationship is nonlinear\nAlso useful, to interpret the coefficients"
  },
  {
    "objectID": "session_4/index.html#coefficients-interpetation",
    "href": "session_4/index.html#coefficients-interpetation",
    "title": "Multiple Regressions",
    "section": "Coefficients interpetation",
    "text": "Coefficients interpetation\n\nExample:\n\n(police_spending and prevention_policies in million dollars) \\[\\text{number_or_crimes} = 0.005\\% - 0.001 \\text{pol_spend} - 0.005 \\text{prev_pol} + 0.002 \\text{population density}\\]\n\nreads: when holding other variables constant a 0.1 million increase in police spending reduces crime rate by 0.001%\ninterpretation?\n\nproblematic because variables have different units\nwe can say that prevention policies are more efficient than police spending ceteris paribus\n\nTake logs: \\[\\log(\\text{number_or_crimes}) = 0.005\\% - 0.15 \\log(\\text{pol_spend}) - 0.4 \\log(\\text{prev_pol}) + 0.2 \\log(\\text{population density})\\]\n\nnow we have an estimate of elasticities\na \\(1\\%\\) increase in police spending leads to a \\(0.15\\%\\) decrease in the number of crimes"
  },
  {
    "objectID": "session_4/index.html#hypotheses",
    "href": "session_4/index.html#hypotheses",
    "title": "Multiple Regressions",
    "section": "Hypotheses",
    "text": "Hypotheses\n\n\n\n\nRecall what we do:\n\nwe have the data \\(X,Y\\)\nwe choose a model: \\[ Y = \\alpha + X \\beta \\]\nfrom the data we compute estimates: \\[\\hat{\\beta}  = (X'X)^{-1} X' Y \\] \\[\\hat{\\alpha} = Y- X \\beta \\]\nestimates are a precise function of data\n\nexact formula not important here\n\n\n\n\n\n\nWe make some hypotheses on the data generation process:\n\n\\(Y = X \\beta + \\epsilon\\)\n\\(\\mathbb{E}\\left[ \\epsilon \\right] = 0\\)\n\\(\\epsilon\\) multivariate normal with covariance matrix \\(\\sigma^2 I_n\\)\n\n\\(\\forall i, \\sigma(\\epsilon_i) = \\sigma\\)\n\\(\\forall i,j, cov(\\epsilon_i, \\epsilon_j) = 0\\)\n\n\n\nUnder these hypotheses:\n\n\\(\\hat{\\beta}\\) is an unbiased estimate of true parameter \\(\\beta\\)\n\ni.e. \\(\\mathbb{E} [\\hat{\\beta}] = \\beta\\)\n\none can prove \\(Var(\\hat{\\beta}) = \\sigma^2 I_n\\)\n\\(\\sigma\\) can be estimated by \\(\\hat{\\sigma}=S\\frac{\\sum_i (y_i-{pred}_i)^2}{N-p}\\)\n\n\\(N-p\\): degrees of freedoms\n\none can estimate: \\(\\sigma(\\hat{\\beta_k})\\)\n\nit is the \\(i\\)-th diagonal element of \\(\\hat{\\sigma}^2 X'X\\)"
  },
  {
    "objectID": "session_4/index.html#is-the-regression-significant",
    "href": "session_4/index.html#is-the-regression-significant",
    "title": "Multiple Regressions",
    "section": "Is the regression significant?",
    "text": "Is the regression significant?\n\n\n\nApproach is very similar to the one-dimensional case\nFisher criterium (F-test):\n\n\\(H0\\): all coeficients are 0\n\ni.e. true model is \\(y=\\alpha + \\epsilon\\)\n\n\\(H1\\): some coefficients are not 0\n\nStatistics: \\[F=\\frac{MSR}{MSE}\\]\n\n\\(MSR\\): mean-squared error of constant model\n\\(MSE\\): mean-squared error of full model\n\n\n\n\n\nUnder:\n\nthe model assumptions about the data generation process\nthe H0 hypothesis\n\n… the distribution of \\(F\\) is known\nIt is remarkable that it doesn’t depend on \\(\\sigma\\) !\n\nOne can produce a p-value.\n\nprobability to obtain this statistics given hypothesis H0\nif very low, H0 is rejected"
  },
  {
    "objectID": "session_4/index.html#is-each-coefficient-significant",
    "href": "session_4/index.html#is-each-coefficient-significant",
    "title": "Multiple Regressions",
    "section": "Is each coefficient significant ?",
    "text": "Is each coefficient significant ?\n\nStudent test. Given a coefficient \\(\\beta_k\\):\n\n\\(H0\\): coefficient is 0\n\\(H1\\): coefficient is not zero\n\nStatistics: \\(t = \\frac{\\hat{\\beta_k}}{\\hat{\\sigma}(\\hat{\\beta_k})}\\)\n\nwhere \\(\\hat{\\sigma}(\\beta_k)\\) is \\(i\\)-th diagonal element of \\(\\hat{\\sigma}^2 X'X\\)\nit compares the estimated value of a coefficient to its estimated standard deviation\n\nUnder the inference hypotheses, distribution of \\(t\\) is known.\n\nit is a student distribution\n\nProcedure:\n\nCompute \\(t\\). Check acceptance threshold \\(t*\\) at probability \\(\\alpha\\) (ex 5%)\nCoefficient is significant with probability \\(1-\\alpha\\) if \\(t&gt;t*\\)\nOr just look at the \\(p-value\\): probability that \\(t\\) would be as high as it is, assuming \\(H0\\)"
  },
  {
    "objectID": "session_4/index.html#confidence-intervals",
    "href": "session_4/index.html#confidence-intervals",
    "title": "Multiple Regressions",
    "section": "Confidence intervals",
    "text": "Confidence intervals\n\nSame as in the 1d case.\nTake estimate \\(\\color{red}{\\beta_i}\\) with an estimate of its standard deviation \\(\\color{red}{\\hat{\\sigma}(\\beta_i)}\\)\nCompute student \\(\\color{red}{t^{\\star}}\\) at \\(\\color{red}{\\alpha}\\) confidence level (ex: \\(\\alpha=5\\\\%\\)) such that:\n\n\\(P(|t|&gt;t^{\\star})&lt;\\alpha\\)\n\nProduce confidence intervals at \\(\\alpha\\) confidence level:\n\n\\([\\color{red}{\\beta_i} - t^{\\star} \\color{red}{\\hat{\\sigma}(\\beta_i)}, \\color{red}{\\beta_i} + t^{\\star} \\color{red}{\\hat{\\sigma}(\\beta_i)}]\\)\n\nInterpretation:\n\nfor a given confidence interval at confidence level \\(\\alpha\\)…\nthe probability that our coefficient was obtained, if the true coefficient were outside of it, is smaller than \\(\\alpha\\)"
  },
  {
    "objectID": "session_4/index.html#other-tests",
    "href": "session_4/index.html#other-tests",
    "title": "Multiple Regressions",
    "section": "Other tests",
    "text": "Other tests\n\nThe tests seen so far rely on strong statistical assumptions (normality, homoscedasticity, etc..)\nSome tests can be used to test these assumptions:\n\nJarque-Bera: is the distribution of data truly normal\nDurbin-Watson: are residuals autocorrelated (makes sense for time-series)\n…\n\nIn case assumptions are not met…\n\n… still possible to do econometrics\n… but beyond the scope of this course"
  },
  {
    "objectID": "session_4/index.html#variable-selection-1",
    "href": "session_4/index.html#variable-selection-1",
    "title": "Multiple Regressions",
    "section": "Variable selection",
    "text": "Variable selection\n\n\n\nI’ve got plenty of data:\n\n\\(y\\): gdp\n\\(x_1\\): investment\n\\(x_2\\): inflation\n\\(x_3\\): education\n\\(x_4\\): unemployment\n…\n\n\n\n\nMany possible regressions:\n\n\\(y = α + \\beta_1 x_1\\)\n\\(y = α + \\beta_2 x_2 + \\beta_3 x_4\\)\n…\n\n\n\n\n. . .\n\nWhich one do I choose ?\n\nputting everything together is not an option (kitchen sink regression)"
  },
  {
    "objectID": "session_4/index.html#not-enough-coefficients",
    "href": "session_4/index.html#not-enough-coefficients",
    "title": "Multiple Regressions",
    "section": "Not enough coefficients",
    "text": "Not enough coefficients\n\nSuppose you run a regression: \\[y = \\alpha + \\beta_1 x_1 + \\epsilon\\] and are genuinely interested in coefficient \\(\\beta_1\\)\nBut unknowingly to you, the actual model is \\[y = \\alpha + \\beta_1 x_1 + \\beta_2 x_2 + \\eta\\]\nThe residual \\(y - \\alpha - \\beta_1 x_1\\) is not white noise\n\nspecification hypotheses are violated\nestimate \\(\\hat{\\beta_1}\\) will have a bias (omitted variable bias)\nto correct the bias we add \\(x_2\\)\n\neven though we are not interested in \\(x_2\\) by itself\nwe control for \\(x_2\\))"
  },
  {
    "objectID": "session_4/index.html#example",
    "href": "session_4/index.html#example",
    "title": "Multiple Regressions",
    "section": "Example",
    "text": "Example\n\n\nSuppose I want to check Okun’s law. I consider the following model: \\[\\text{gdp_growth} = \\alpha + \\beta \\times \\text{unemployment}\\]\nI obtain: \\[\\text{gdp_growth} = 0.01 - 0.1 \\times \\text{unemployment} + e_i\\]\nThen I inspect visually the residuals: not normal at all!\nConclusion: my regression is misspecified, \\(0.1\\) is a biased (useless) estimate\nI need to control for additional variables. For instance: \\[\\text{gdp_growth} = \\alpha + \\beta_1 \\text{unemployment} + \\beta_2 \\text{interest rate}\\]\nUntil the residuals are actually white noise"
  },
  {
    "objectID": "session_4/index.html#colinear-regressors",
    "href": "session_4/index.html#colinear-regressors",
    "title": "Multiple Regressions",
    "section": "Colinear regressors",
    "text": "Colinear regressors\n\n\nWhat happens if two regressors are (almost) colinear? \\[y = \\alpha + \\beta_1 x_1 + \\beta_2 x_2\\] where \\(x_2 = \\kappa x_1\\)\nIntuitively: parameters are not unique\n\nif \\(y = \\alpha + \\beta_1 x_1\\) is the right model…\nthen \\(y = \\alpha + \\beta_1 \\lambda x_1 + \\beta_2 (1-\\lambda) \\frac{1}{\\kappa} x_2\\) is exactly as good…\n\nMathematically: \\((X'X)\\) is not invertible.\nWhen regressors are almost colinear, coefficients can have a lot of variability.\nTest:\n\ncorrelation statistics\ncorrelation plot"
  },
  {
    "objectID": "session_4/index.html#choosing-regressors",
    "href": "session_4/index.html#choosing-regressors",
    "title": "Multiple Regressions",
    "section": "Choosing regressors",
    "text": "Choosing regressors\n\\[y = \\alpha + \\beta_1 x_1 + ... \\beta_n x_n\\]\nWhich regressors to choose ?\n\nMethod 1 : remove coefficients with lowest t (less significant) to maximize adjusted R-squared\n\nremove regressors with lowest t\n\nnot the one you are interested in ;)\n\nregress again\nsee if adjusted \\(R^2\\) is decreasing\n\nif so continue\notherwise cancel last step and stop\n\n\nMethod 2 : choose combination to maximize Akaike Information Criterium\n\nAIC: \\(p - log(L)\\)\n\\(L\\) is likelihood\ncomputed by all good econometric softwares"
  },
  {
    "objectID": "session_4/Regressions.html",
    "href": "session_4/Regressions.html",
    "title": "Regressions",
    "section": "",
    "text": "Import the Duncan/carData dataset\n\nimport statsmodels.api as sm\ndataset = sm.datasets.get_rdataset(\"Duncan\", \"carData\")\ndf = dataset.data\ndf.head()\n\n\n\n\n\n\n\n\ntype\nincome\neducation\nprestige\n\n\n\n\naccountant\nprof\n62\n86\n82\n\n\npilot\nprof\n72\n76\n83\n\n\narchitect\nprof\n75\n92\n90\n\n\nauthor\nprof\n55\n90\n76\n\n\nchemist\nprof\n64\n86\n90\n\n\n\n\n\n\n\nEstimate by hand the model \\(\\text{income} = \\alpha + \\beta \\times \\text{education}\\) . Plot.\nCompute total, explained, unexplained variance. Compute R^2 statistics\nUse statsmodels (formula API) to estimate \\(\\text{income} = \\alpha + \\beta \\times \\text{education}\\). Comment regression statistics.\n\n#https://www.statsmodels.org/stable/generated/statsmodels.formula.api.ols.html\n\nfrom statsmodels.formula import api as smf\n\nmodel_1 = smf.ols(\"income ~ education\", df)\nres_1 = model_1.fit()\n\n&lt;statsmodels.regression.linear_model.RegressionResultsWrapper at 0x7ffad5b135e0&gt;\n\n\n\nres_1.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nincome\nR-squared:\n0.525\n\n\nModel:\nOLS\nAdj. R-squared:\n0.514\n\n\nMethod:\nLeast Squares\nF-statistic:\n47.51\n\n\nDate:\nTue, 02 Feb 2021\nProb (F-statistic):\n1.84e-08\n\n\nTime:\n11:29:50\nLog-Likelihood:\n-190.42\n\n\nNo. Observations:\n45\nAIC:\n384.8\n\n\nDf Residuals:\n43\nBIC:\n388.5\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n10.6035\n5.198\n2.040\n0.048\n0.120\n21.087\n\n\neducation\n0.5949\n0.086\n6.893\n0.000\n0.421\n0.769\n\n\n\n\n\n\nOmnibus:\n9.841\nDurbin-Watson:\n1.736\n\n\nProb(Omnibus):\n0.007\nJarque-Bera (JB):\n10.609\n\n\nSkew:\n0.776\nProb(JB):\n0.00497\n\n\nKurtosis:\n4.802\nCond. No.\n123.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe estimated regresssion is income = 10.6 + 0.59 education. At a 5% p-value level both the intercept and the coefficient are significant. R-squared is 0.52: the model explains half of the variance.\nUse statsmodels to estimate \\(\\text{income} = \\alpha + \\beta \\times \\text{prestige}\\). Comment regression statistics.\n\n# \n\n__Use statsmodels to estimate $ = + + _2 $. Comment regression statistics.__\nWHich model would you recommend? For which purpose?\nPlot the regression with prestige\nCheck visually normality of residuals"
  },
  {
    "objectID": "session_4/Regressions.html#linear-regressions",
    "href": "session_4/Regressions.html#linear-regressions",
    "title": "Regressions",
    "section": "",
    "text": "Import the Duncan/carData dataset\n\nimport statsmodels.api as sm\ndataset = sm.datasets.get_rdataset(\"Duncan\", \"carData\")\ndf = dataset.data\ndf.head()\n\n\n\n\n\n\n\n\ntype\nincome\neducation\nprestige\n\n\n\n\naccountant\nprof\n62\n86\n82\n\n\npilot\nprof\n72\n76\n83\n\n\narchitect\nprof\n75\n92\n90\n\n\nauthor\nprof\n55\n90\n76\n\n\nchemist\nprof\n64\n86\n90\n\n\n\n\n\n\n\nEstimate by hand the model \\(\\text{income} = \\alpha + \\beta \\times \\text{education}\\) . Plot.\nCompute total, explained, unexplained variance. Compute R^2 statistics\nUse statsmodels (formula API) to estimate \\(\\text{income} = \\alpha + \\beta \\times \\text{education}\\). Comment regression statistics.\n\n#https://www.statsmodels.org/stable/generated/statsmodels.formula.api.ols.html\n\nfrom statsmodels.formula import api as smf\n\nmodel_1 = smf.ols(\"income ~ education\", df)\nres_1 = model_1.fit()\n\n&lt;statsmodels.regression.linear_model.RegressionResultsWrapper at 0x7ffad5b135e0&gt;\n\n\n\nres_1.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nincome\nR-squared:\n0.525\n\n\nModel:\nOLS\nAdj. R-squared:\n0.514\n\n\nMethod:\nLeast Squares\nF-statistic:\n47.51\n\n\nDate:\nTue, 02 Feb 2021\nProb (F-statistic):\n1.84e-08\n\n\nTime:\n11:29:50\nLog-Likelihood:\n-190.42\n\n\nNo. Observations:\n45\nAIC:\n384.8\n\n\nDf Residuals:\n43\nBIC:\n388.5\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n10.6035\n5.198\n2.040\n0.048\n0.120\n21.087\n\n\neducation\n0.5949\n0.086\n6.893\n0.000\n0.421\n0.769\n\n\n\n\n\n\nOmnibus:\n9.841\nDurbin-Watson:\n1.736\n\n\nProb(Omnibus):\n0.007\nJarque-Bera (JB):\n10.609\n\n\nSkew:\n0.776\nProb(JB):\n0.00497\n\n\nKurtosis:\n4.802\nCond. No.\n123.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe estimated regresssion is income = 10.6 + 0.59 education. At a 5% p-value level both the intercept and the coefficient are significant. R-squared is 0.52: the model explains half of the variance.\nUse statsmodels to estimate \\(\\text{income} = \\alpha + \\beta \\times \\text{prestige}\\). Comment regression statistics.\n\n# \n\n__Use statsmodels to estimate $ = + + _2 $. Comment regression statistics.__\nWHich model would you recommend? For which purpose?\nPlot the regression with prestige\nCheck visually normality of residuals"
  },
  {
    "objectID": "session_4/Regressions.html#finding-the-right-model",
    "href": "session_4/Regressions.html#finding-the-right-model",
    "title": "Regressions",
    "section": "Finding the right model",
    "text": "Finding the right model\nImport dataset from data.dta. Explore dataset (statistics, plots)\nOur goal is to explain z by x and y. Run a regression.\nExamine the residuals of the regression. What’s wrong? Remedy?"
  },
  {
    "objectID": "session_4/Regressions.html#taylor-rule",
    "href": "session_4/Regressions.html#taylor-rule",
    "title": "Regressions",
    "section": "Taylor Rule",
    "text": "Taylor Rule\nIn 1993, John taylor, estimated, using US data the regression: \\(i_t = i^{\\star} + \\alpha_{\\pi} \\pi_t + \\alpha_{\\pi} y_t\\) where \\(\\pi_t\\) is inflation and \\(y_t\\) the output gap (let’s say deviation from real gdp from the trend). He found that both coefficients were not significantly different from \\(0.5\\). Our goal, is to replicate the same analysis.\n\nImporting the Data\nImport macrodata dataset from statsmodels (https://www.statsmodels.org/devel/datasets/generated/macrodata.html). Describe briefly its content using the metadata.\n\nimport statsmodels.api as sm\ndataset = sm.datasets.macrodata.load_pandas()\n\n\n# the dataset object contains some data on the dataset: explore them (dataset.+Tab)\n\nExtract the dataframe from the dataset object. Print first lines and summary statistics.\n\ndf = dataset.data\ndf\n\n\n\n\n\n\n\n\nyear\nquarter\nrealgdp\nrealcons\nrealinv\nrealgovt\nrealdpi\ncpi\nm1\ntbilrate\nunemp\npop\ninfl\nrealint\n\n\n\n\n0\n1959.0\n1.0\n2710.349\n1707.4\n286.898\n470.045\n1886.9\n28.980\n139.7\n2.82\n5.8\n177.146\n0.00\n0.00\n\n\n1\n1959.0\n2.0\n2778.801\n1733.7\n310.859\n481.301\n1919.7\n29.150\n141.7\n3.08\n5.1\n177.830\n2.34\n0.74\n\n\n2\n1959.0\n3.0\n2775.488\n1751.8\n289.226\n491.260\n1916.4\n29.350\n140.5\n3.82\n5.3\n178.657\n2.74\n1.09\n\n\n3\n1959.0\n4.0\n2785.204\n1753.7\n299.356\n484.052\n1931.3\n29.370\n140.0\n4.33\n5.6\n179.386\n0.27\n4.06\n\n\n4\n1960.0\n1.0\n2847.699\n1770.5\n331.722\n462.199\n1955.5\n29.540\n139.6\n3.50\n5.2\n180.007\n2.31\n1.19\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n198\n2008.0\n3.0\n13324.600\n9267.7\n1990.693\n991.551\n9838.3\n216.889\n1474.7\n1.17\n6.0\n305.270\n-3.16\n4.33\n\n\n199\n2008.0\n4.0\n13141.920\n9195.3\n1857.661\n1007.273\n9920.4\n212.174\n1576.5\n0.12\n6.9\n305.952\n-8.79\n8.91\n\n\n200\n2009.0\n1.0\n12925.410\n9209.2\n1558.494\n996.287\n9926.4\n212.671\n1592.8\n0.22\n8.1\n306.547\n0.94\n-0.71\n\n\n201\n2009.0\n2.0\n12901.504\n9189.0\n1456.678\n1023.528\n10077.5\n214.469\n1653.6\n0.18\n9.2\n307.226\n3.37\n-3.19\n\n\n202\n2009.0\n3.0\n12990.341\n9256.0\n1486.398\n1044.088\n10040.6\n216.385\n1673.9\n0.12\n9.6\n308.013\n3.56\n-3.44\n\n\n\n\n203 rows × 14 columns\n\n\n\n\ndf.head()\n\n\n\n\n\n\n\n\nyear\nquarter\nrealgdp\nrealcons\nrealinv\nrealgovt\nrealdpi\ncpi\nm1\ntbilrate\nunemp\npop\ninfl\nrealint\n\n\n\n\n0\n1959.0\n1.0\n2710.349\n1707.4\n286.898\n470.045\n1886.9\n28.98\n139.7\n2.82\n5.8\n177.146\n0.00\n0.00\n\n\n1\n1959.0\n2.0\n2778.801\n1733.7\n310.859\n481.301\n1919.7\n29.15\n141.7\n3.08\n5.1\n177.830\n2.34\n0.74\n\n\n2\n1959.0\n3.0\n2775.488\n1751.8\n289.226\n491.260\n1916.4\n29.35\n140.5\n3.82\n5.3\n178.657\n2.74\n1.09\n\n\n3\n1959.0\n4.0\n2785.204\n1753.7\n299.356\n484.052\n1931.3\n29.37\n140.0\n4.33\n5.6\n179.386\n0.27\n4.06\n\n\n4\n1960.0\n1.0\n2847.699\n1770.5\n331.722\n462.199\n1955.5\n29.54\n139.6\n3.50\n5.2\n180.007\n2.31\n1.19\n\n\n\n\n\n\n\n\n\nPreparing the Data\nCompute inflation as the growth and store it in the dataframe as variable π.\nAdd nominal interest rate to the database (use the Fisher relation).\nDetrend GDP using Hodrick-Prescott filter. If needed, Check wikipedia and the documentation. The result is a trend tau and a residual epsilon. Store log(tau/residual) as y"
  },
  {
    "objectID": "session_4/Regressions.html#run-the-regression",
    "href": "session_4/Regressions.html#run-the-regression",
    "title": "Regressions",
    "section": "Run the regression",
    "text": "Run the regression\nRun the basic regression. Interpret the results.\nWhich control variables would you propose to add? Does it increase prediction power? How do you interpret that?"
  },
  {
    "objectID": "session_4/Regressions_correction_2022.html",
    "href": "session_4/Regressions_correction_2022.html",
    "title": "Regressions",
    "section": "",
    "text": "Import the Duncan/carData dataset\n\nimport statsmodels.api as sm\ndataset = sm.datasets.get_rdataset(\"Duncan\", \"carData\")\ndf = dataset.data\ndf.head()\n\n\n\n\n\n\n\n\ntype\nincome\neducation\nprestige\n\n\n\n\naccountant\nprof\n62\n86\n82\n\n\npilot\nprof\n72\n76\n83\n\n\narchitect\nprof\n75\n92\n90\n\n\nauthor\nprof\n55\n90\n76\n\n\nchemist\nprof\n64\n86\n90\n\n\n\n\n\n\n\nEstimate by hand the model \\(\\text{income} = \\alpha + \\beta \\times \\text{education}\\) . Plot.\n\n# skipped. requires the formula from the course\n\nCompute total, explained, unexplained variance. Compute R^2 statistics\n\n# skipped. requires the formula from the course\n\nUse statsmodels (formula API) to estimate \\(\\text{income} = \\alpha + \\beta \\times \\text{education}\\). Comment regression statistics.\n\n#https://www.statsmodels.org/stable/generated/statsmodels.formula.api.ols.html\n\nfrom statsmodels.formula import api as smf\n\nmodel_1 = smf.ols(\"income ~ education\", df)\nres_1 = model_1.fit()\n\n&lt;statsmodels.regression.linear_model.RegressionResultsWrapper at 0x7ffad5b135e0&gt;\n\n\n\nres_1.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nincome\nR-squared:\n0.525\n\n\nModel:\nOLS\nAdj. R-squared:\n0.514\n\n\nMethod:\nLeast Squares\nF-statistic:\n47.51\n\n\nDate:\nTue, 02 Feb 2021\nProb (F-statistic):\n1.84e-08\n\n\nTime:\n11:29:50\nLog-Likelihood:\n-190.42\n\n\nNo. Observations:\n45\nAIC:\n384.8\n\n\nDf Residuals:\n43\nBIC:\n388.5\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n10.6035\n5.198\n2.040\n0.048\n0.120\n21.087\n\n\neducation\n0.5949\n0.086\n6.893\n0.000\n0.421\n0.769\n\n\n\n\n\n\nOmnibus:\n9.841\nDurbin-Watson:\n1.736\n\n\nProb(Omnibus):\n0.007\nJarque-Bera (JB):\n10.609\n\n\nSkew:\n0.776\nProb(JB):\n0.00497\n\n\nKurtosis:\n4.802\nCond. No.\n123.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe estimated regresssion is income = 10.6 + 0.59 education. At a 5% p-value level both the intercept and the coefficient are significant. R-squared is 0.52: the model explains half of the variance.\nUse statsmodels to estimate \\(\\text{income} = \\alpha + \\beta \\times \\text{prestige}\\). Comment regression statistics.\n\nformula = \"income ~ education\"\n\n\ndf.head()\n\n\n\n\n\n\n\n\ntype\nincome\neducation\nprestige\n\n\n\n\naccountant\nprof\n62\n86\n82\n\n\npilot\nprof\n72\n76\n83\n\n\narchitect\nprof\n75\n92\n90\n\n\nauthor\nprof\n55\n90\n76\n\n\nchemist\nprof\n64\n86\n90\n\n\n\n\n\n\n\n\nmodel_2 = smf.ols(\"income ~ prestige\", df)\nres_2 = model_2.fit()\n\n\nres_2.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nincome\nR-squared:\n0.702\n\n\nModel:\nOLS\nAdj. R-squared:\n0.695\n\n\nMethod:\nLeast Squares\nF-statistic:\n101.3\n\n\nDate:\nTue, 02 Feb 2021\nProb (F-statistic):\n7.14e-13\n\n\nTime:\n11:55:59\nLog-Likelihood:\n-179.93\n\n\nNo. Observations:\n45\nAIC:\n363.9\n\n\nDf Residuals:\n43\nBIC:\n367.5\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n10.8840\n3.678\n2.959\n0.005\n3.467\n18.301\n\n\nprestige\n0.6497\n0.065\n10.062\n0.000\n0.519\n0.780\n\n\n\n\n\n\nOmnibus:\n8.893\nDurbin-Watson:\n2.048\n\n\nProb(Omnibus):\n0.012\nJarque-Bera (JB):\n19.848\n\n\nSkew:\n0.047\nProb(JB):\n4.90e-05\n\n\nKurtosis:\n6.252\nCond. No.\n104.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe estimated regresssion is income = 10.6 + 0.59 education. At a 0.5% p-value level both the intercept and the coefficient are significant. R-squared is 0.70: the model predicts income better than the former one.\nUse statsmodels to estimate \\(\\text{income} = \\alpha + \\beta \\times \\text{education} + \\beta_2 \\times \\text{prestige}\\). Comment regression statistics.\n\nmodel_3 = smf.ols(\"income ~ education + prestige\", df)\nres_3 = model_3.fit()\n\n\nres_3.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nincome\nR-squared:\n0.702\n\n\nModel:\nOLS\nAdj. R-squared:\n0.688\n\n\nMethod:\nLeast Squares\nF-statistic:\n49.55\n\n\nDate:\nTue, 02 Feb 2021\nProb (F-statistic):\n8.88e-12\n\n\nTime:\n11:56:03\nLog-Likelihood:\n-179.90\n\n\nNo. Observations:\n45\nAIC:\n365.8\n\n\nDf Residuals:\n42\nBIC:\n371.2\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n10.4264\n4.164\n2.504\n0.016\n2.024\n18.829\n\n\neducation\n0.0323\n0.132\n0.244\n0.808\n-0.234\n0.299\n\n\nprestige\n0.6237\n0.125\n5.003\n0.000\n0.372\n0.875\n\n\n\n\n\n\nOmnibus:\n9.200\nDurbin-Watson:\n2.053\n\n\nProb(Omnibus):\n0.010\nJarque-Bera (JB):\n21.265\n\n\nSkew:\n0.075\nProb(JB):\n2.41e-05\n\n\nKurtosis:\n6.364\nCond. No.\n168.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nThe \\(R^2\\) is only slightly higher than last model, but adjusted \\(R^2\\) is actually lower: the model has less predictive power.\nThe coefficient for education is not significant. It should be dropped from the regresssion.\nThis might happen, because education and prestige are correlated. Let’s check it:\n\ndf.corr()\n\n\n\n\n\n\n\n\nincome\neducation\nprestige\n\n\n\n\nincome\n1.000000\n0.724512\n0.837801\n\n\neducation\n0.724512\n1.000000\n0.851916\n\n\nprestige\n0.837801\n0.851916\n1.000000\n\n\n\n\n\n\n\nEducation and prestige are correlated at 83%. It makes no sense keeping the two in the same regression.\nWHich model would you recommend? For which purpose?\nIf the goal is to predict income, the one with prestige only, has the highest prediction power. If we are interested in the effect of education, we keep only education.\nPlot the regression with prestige\n\na = res_2.params.Intercept\nb = res_2.params.prestige\n\n\nx = df['prestige']\n\n\ny = a + b*x\n\n\nfrom matplotlib import pyplot as plt\nplt.plot(x, df['income'],'o')\nplt.plot(x, y)\nplt.xlabel(\"prestige\")\nplt.xlabel(\"income\")\n\nText(0.5, 0, 'income')\n\n\n\n\n\nCheck visually normality of residuals\n\npred = a + b*x\nactual = df['income']\nresid = actual - pred  # same as res_2.resid\n\n\nplt.plot(x,resid, 'o')\n\n\n\n\n\nplt.hist(resid)\n\n(array([0.00255915, 0.        , 0.        , 0.00511829, 0.0486238 ,\n        0.02815062, 0.01535488, 0.01023659, 0.00255915, 0.00255915]),\n array([-46.40643935, -37.72299114, -29.03954294, -20.35609473,\n        -11.67264653,  -2.98919832,   5.69424989,  14.37769809,\n         23.0611463 ,  31.74459451,  40.42804271]),\n &lt;BarContainer object of 10 artists&gt;)"
  },
  {
    "objectID": "session_4/Regressions_correction_2022.html#linear-regressions",
    "href": "session_4/Regressions_correction_2022.html#linear-regressions",
    "title": "Regressions",
    "section": "",
    "text": "Import the Duncan/carData dataset\n\nimport statsmodels.api as sm\ndataset = sm.datasets.get_rdataset(\"Duncan\", \"carData\")\ndf = dataset.data\ndf.head()\n\n\n\n\n\n\n\n\ntype\nincome\neducation\nprestige\n\n\n\n\naccountant\nprof\n62\n86\n82\n\n\npilot\nprof\n72\n76\n83\n\n\narchitect\nprof\n75\n92\n90\n\n\nauthor\nprof\n55\n90\n76\n\n\nchemist\nprof\n64\n86\n90\n\n\n\n\n\n\n\nEstimate by hand the model \\(\\text{income} = \\alpha + \\beta \\times \\text{education}\\) . Plot.\n\n# skipped. requires the formula from the course\n\nCompute total, explained, unexplained variance. Compute R^2 statistics\n\n# skipped. requires the formula from the course\n\nUse statsmodels (formula API) to estimate \\(\\text{income} = \\alpha + \\beta \\times \\text{education}\\). Comment regression statistics.\n\n#https://www.statsmodels.org/stable/generated/statsmodels.formula.api.ols.html\n\nfrom statsmodels.formula import api as smf\n\nmodel_1 = smf.ols(\"income ~ education\", df)\nres_1 = model_1.fit()\n\n&lt;statsmodels.regression.linear_model.RegressionResultsWrapper at 0x7ffad5b135e0&gt;\n\n\n\nres_1.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nincome\nR-squared:\n0.525\n\n\nModel:\nOLS\nAdj. R-squared:\n0.514\n\n\nMethod:\nLeast Squares\nF-statistic:\n47.51\n\n\nDate:\nTue, 02 Feb 2021\nProb (F-statistic):\n1.84e-08\n\n\nTime:\n11:29:50\nLog-Likelihood:\n-190.42\n\n\nNo. Observations:\n45\nAIC:\n384.8\n\n\nDf Residuals:\n43\nBIC:\n388.5\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n10.6035\n5.198\n2.040\n0.048\n0.120\n21.087\n\n\neducation\n0.5949\n0.086\n6.893\n0.000\n0.421\n0.769\n\n\n\n\n\n\nOmnibus:\n9.841\nDurbin-Watson:\n1.736\n\n\nProb(Omnibus):\n0.007\nJarque-Bera (JB):\n10.609\n\n\nSkew:\n0.776\nProb(JB):\n0.00497\n\n\nKurtosis:\n4.802\nCond. No.\n123.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe estimated regresssion is income = 10.6 + 0.59 education. At a 5% p-value level both the intercept and the coefficient are significant. R-squared is 0.52: the model explains half of the variance.\nUse statsmodels to estimate \\(\\text{income} = \\alpha + \\beta \\times \\text{prestige}\\). Comment regression statistics.\n\nformula = \"income ~ education\"\n\n\ndf.head()\n\n\n\n\n\n\n\n\ntype\nincome\neducation\nprestige\n\n\n\n\naccountant\nprof\n62\n86\n82\n\n\npilot\nprof\n72\n76\n83\n\n\narchitect\nprof\n75\n92\n90\n\n\nauthor\nprof\n55\n90\n76\n\n\nchemist\nprof\n64\n86\n90\n\n\n\n\n\n\n\n\nmodel_2 = smf.ols(\"income ~ prestige\", df)\nres_2 = model_2.fit()\n\n\nres_2.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nincome\nR-squared:\n0.702\n\n\nModel:\nOLS\nAdj. R-squared:\n0.695\n\n\nMethod:\nLeast Squares\nF-statistic:\n101.3\n\n\nDate:\nTue, 02 Feb 2021\nProb (F-statistic):\n7.14e-13\n\n\nTime:\n11:55:59\nLog-Likelihood:\n-179.93\n\n\nNo. Observations:\n45\nAIC:\n363.9\n\n\nDf Residuals:\n43\nBIC:\n367.5\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n10.8840\n3.678\n2.959\n0.005\n3.467\n18.301\n\n\nprestige\n0.6497\n0.065\n10.062\n0.000\n0.519\n0.780\n\n\n\n\n\n\nOmnibus:\n8.893\nDurbin-Watson:\n2.048\n\n\nProb(Omnibus):\n0.012\nJarque-Bera (JB):\n19.848\n\n\nSkew:\n0.047\nProb(JB):\n4.90e-05\n\n\nKurtosis:\n6.252\nCond. No.\n104.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe estimated regresssion is income = 10.6 + 0.59 education. At a 0.5% p-value level both the intercept and the coefficient are significant. R-squared is 0.70: the model predicts income better than the former one.\nUse statsmodels to estimate \\(\\text{income} = \\alpha + \\beta \\times \\text{education} + \\beta_2 \\times \\text{prestige}\\). Comment regression statistics.\n\nmodel_3 = smf.ols(\"income ~ education + prestige\", df)\nres_3 = model_3.fit()\n\n\nres_3.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nincome\nR-squared:\n0.702\n\n\nModel:\nOLS\nAdj. R-squared:\n0.688\n\n\nMethod:\nLeast Squares\nF-statistic:\n49.55\n\n\nDate:\nTue, 02 Feb 2021\nProb (F-statistic):\n8.88e-12\n\n\nTime:\n11:56:03\nLog-Likelihood:\n-179.90\n\n\nNo. Observations:\n45\nAIC:\n365.8\n\n\nDf Residuals:\n42\nBIC:\n371.2\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n10.4264\n4.164\n2.504\n0.016\n2.024\n18.829\n\n\neducation\n0.0323\n0.132\n0.244\n0.808\n-0.234\n0.299\n\n\nprestige\n0.6237\n0.125\n5.003\n0.000\n0.372\n0.875\n\n\n\n\n\n\nOmnibus:\n9.200\nDurbin-Watson:\n2.053\n\n\nProb(Omnibus):\n0.010\nJarque-Bera (JB):\n21.265\n\n\nSkew:\n0.075\nProb(JB):\n2.41e-05\n\n\nKurtosis:\n6.364\nCond. No.\n168.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nThe \\(R^2\\) is only slightly higher than last model, but adjusted \\(R^2\\) is actually lower: the model has less predictive power.\nThe coefficient for education is not significant. It should be dropped from the regresssion.\nThis might happen, because education and prestige are correlated. Let’s check it:\n\ndf.corr()\n\n\n\n\n\n\n\n\nincome\neducation\nprestige\n\n\n\n\nincome\n1.000000\n0.724512\n0.837801\n\n\neducation\n0.724512\n1.000000\n0.851916\n\n\nprestige\n0.837801\n0.851916\n1.000000\n\n\n\n\n\n\n\nEducation and prestige are correlated at 83%. It makes no sense keeping the two in the same regression.\nWHich model would you recommend? For which purpose?\nIf the goal is to predict income, the one with prestige only, has the highest prediction power. If we are interested in the effect of education, we keep only education.\nPlot the regression with prestige\n\na = res_2.params.Intercept\nb = res_2.params.prestige\n\n\nx = df['prestige']\n\n\ny = a + b*x\n\n\nfrom matplotlib import pyplot as plt\nplt.plot(x, df['income'],'o')\nplt.plot(x, y)\nplt.xlabel(\"prestige\")\nplt.xlabel(\"income\")\n\nText(0.5, 0, 'income')\n\n\n\n\n\nCheck visually normality of residuals\n\npred = a + b*x\nactual = df['income']\nresid = actual - pred  # same as res_2.resid\n\n\nplt.plot(x,resid, 'o')\n\n\n\n\n\nplt.hist(resid)\n\n(array([0.00255915, 0.        , 0.        , 0.00511829, 0.0486238 ,\n        0.02815062, 0.01535488, 0.01023659, 0.00255915, 0.00255915]),\n array([-46.40643935, -37.72299114, -29.03954294, -20.35609473,\n        -11.67264653,  -2.98919832,   5.69424989,  14.37769809,\n         23.0611463 ,  31.74459451,  40.42804271]),\n &lt;BarContainer object of 10 artists&gt;)"
  },
  {
    "objectID": "session_4/Regressions_correction_2022.html#finding-the-right-model",
    "href": "session_4/Regressions_correction_2022.html#finding-the-right-model",
    "title": "Regressions",
    "section": "Finding the right model",
    "text": "Finding the right model\nImport dataset from data.dta. Explore dataset (statistics, plots)\n\nimport pandas\n\n\ndf = pandas.read_stata('data.dta')\ndf.head()\n\n\n\n\n\n\n\n\nindex\nx\ny\nz\n\n\n\n\n0\n0\n1.504053\n0.543556\n1.917895\n\n\n1\n1\n43.619758\n0.543113\n4.058487\n\n\n2\n2\n1.226398\n0.736955\n1.785403\n\n\n3\n3\n89.103260\n0.996219\n6.321152\n\n\n4\n4\n32.117073\n0.140142\n3.445228\n\n\n\n\n\n\n\nOur goal is to explain z by x and y. Run a regression.\n\nfrom statsmodels.formula import api as smf\n\n\nmodel = smf.ols('z ~ x + y', data=df)\nres = model.fit()\n\n\nres.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nz\nR-squared:\n0.800\n\n\nModel:\nOLS\nAdj. R-squared:\n0.791\n\n\nMethod:\nLeast Squares\nF-statistic:\n93.90\n\n\nDate:\nTue, 23 Feb 2021\nProb (F-statistic):\n3.82e-17\n\n\nTime:\n10:41:10\nLog-Likelihood:\n-57.244\n\n\nNo. Observations:\n50\nAIC:\n120.5\n\n\nDf Residuals:\n47\nBIC:\n126.2\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n1.2177\n0.243\n5.019\n0.000\n0.730\n1.706\n\n\nx\n0.0356\n0.003\n12.235\n0.000\n0.030\n0.041\n\n\ny\n1.9128\n0.369\n5.177\n0.000\n1.169\n2.656\n\n\n\n\n\n\nOmnibus:\n3.205\nDurbin-Watson:\n1.859\n\n\nProb(Omnibus):\n0.201\nJarque-Bera (JB):\n2.349\n\n\nSkew:\n0.277\nProb(JB):\n0.309\n\n\nKurtosis:\n3.906\nCond. No.\n187.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nComments: regression looks significant. \\(R^2\\) looks good. Fisher statistics, is conclusive (the hypothesis H0 that all coefficients are zero is rejected at a 3.82e-17 confidence level.. The student statistics are also quite high. For each coefficient, the hypothesis H0 that coefficient is zero is rejected at a 0.001 confidence level.\nExamine the residuals of the regression. What’s wrong? Remedy?\n\nfrom matplotlib import pyplot as plt\n\n\nplt.plot(res.resid, 'o')\n\n\n\n\n\nplt.subplot(131)\nplt.plot(df['x'], df['y'],'o')\nplt.subplot(132)\nplt.plot(df['y'], df['z'],'o')\nplt.subplot(133)\nplt.plot(df['x'], df['z'],'o')\nplt.xlabel(\"x\")\nplt.ylabel(\"z\")\n\nplt.tight_layout()\n\n\n\n\n\nimport numpy as np\nplt.plot( np.log(df['z']), np.log(df['x']), 'o' )\n\n\n\n\nApparently, there is a linear relationship between ‘log(x)’ and log(y)\n\nfrom numpy import log\n\n\nmodel = smf.ols('log(z) ~ log(x) + y', data=df)\nres = model.fit()\n\n\nres.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nlog(z)\nR-squared:\n0.957\n\n\nModel:\nOLS\nAdj. R-squared:\n0.955\n\n\nMethod:\nLeast Squares\nF-statistic:\n525.6\n\n\nDate:\nTue, 23 Feb 2021\nProb (F-statistic):\n6.89e-33\n\n\nTime:\n10:41:56\nLog-Likelihood:\n44.223\n\n\nNo. Observations:\n50\nAIC:\n-82.45\n\n\nDf Residuals:\n47\nBIC:\n-76.71\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n0.0128\n0.039\n0.326\n0.746\n-0.066\n0.092\n\n\nlog(x)\n0.2957\n0.010\n29.878\n0.000\n0.276\n0.316\n\n\ny\n0.6164\n0.048\n12.735\n0.000\n0.519\n0.714\n\n\n\n\n\n\nOmnibus:\n4.351\nDurbin-Watson:\n2.490\n\n\nProb(Omnibus):\n0.114\nJarque-Bera (JB):\n1.936\n\n\nSkew:\n0.089\nProb(JB):\n0.380\n\n\nKurtosis:\n2.052\nCond. No.\n12.1\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nplt.plot(res.resid, 'o')"
  },
  {
    "objectID": "session_4/Regressions_correction_2022.html#taylor-rule",
    "href": "session_4/Regressions_correction_2022.html#taylor-rule",
    "title": "Regressions",
    "section": "Taylor Rule",
    "text": "Taylor Rule\nIn 1993, John taylor, estimated, using US data the regression: \\(i_t = i^{\\star} + \\alpha_{\\pi} \\pi_t + \\alpha_{\\pi} y_t\\) where \\(\\pi_t\\) is inflation and \\(y_t\\) the output gap (let’s say deviation from real gdp from the trend). He found that both coefficients were not significantly different from \\(0.5\\). Our goal, is to replicate the same analysis.\nImport macro data from statsmodels (https://www.statsmodels.org/devel/datasets/generated/macrodata.html)\n\nimport statsmodels\n\n## google: stats models macrodata\n## google: statsmodels datasets  -&gt; example in the tutorial\n\n# https://www.statsmodels.org/0.6.1/datasets/index.html\n# example about how to use lengley database\n\n\nimport statsmodels.api as sm\n\n\nsm.datasets.macrodata\n\n&lt;module 'statsmodels.datasets.macrodata' from '/home/pablo/.local/opt/miniconda3/lib/python3.8/site-packages/statsmodels/datasets/macrodata/__init__.py'&gt;\n\n\n\nds = sm.datasets.macrodata.load_pandas()\n\n\ndf = ds.raw_data\ndf.head()\n\n\n\n\n\n\n\n\nyear\nquarter\nrealgdp\nrealcons\nrealinv\nrealgovt\nrealdpi\ncpi\nm1\ntbilrate\nunemp\npop\ninfl\nrealint\n\n\n\n\n0\n1959.0\n1.0\n2710.349\n1707.4\n286.898\n470.045\n1886.9\n28.98\n139.7\n2.82\n5.8\n177.146\n0.00\n0.00\n\n\n1\n1959.0\n2.0\n2778.801\n1733.7\n310.859\n481.301\n1919.7\n29.15\n141.7\n3.08\n5.1\n177.830\n2.34\n0.74\n\n\n2\n1959.0\n3.0\n2775.488\n1751.8\n289.226\n491.260\n1916.4\n29.35\n140.5\n3.82\n5.3\n178.657\n2.74\n1.09\n\n\n3\n1959.0\n4.0\n2785.204\n1753.7\n299.356\n484.052\n1931.3\n29.37\n140.0\n4.33\n5.6\n179.386\n0.27\n4.06\n\n\n4\n1960.0\n1.0\n2847.699\n1770.5\n331.722\n462.199\n1955.5\n29.54\n139.6\n3.50\n5.2\n180.007\n2.31\n1.19\n\n\n\n\n\n\n\nCreate a database with all variables of interest including detrended gdp\n\ngdp = df['realgdp']\ninflation = df['infl']\nrealint = df['realint']\n\n\nddf = df # \n\n\nddf.head()\n\n\n\n\n\n\n\n\nyear\nquarter\nrealgdp\nrealcons\nrealinv\nrealgovt\nrealdpi\ncpi\nm1\ntbilrate\nunemp\npop\ninfl\nrealint\n\n\n\n\n0\n1959.0\n1.0\n2710.349\n1707.4\n286.898\n470.045\n1886.9\n28.98\n139.7\n2.82\n5.8\n177.146\n0.00\n0.00\n\n\n1\n1959.0\n2.0\n2778.801\n1733.7\n310.859\n481.301\n1919.7\n29.15\n141.7\n3.08\n5.1\n177.830\n2.34\n0.74\n\n\n2\n1959.0\n3.0\n2775.488\n1751.8\n289.226\n491.260\n1916.4\n29.35\n140.5\n3.82\n5.3\n178.657\n2.74\n1.09\n\n\n3\n1959.0\n4.0\n2785.204\n1753.7\n299.356\n484.052\n1931.3\n29.37\n140.0\n4.33\n5.6\n179.386\n0.27\n4.06\n\n\n4\n1960.0\n1.0\n2847.699\n1770.5\n331.722\n462.199\n1955.5\n29.54\n139.6\n3.50\n5.2\n180.007\n2.31\n1.19\n\n\n\n\n\n\n\nWe use the fisher relation: \\(r_t = i_t - \\pi_t\\)\n\nddf['ir'] = ddf['realint'] + ddf['infl']\n\nto detrend the gdp, we use hp-filter function from scipy google: hpfilter scipy\n\nfrom statsmodels.tsa.filters.hp_filter import hpfilter\n\n\ncycle, trend = hpfilter(ddf['realgdp'])\n\n\nddf['gdp'] = cycle/trend*100 # nominal interest rate and inflation are in percent\n\n\nddf.head()\n\n\n\n\n\n\n\n\nyear\nquarter\nrealgdp\nrealcons\nrealinv\nrealgovt\nrealdpi\ncpi\nm1\ntbilrate\nunemp\npop\ninfl\nrealint\nir\ngdp\n\n\n\n\n0\n1959.0\n1.0\n2710.349\n1707.4\n286.898\n470.045\n1886.9\n28.98\n139.7\n2.82\n5.8\n177.146\n0.00\n0.00\n0.00\n1.479383\n\n\n1\n1959.0\n2.0\n2778.801\n1733.7\n310.859\n481.301\n1919.7\n29.15\n141.7\n3.08\n5.1\n177.830\n2.34\n0.74\n3.08\n2.967657\n\n\n2\n1959.0\n3.0\n2775.488\n1751.8\n289.226\n491.260\n1916.4\n29.35\n140.5\n3.82\n5.3\n178.657\n2.74\n1.09\n3.83\n1.792534\n\n\n3\n1959.0\n4.0\n2785.204\n1753.7\n299.356\n484.052\n1931.3\n29.37\n140.0\n4.33\n5.6\n179.386\n0.27\n4.06\n4.33\n1.110571\n\n\n4\n1960.0\n1.0\n2847.699\n1770.5\n331.722\n462.199\n1955.5\n29.54\n139.6\n3.50\n5.2\n180.007\n2.31\n1.19\n3.50\n2.331547\n\n\n\n\n\n\n\nRun the basic regression\n\nfrom statsmodels.formula import api as sm\n\n\nmodel = sm.ols(\"ir ~ infl + gdp\", data=ddf) # no intercept\nresults = model.fit()\nresults.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nir\nR-squared:\n0.389\n\n\nModel:\nOLS\nAdj. R-squared:\n0.383\n\n\nMethod:\nLeast Squares\nF-statistic:\n63.65\n\n\nDate:\nTue, 02 Mar 2021\nProb (F-statistic):\n4.06e-22\n\n\nTime:\n11:54:15\nLog-Likelihood:\n-448.17\n\n\nNo. Observations:\n203\nAIC:\n902.3\n\n\nDf Residuals:\n200\nBIC:\n912.3\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n3.2035\n0.252\n12.696\n0.000\n2.706\n3.701\n\n\ninfl\n0.5288\n0.050\n10.557\n0.000\n0.430\n0.628\n\n\ngdp\n0.0795\n0.105\n0.759\n0.449\n-0.127\n0.286\n\n\n\n\n\n\nOmnibus:\n30.222\nDurbin-Watson:\n0.417\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n50.662\n\n\nSkew:\n0.796\nProb(JB):\n9.98e-12\n\n\nKurtosis:\n4.858\nCond. No.\n8.56\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nWhich control variables would you propose to add? Does it increase prediction power? How do you interpret that?\n\nddf.head()\n\n\n\n\n\n\n\n\nyear\nquarter\nrealgdp\nrealcons\nrealinv\nrealgovt\nrealdpi\ncpi\nm1\ntbilrate\nunemp\npop\ninfl\nrealint\nir\ngdp\n\n\n\n\n0\n1959.0\n1.0\n2710.349\n1707.4\n286.898\n470.045\n1886.9\n28.98\n139.7\n2.82\n5.8\n177.146\n0.00\n0.00\n0.00\n1.479383\n\n\n1\n1959.0\n2.0\n2778.801\n1733.7\n310.859\n481.301\n1919.7\n29.15\n141.7\n3.08\n5.1\n177.830\n2.34\n0.74\n3.08\n2.967657\n\n\n2\n1959.0\n3.0\n2775.488\n1751.8\n289.226\n491.260\n1916.4\n29.35\n140.5\n3.82\n5.3\n178.657\n2.74\n1.09\n3.83\n1.792534\n\n\n3\n1959.0\n4.0\n2785.204\n1753.7\n299.356\n484.052\n1931.3\n29.37\n140.0\n4.33\n5.6\n179.386\n0.27\n4.06\n4.33\n1.110571\n\n\n4\n1960.0\n1.0\n2847.699\n1770.5\n331.722\n462.199\n1955.5\n29.54\n139.6\n3.50\n5.2\n180.007\n2.31\n1.19\n3.50\n2.331547\n\n\n\n\n\n\n\n\nmodel = sm.ols(\"ir ~ infl + gdp + pop + unemp -1\", data=ddf) # no intercept\nresults = model.fit()\nresults.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nir\nR-squared (uncentered):\n0.884\n\n\nModel:\nOLS\nAdj. R-squared (uncentered):\n0.882\n\n\nMethod:\nLeast Squares\nF-statistic:\n380.2\n\n\nDate:\nTue, 02 Mar 2021\nProb (F-statistic):\n5.64e-92\n\n\nTime:\n11:58:05\nLog-Likelihood:\n-432.84\n\n\nNo. Observations:\n203\nAIC:\n873.7\n\n\nDf Residuals:\n199\nBIC:\n886.9\n\n\nDf Model:\n4\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\ninfl\n0.4380\n0.049\n8.895\n0.000\n0.341\n0.535\n\n\ngdp\n0.5710\n0.120\n4.739\n0.000\n0.333\n0.809\n\n\npop\n-0.0050\n0.002\n-2.068\n0.040\n-0.010\n-0.000\n\n\nunemp\n0.8064\n0.108\n7.458\n0.000\n0.593\n1.020\n\n\n\n\n\n\nOmnibus:\n5.307\nDurbin-Watson:\n0.391\n\n\nProb(Omnibus):\n0.070\nJarque-Bera (JB):\n7.501\n\n\nSkew:\n0.070\nProb(JB):\n0.0235\n\n\nKurtosis:\n3.931\nCond. No.\n247.\n\n\n\nNotes:[1] R² is computed without centering (uncentered) since the model does not contain a constant.[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nAt confidence level 2.5% gdp is between 0.333 and 0.809.\nAt confidence level 2.5% infl is between 0.341 and 0.535.\nThe coefficients would be significantly different from 0.5 if 0.5 was not in the condifence interval."
  },
  {
    "objectID": "session_4/transcript.html",
    "href": "session_4/transcript.html",
    "title": "Multiple Regressions",
    "section": "",
    "text": "type\nincome\neducation\nprestige\n\n\n\n\naccountant\nprof\n62\n86\n82\n\n\npilot\nprof\n72\n76\n83\n\n\narchitect\nprof\n75\n92\n90\n\n\nauthor\nprof\n55\n90\n76\n\n\nchemist\nprof\n64\n86\n90\n\n\n\n\n\nLast week we “ran” a linear regression: \\(y = \\alpha + \\beta x\\). Result: \\[\\text{income} = xx + 0.72 \\text{education}\\]\nShould we have looked at “prestige” instead ? \\[\\text{income} = xx + 0.83 \\text{prestige}\\]\nWhich one is better?\n\n\n\n\n\n\n\n\n\nif the goal is to predict: the one with higher explained variance\n\nprestige has higher \\(R^2\\) (\\(0.83^2\\))\n\nunless we are interested in the effect of education\n\n\n\n\n\n\nWhat about using both?\n\n2 variables model: \\[\\text{income} = \\alpha + \\beta_1 \\text{education} + \\beta_2 \\text{prestige}\\]\nwill probably improve prediction power (explained variance)\n\\(\\beta_1\\) might not be meaningful on its own anymore (education and prestige are correlated)\n\n\n\n\n\n\nNow we are trying to fit a plane to a cloud of points.\n \n\n\n\n\n\nTake all observations: \\((\\text{income}\\_n,\\text{education}\\_n,\\text{prestige}\\_n)\\_{n\\in[0,N]}\\)\nObjective: sum of squares \\[ L(\\alpha, \\beta_1, \\beta_2) = \\sum_i \\left( \\underbrace{ \\alpha + \\beta_1 \\text{education}\\_n + \\beta_2 \\text{prestige}\\_n - \\text{income}\\_n }\\_{e_n=\\text{prediction error} }\\right)^2 \\]\nMinimize loss function in \\(\\alpha\\), \\(\\beta_1\\), \\(\\beta_2\\)\nAgain, we can perform numerical optimization (machine learning approach)\n\n… but there is an explicit formula\n\n\n\n\n\n\n\n\n\n\\[Y = \\begin{bmatrix}\n\\text{income}_1 \\\\\\\\\n\\vdots \\\\\\\\\n\\text{income}_N\n\\end{bmatrix}\\] \\[X = \\begin{bmatrix}\n1 & \\text{education}_1 & \\text{prestige}_1 \\\\\\\\\n\\vdots & \\vdots & \\vdots \\\\\\\\\n1 &\\text{education}_N & \\text{prestige}_N\n\\end{bmatrix}\\]\n\n\nMatrix Version (look for \\(B = \\left( \\alpha, \\beta_1 , \\beta_2 \\right)\\)): \\[Y =  X B + E\\]\nNote that constant can be interpreted as a “variable”\nLoss function \\[L(A,B) = (Y - X B)' (Y - X B)\\]\nResult of minimization \\(\\min_{(A,B)} L(A,B)\\) : \\[\\begin{bmatrix}\\alpha & \\beta_1 & \\beta_2 \\end{bmatrix} = (X'X)^{-1} X' Y \\]\n\n\n\n\n\n\n\nResult: \\[\\text{income} = 10.43  + 0.03 \\times \\text{education} + 0.62 \\times \\text{prestige}\\]\nQuestions:\n\nis it a better regression than the other?\nis the coefficient in front of education significant?\nhow do we interpret it?\ncan we build confidence intervals?"
  },
  {
    "objectID": "session_4/transcript.html#remember-dataset-from-last-time",
    "href": "session_4/transcript.html#remember-dataset-from-last-time",
    "title": "Multiple Regressions",
    "section": "",
    "text": "type\nincome\neducation\nprestige\n\n\n\n\naccountant\nprof\n62\n86\n82\n\n\npilot\nprof\n72\n76\n83\n\n\narchitect\nprof\n75\n92\n90\n\n\nauthor\nprof\n55\n90\n76\n\n\nchemist\nprof\n64\n86\n90\n\n\n\n\n\nLast week we “ran” a linear regression: \\(y = \\alpha + \\beta x\\). Result: \\[\\text{income} = xx + 0.72 \\text{education}\\]\nShould we have looked at “prestige” instead ? \\[\\text{income} = xx + 0.83 \\text{prestige}\\]\nWhich one is better?"
  },
  {
    "objectID": "session_4/transcript.html#prestige-or-education",
    "href": "session_4/transcript.html#prestige-or-education",
    "title": "Multiple Regressions",
    "section": "",
    "text": "if the goal is to predict: the one with higher explained variance\n\nprestige has higher \\(R^2\\) (\\(0.83^2\\))\n\nunless we are interested in the effect of education"
  },
  {
    "objectID": "session_4/transcript.html#multiple-regression",
    "href": "session_4/transcript.html#multiple-regression",
    "title": "Multiple Regressions",
    "section": "",
    "text": "What about using both?\n\n2 variables model: \\[\\text{income} = \\alpha + \\beta_1 \\text{education} + \\beta_2 \\text{prestige}\\]\nwill probably improve prediction power (explained variance)\n\\(\\beta_1\\) might not be meaningful on its own anymore (education and prestige are correlated)"
  },
  {
    "objectID": "session_4/transcript.html#fitting-a-model",
    "href": "session_4/transcript.html#fitting-a-model",
    "title": "Multiple Regressions",
    "section": "",
    "text": "Now we are trying to fit a plane to a cloud of points."
  },
  {
    "objectID": "session_4/transcript.html#minimization-criterium",
    "href": "session_4/transcript.html#minimization-criterium",
    "title": "Multiple Regressions",
    "section": "",
    "text": "Take all observations: \\((\\text{income}\\_n,\\text{education}\\_n,\\text{prestige}\\_n)\\_{n\\in[0,N]}\\)\nObjective: sum of squares \\[ L(\\alpha, \\beta_1, \\beta_2) = \\sum_i \\left( \\underbrace{ \\alpha + \\beta_1 \\text{education}\\_n + \\beta_2 \\text{prestige}\\_n - \\text{income}\\_n }\\_{e_n=\\text{prediction error} }\\right)^2 \\]\nMinimize loss function in \\(\\alpha\\), \\(\\beta_1\\), \\(\\beta_2\\)\nAgain, we can perform numerical optimization (machine learning approach)\n\n… but there is an explicit formula"
  },
  {
    "objectID": "session_4/transcript.html#ordinary-least-square",
    "href": "session_4/transcript.html#ordinary-least-square",
    "title": "Multiple Regressions",
    "section": "",
    "text": "\\[Y = \\begin{bmatrix}\n\\text{income}_1 \\\\\\\\\n\\vdots \\\\\\\\\n\\text{income}_N\n\\end{bmatrix}\\] \\[X = \\begin{bmatrix}\n1 & \\text{education}_1 & \\text{prestige}_1 \\\\\\\\\n\\vdots & \\vdots & \\vdots \\\\\\\\\n1 &\\text{education}_N & \\text{prestige}_N\n\\end{bmatrix}\\]\n\n\nMatrix Version (look for \\(B = \\left( \\alpha, \\beta_1 , \\beta_2 \\right)\\)): \\[Y =  X B + E\\]\nNote that constant can be interpreted as a “variable”\nLoss function \\[L(A,B) = (Y - X B)' (Y - X B)\\]\nResult of minimization \\(\\min_{(A,B)} L(A,B)\\) : \\[\\begin{bmatrix}\\alpha & \\beta_1 & \\beta_2 \\end{bmatrix} = (X'X)^{-1} X' Y \\]"
  },
  {
    "objectID": "session_4/transcript.html#solution",
    "href": "session_4/transcript.html#solution",
    "title": "Multiple Regressions",
    "section": "",
    "text": "Result: \\[\\text{income} = 10.43  + 0.03 \\times \\text{education} + 0.62 \\times \\text{prestige}\\]\nQuestions:\n\nis it a better regression than the other?\nis the coefficient in front of education significant?\nhow do we interpret it?\ncan we build confidence intervals?"
  },
  {
    "objectID": "session_4/transcript.html#explained-variance-1",
    "href": "session_4/transcript.html#explained-variance-1",
    "title": "Multiple Regressions",
    "section": "Explained Variance",
    "text": "Explained Variance\n\nAs in the 1d case we can compare:\n\nthe variability of the model predictions (\\(MSS\\))\nthe variance of the data (\\(TSS\\), T for total)\n\nCoefficient of determination: \\[R^2 = \\frac{MSS}{TSS}\\]\nOr: \\[R^2 = 1-\\frac{RSS}{SST}\\] where \\(RSS\\) is the non explained variance"
  },
  {
    "objectID": "session_4/transcript.html#adjusted-r-squared",
    "href": "session_4/transcript.html#adjusted-r-squared",
    "title": "Multiple Regressions",
    "section": "Adjusted R squared",
    "text": "Adjusted R squared\n\n\n\nIn our example:\n\n\n\n\n\n\n\n\n\nRegression\n\\(R^2\\)\n \\(R^2_{adj}\\) \n\n\n\n\neducation\n0.525\n 0.514 \n\n\nprestige\n0.702\n 0.695 \n\n\neducation + prestige\n0.7022\n 0.688 \n\n\n\n\n\n\nFact:\n\nadding more regressors always improve \\(R^2\\)\nwhy not throw everything in? (kitchen sink regressions)\n\ntwo many regressors: overfitting the data\n\n\nPenalise additional regressors: adjusted R^2\n\nexample formula:\n\n\\(N\\): number of observations\n\\(p\\) number of variables \\[R^2_{adj} = 1-(1-R^2)\\frac{N-1}{N-p-1}\\]"
  },
  {
    "objectID": "session_4/transcript.html#making-a-regression-with-statsmodels",
    "href": "session_4/transcript.html#making-a-regression-with-statsmodels",
    "title": "Multiple Regressions",
    "section": "Making a regression with statsmodels",
    "text": "Making a regression with statsmodels\nimport statsmodels\nWe use a special API inspired by R:\nimport statsmodels.formula.api as smf"
  },
  {
    "objectID": "session_4/transcript.html#performing-a-regression",
    "href": "session_4/transcript.html#performing-a-regression",
    "title": "Multiple Regressions",
    "section": "Performing a regression",
    "text": "Performing a regression\n\nRunning a regression with statsmodels\n\nmodel = smf.ols('income ~ education',  df)  # model\nres = model.fit()  # perform the regression\nres.describe()\n\n‘income ~ education’ is the model formula\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 income   R-squared:                       0.525\nModel:                            OLS   Adj. R-squared:                  0.514\nMethod:                 Least Squares   F-statistic:                     47.51\nDate:                Tue, 02 Feb 2021   Prob (F-statistic):           1.84e-08\nTime:                        05:21:25   Log-Likelihood:                -190.42\nNo. Observations:                  45   AIC:                             384.8\nDf Residuals:                      43   BIC:                             388.5\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n==============================================================================\nIntercept     10.6035      5.198      2.040      0.048       0.120      21.087\neducation      0.5949      0.086      6.893      0.000       0.421       0.769\n==============================================================================\nOmnibus:                        9.841   Durbin-Watson:                   1.736\nProb(Omnibus):                  0.007   Jarque-Bera (JB):               10.609\nSkew:                           0.776   Prob(JB):                      0.00497\nKurtosis:                       4.802   Cond. No.                         123.\n=============================================================================="
  },
  {
    "objectID": "session_4/transcript.html#formula-mini-language",
    "href": "session_4/transcript.html#formula-mini-language",
    "title": "Multiple Regressions",
    "section": "Formula mini-language",
    "text": "Formula mini-language\n\nWith statsmodels formulas, can be supplied with R-style syntax\nExamples:\n\n\n\n\n\n\n\n\nFormula\nModel\n\n\n\n\nincome ~ education\n\\(\\text{income}_i = \\alpha + \\beta \\text{education}_i\\)\n\n\nincome ~ prestige\n\\(\\text{income}_i = \\alpha + \\beta \\text{prestige}_i\\)\n\n\nincome ~ prestige - 1\n\\(\\text{income}_i = \\beta \\text{prestige}_i\\) (no intercept)\n\n\nincome ~ education + prestige\n\\(\\text{income}_i = \\alpha + \\beta_1 \\text{education}_i + \\beta_2 \\text{prestige}_i\\)"
  },
  {
    "objectID": "session_4/transcript.html#formula-mini-language-1",
    "href": "session_4/transcript.html#formula-mini-language-1",
    "title": "Multiple Regressions",
    "section": "Formula mini-language",
    "text": "Formula mini-language\n\nOne can use formulas to apply transformations to variables\n\n\n\n\n\n\n\n\nFormula\nModel\n\n\n\n\nlog(P) ~ log(M) + log(Y)\n\\(\\log(P_i) = \\alpha + \\alpha_1 \\log(M_i) + \\alpha_2 \\log(Y_i)\\) (log-log)\n\n\nlog(Y) ~ i\n\\(\\log(P_i) = \\alpha + i_i\\) (semi-logs)\n\n\n\n\nThis is useful if the true relationship is nonlinear\nAlso useful, to interpret the coefficients"
  },
  {
    "objectID": "session_4/transcript.html#coefficients-interpetation",
    "href": "session_4/transcript.html#coefficients-interpetation",
    "title": "Multiple Regressions",
    "section": "Coefficients interpetation",
    "text": "Coefficients interpetation\n\nExample:\n\n(police_spending and prevention_policies in million dollars) \\[\\text{number_or_crimes} = 0.005\\% - 0.001 \\text{pol_spend} - 0.005 \\text{prev_pol} + 0.002 \\text{population density}\\]\n\nreads: when holding other variables constant a 0.1 million increase in police spending reduces crime rate by 0.001%\ninterpretation?\n\nproblematic because variables have different units\nwe can say that prevention policies are more efficient than police spending ceteris paribus\n\nTake logs: \\[\\log(\\text{number_or_crimes}) = 0.005\\% - 0.15 \\log(\\text{pol_spend}) - 0.4 \\log(\\text{prev_pol}) + 0.2 \\log(\\text{population density})\\]\n\nnow we have an estimate of elasticities\na \\(1\\%\\) increase in police spending leads to a \\(0.15\\%\\) decrease in the number of crimes"
  },
  {
    "objectID": "session_4/transcript.html#hypotheses",
    "href": "session_4/transcript.html#hypotheses",
    "title": "Multiple Regressions",
    "section": "Hypotheses",
    "text": "Hypotheses\n\n\n\n\nRecall what we do:\n\nwe have the data \\(X,Y\\)\nwe choose a model: \\[ Y = \\alpha + X \\beta \\]\nfrom the data we compute estimates: \\[\\hat{\\beta}  = (X'X)^{-1} X' Y \\] \\[\\hat{\\alpha} = Y- X \\beta \\]\nestimates are a precise function of data\n\nexact formula not important here\n\n\n\n\n\n\nWe make some hypotheses on the data generation process:\n\n\\(Y = X \\beta + \\epsilon\\)\n\\(\\mathbb{E}\\left[ \\epsilon \\right] = 0\\)\n\\(\\epsilon\\) multivariate normal with covariance matrix \\(\\sigma^2 I_n\\)\n\n\\(\\forall i, \\sigma(\\epsilon_i) = \\sigma\\)\n\\(\\forall i,j, cov(\\epsilon_i, \\epsilon_j) = 0\\)\n\n\n\nUnder these hypotheses:\n\n\\(\\hat{\\beta}\\) is an unbiased estimate of true parameter \\(\\beta\\)\n\ni.e. \\(\\mathbb{E} [\\hat{\\beta}] = \\beta\\)\n\none can prove \\(Var(\\hat{\\beta}) = \\sigma^2 I_n\\)\n\\(\\sigma\\) can be estimated by \\(\\hat{\\sigma}=S\\frac{\\sum_i (y_i-{pred}_i)^2}{N-p}\\)\n\n\\(N-p\\): degrees of freedoms\n\none can estimate: \\(\\sigma(\\hat{\\beta_k})\\)\n\nit is the \\(i\\)-th diagonal element of \\(\\hat{\\sigma}^2 X'X\\)"
  },
  {
    "objectID": "session_4/transcript.html#is-the-regression-significant",
    "href": "session_4/transcript.html#is-the-regression-significant",
    "title": "Multiple Regressions",
    "section": "Is the regression significant?",
    "text": "Is the regression significant?\n\n\n\nApproach is very similar to the one-dimensional case\nFisher criterium (F-test):\n\n\\(H0\\): all coeficients are 0\n\ni.e. true model is \\(y=\\alpha + \\epsilon\\)\n\n\\(H1\\): some coefficients are not 0\n\nStatistics: \\[F=\\frac{MSR}{MSE}\\]\n\n\\(MSR\\): mean-squared error of constant model\n\\(MSE\\): mean-squared error of full model\n\n\n\n\n\nUnder:\n\nthe model assumptions about the data generation process\nthe H0 hypothesis\n\n… the distribution of \\(F\\) is known\nIt is remarkable that it doesn’t depend on \\(\\sigma\\) !\n\nOne can produce a p-value.\n\nprobability to obtain this statistics given hypothesis H0\nif very low, H0 is rejected"
  },
  {
    "objectID": "session_4/transcript.html#is-each-coefficient-significant",
    "href": "session_4/transcript.html#is-each-coefficient-significant",
    "title": "Multiple Regressions",
    "section": "Is each coefficient significant ?",
    "text": "Is each coefficient significant ?\n\nStudent test. Given a coefficient \\(\\beta_k\\):\n\n\\(H0\\): coefficient is 0\n\\(H1\\): coefficient is not zero\n\nStatistics: \\(t = \\frac{\\hat{\\beta_k}}{\\hat{\\sigma}(\\hat{\\beta_k})}\\)\n\nwhere \\(\\hat{\\sigma}(\\beta_k)\\) is \\(i\\)-th diagonal element of \\(\\hat{\\sigma}^2 X'X\\)\nit compares the estimated value of a coefficient to its estimated standard deviation\n\nUnder the inference hypotheses, distribution of \\(t\\) is known.\n\nit is a student distribution\n\nProcedure:\n\nCompute \\(t\\). Check acceptance threshold \\(t*\\) at probability \\(\\alpha\\) (ex 5%)\nCoefficient is significant with probability \\(1-\\alpha\\) if \\(t&gt;t*\\)\nOr just look at the \\(p-value\\): probability that \\(t\\) would be as high as it is, assuming \\(H0\\)"
  },
  {
    "objectID": "session_4/transcript.html#confidence-intervals",
    "href": "session_4/transcript.html#confidence-intervals",
    "title": "Multiple Regressions",
    "section": "Confidence intervals",
    "text": "Confidence intervals\n\nSame as in the 1d case.\nTake estimate \\(\\color{red}{\\beta_i}\\) with an estimate of its standard deviation \\(\\color{red}{\\hat{\\sigma}(\\beta_i)}\\)\nCompute student \\(\\color{red}{t^{\\star}}\\) at \\(\\color{red}{\\alpha}\\) confidence level (ex: \\(\\alpha=5\\\\%\\)) such that:\n\n\\(P(|t|&gt;t^{\\star})&lt;\\alpha\\)\n\nProduce confidence intervals at \\(\\alpha\\) confidence level:\n\n\\([\\color{red}{\\beta_i} - t^{\\star} \\color{red}{\\hat{\\sigma}(\\beta_i)}, \\color{red}{\\beta_i} + t^{\\star} \\color{red}{\\hat{\\sigma}(\\beta_i)}]\\)\n\nInterpretation:\n\nfor a given confidence interval at confidence level \\(\\alpha\\)…\nthe probability that our coefficient was obtained, if the true coefficient were outside of it, is smaller than \\(\\alpha\\)"
  },
  {
    "objectID": "session_4/transcript.html#other-tests",
    "href": "session_4/transcript.html#other-tests",
    "title": "Multiple Regressions",
    "section": "Other tests",
    "text": "Other tests\n\nThe tests seen so far rely on strong statistical assumptions (normality, homoscedasticity, etc..)\nSome tests can be used to test these assumptions:\n\nJarque-Bera: is the distribution of data truly normal\nDurbin-Watson: are residuals autocorrelated (makes sense for time-series)\n…\n\nIn case assumptions are not met…\n\n… still possible to do econometrics\n… but beyond the scope of this course"
  },
  {
    "objectID": "session_4/transcript.html#variable-selection-1",
    "href": "session_4/transcript.html#variable-selection-1",
    "title": "Multiple Regressions",
    "section": "Variable selection",
    "text": "Variable selection\n\n\n\nI’ve got plenty of data:\n\n\\(y\\): gdp\n\\(x_1\\): investment\n\\(x_2\\): inflation\n\\(x_3\\): education\n\\(x_4\\): unemployment\n…\n\n\n\n\nMany possible regressions:\n\n\\(y = α + \\beta_1 x_1\\)\n\\(y = α + \\beta_2 x_2 + \\beta_3 x_4\\)\n…\n\n\n\n\n. . .\n\nWhich one do I choose ?\n\nputting everything together is not an option (kitchen sink regression)"
  },
  {
    "objectID": "session_4/transcript.html#not-enough-coefficients",
    "href": "session_4/transcript.html#not-enough-coefficients",
    "title": "Multiple Regressions",
    "section": "Not enough coefficients",
    "text": "Not enough coefficients\n\nSuppose you run a regression: \\[y = \\alpha + \\beta_1 x_1 + \\epsilon\\] and are genuinely interested in coefficient \\(\\beta_1\\)\nBut unknowingly to you, the actual model is \\[y = \\alpha + \\beta_1 x_1 + \\beta_2 x_2 + \\eta\\]\nThe residual \\(y - \\alpha - \\beta_1 x_1\\) is not white noise\n\nspecification hypotheses are violated\nestimate \\(\\hat{\\beta_1}\\) will have a bias (omitted variable bias)\nto correct the bias we add \\(x_2\\)\n\neven though we are not interested in \\(x_2\\) by itself\nwe control for \\(x_2\\))"
  },
  {
    "objectID": "session_4/transcript.html#example",
    "href": "session_4/transcript.html#example",
    "title": "Multiple Regressions",
    "section": "Example",
    "text": "Example\n\n\nSuppose I want to check Okun’s law. I consider the following model: \\[\\text{gdp_growth} = \\alpha + \\beta \\times \\text{unemployment}\\]\nI obtain: \\[\\text{gdp_growth} = 0.01 - 0.1 \\times \\text{unemployment} + e_i\\]\nThen I inspect visually the residuals: not normal at all!\nConclusion: my regression is misspecified, \\(0.1\\) is a biased (useless) estimate\nI need to control for additional variables. For instance: \\[\\text{gdp_growth} = \\alpha + \\beta_1 \\text{unemployment} + \\beta_2 \\text{interest rate}\\]\nUntil the residuals are actually white noise"
  },
  {
    "objectID": "session_4/transcript.html#colinear-regressors",
    "href": "session_4/transcript.html#colinear-regressors",
    "title": "Multiple Regressions",
    "section": "Colinear regressors",
    "text": "Colinear regressors\n\n\nWhat happens if two regressors are (almost) colinear? \\[y = \\alpha + \\beta_1 x_1 + \\beta_2 x_2\\] where \\(x_2 = \\kappa x_1\\)\nIntuitively: parameters are not unique\n\nif \\(y = \\alpha + \\beta_1 x_1\\) is the right model…\nthen \\(y = \\alpha + \\beta_1 \\lambda x_1 + \\beta_2 (1-\\lambda) \\frac{1}{\\kappa} x_2\\) is exactly as good…\n\nMathematically: \\((X'X)\\) is not invertible.\nWhen regressors are almost colinear, coefficients can have a lot of variability.\nTest:\n\ncorrelation statistics\ncorrelation plot"
  },
  {
    "objectID": "session_4/transcript.html#choosing-regressors",
    "href": "session_4/transcript.html#choosing-regressors",
    "title": "Multiple Regressions",
    "section": "Choosing regressors",
    "text": "Choosing regressors\n\\[y = \\alpha + \\beta_1 x_1 + ... \\beta_n x_n\\]\nWhich regressors to choose ?\n\nMethod 1 : remove coefficients with lowest t (less significant) to maximize adjusted R-squared\n\nremove regressors with lowest t\n\nnot the one you are interested in ;)\n\nregress again\nsee if adjusted \\(R^2\\) is decreasing\n\nif so continue\notherwise cancel last step and stop\n\n\nMethod 2 : choose combination to maximize Akaike Information Criterium\n\nAIC: \\(p - log(L)\\)\n\\(L\\) is likelihood\ncomputed by all good econometric softwares"
  },
  {
    "objectID": "session_6/slides.html#data",
    "href": "session_6/slides.html#data",
    "title": "Introduction to Instrumental Variables",
    "section": "Data",
    "text": "Data\n\nOur multilinear regression: \\[y = \\alpha + \\beta x_1 + \\cdots + \\beta x_n\\]\nSo far, we have only considered real variables: (\\(x_i \\in \\mathbb{R}\\)).\n\nex: \\(x_{\\text{gdp}} = \\alpha + \\beta_1 x_{\\text{unemployment}} + \\beta_2 x_{\\text{inflation}}\\)\n\nHow do we deal with the following cases?\n\nbinary variable: \\(x\\in \\{0,1\\}\\) (or \\(\\{True, False}\\))\n\nex: \\(\\text{gonetowar}\\), \\(\\text{hasdegree}\\)\n\ncategorical variable:\n\nex: survey result (0: I don’t know, 1: I strongly disagree, 2: I disagree, 3: I agree, 4: I strongly agree)\nthere is no ranking of answers\nwhen there is ranking: hierarchical index\n\nnonnumerical variables:\n\nex: (flower type: \\(x\\in \\text{myosotis}, \\text{rose}, ...\\))"
  },
  {
    "objectID": "session_6/slides.html#binary-variable",
    "href": "session_6/slides.html#binary-variable",
    "title": "Introduction to Instrumental Variables",
    "section": "Binary variable",
    "text": "Binary variable\n\nNothing to be done: just make sure variables take values 0 or 1. \\[y_\\text{salary} = \\alpha + \\beta x_{\\text{gonetowar}}\\]\nInterpretation:\n\nhaving gone to war is associated with a \\(\\beta\\) increase (or decrease?) in salary (still no causality)"
  },
  {
    "objectID": "session_6/slides.html#categorical-variable",
    "href": "session_6/slides.html#categorical-variable",
    "title": "Introduction to Instrumental Variables",
    "section": "Categorical variable",
    "text": "Categorical variable\n\nLook at the model: \\[y_{\\text{CO2 emission}} = \\alpha + \\beta x_{\\text{yellow vest support}} \\]\nWhere \\(y_{\\text{CO2 emission}}\\) is an individual’s CO2 emissions and \\(x_{\\text{yellow vest support}}\\) is the response the the question Are you in agreement with the yellow vests demands?.\nResponse is coded up as:\n\n0: Strongly disagree\n1: Disagree\n2: Neutral\n3: Agree\n4: Strongly agree\n\nIf the variable was used directly, how would you intepret the coefficient \\(\\beta\\) ?\n\nindex is hierarchical\nbut the distances between 1 and 2 or 2 and 3 are not comparable…"
  },
  {
    "objectID": "session_6/slides.html#hierarchical-index-2",
    "href": "session_6/slides.html#hierarchical-index-2",
    "title": "Introduction to Instrumental Variables",
    "section": "Hierarchical index (2):",
    "text": "Hierarchical index (2):\n\nWe use one dummy variable per possible answer.\n\n\n\n\n\n\n\n\n\n\n\n\\(D_{\\text{Strongly Disagree}}\\)\n\\(D_{\\text{Disagree}}\\)\n\\(D_{\\text{Neutral}}\\)\n\\(D_{\\text{Agree}}\\)\n\\(D_{\\text{Strongly Agree}}\\)\n\n\n\n\n1\n0\n0\n0\n0\n\n\n0\n1\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n1\n0\n\n\n0\n0\n0\n0\n1\n\n\n\n\nValues are linked by the specific dummy coding.\n\nthe choice of the reference group (with 0) is not completely neutral\n\nfor linear regressions, we can ignore its implications\n\nit must be frequent enough in the data\neffects coding: reference group takes -1 instead of 0\n\nNote that hierarchy is lost. The same treatment can be applied to non-hierachical variables\nNow our variables are perfectly colinear:\n\nwe can deduce one from all the others\nwe drop one from the regression: the reference group TODO"
  },
  {
    "objectID": "session_6/slides.html#hierarchical-index-3",
    "href": "session_6/slides.html#hierarchical-index-3",
    "title": "Introduction to Instrumental Variables",
    "section": "Hierarchical index (3)",
    "text": "Hierarchical index (3)\n\\[y_{\\text{CO2 emission}} = \\alpha + \\beta_1 x_{\\text{strdis}} + \\beta_2 x_{\\text{dis}} + \\beta_3 x_{\\text{agr}} + \\beta_4 x_{\\text{stragr}}\\]\n\nInterpretation:\n\nbeing in the group which strongly agrees to the yellow vest’s claim is associated with an additional \\(\\beta_4\\) increase in CO2 consumption compared with members of the neutral group"
  },
  {
    "objectID": "session_6/slides.html#nonnumerical-variables",
    "href": "session_6/slides.html#nonnumerical-variables",
    "title": "Introduction to Instrumental Variables",
    "section": "Nonnumerical variables",
    "text": "Nonnumerical variables\n\n\n\n\nWhat about nonnumerical variables?\n\nWhen variables take nonnumerical variables, we convert them to numerical variables.\n\nExample:\n\n\n\n\nactivity\ncode\n\n\n\n\nmassage therapist\n1\n\n\nmortician\n2\n\n\narcheologist\n3\n\n\nfinancial clerks\n4\n\n\n\n\n\n\nThen we convert to dummy variables exactly like hierarchical indices\n\nhere \\(\\text{massage therapist}\\) is taken as reference\n\n\n\n\n\n\n\n\n\n\n\\(D_{\\text{mortician}}\\)\n\\(D_{\\text{archeologist}}\\)\n\\(D_{\\text{financial clerks}}\\)\n\n\n\n\n1\n0\n0\n\n\n0\n1\n0\n\n\n0\n0\n1"
  },
  {
    "objectID": "session_6/slides.html#hands-on",
    "href": "session_6/slides.html#hands-on",
    "title": "Introduction to Instrumental Variables",
    "section": "Hands-on",
    "text": "Hands-on\nUse statsmodels to create dummy variables with formula API.\n\nReplace\n\nsalary ~ activity\n\nby:\n\nsalary ~ C(activity)\nThere is an options to choose the reference group\nsalary ~ C(activity, Treatment(reference=\"archeologist\"))"
  },
  {
    "objectID": "session_6/slides.html#what-is-causality",
    "href": "session_6/slides.html#what-is-causality",
    "title": "Introduction to Instrumental Variables",
    "section": "What is causality?",
    "text": "What is causality?\n\n\n\n\nGroucho Marx\n\n\nClear? Huh! Why a four-year-old child could understand this report! Run out and find me a four-year-old child, I can’t make head or tail of it."
  },
  {
    "objectID": "session_6/slides.html#spurious-correlation",
    "href": "session_6/slides.html#spurious-correlation",
    "title": "Introduction to Instrumental Variables",
    "section": "Spurious correlation",
    "text": "Spurious correlation\n\nSpurious Correlation\nWe have seen spurious correlation before\n\nit happens when two series comove without being actually correlated\n\nAlso, two series might be correlated without one causing the other\n\nex: countries eating more chocolate have more nobel prices…"
  },
  {
    "objectID": "session_6/slides.html#definitions",
    "href": "session_6/slides.html#definitions",
    "title": "Introduction to Instrumental Variables",
    "section": "Definitions?",
    "text": "Definitions?\nBut how do we define\n\ncorrelation\ncausality\n\n?\nBoth concepts are actually hard to define:\n\nin statistics (and econometrices) they refer to the generating process\nif the data was generated again, would you observe the same relations?\n\nFor instance correlation between \\(X\\) and \\(Y\\) is just the average correlation taken over many draws \\(\\omega\\) of the data: \\[E_{\\omega}\\left[ (X-E[X])(Y-E[Y])\\right]\\]"
  },
  {
    "objectID": "session_6/slides.html#how-do-we-define-causality-1",
    "href": "session_6/slides.html#how-do-we-define-causality-1",
    "title": "Introduction to Instrumental Variables",
    "section": "How do we define causality (1)",
    "text": "How do we define causality (1)\n\nIn math, we have implication: \\(A \\implies B\\)\n\napplies to statements that can be either true or false\ngiven \\(A\\) and \\(B\\), \\(A\\) implies \\(B\\) unless \\(A\\) is true and \\(B\\) is false\nparadox of the drinker: at any time, there exists a person such that: if this person drinks, then everybody drinks\n\nIn a mathematical universe taking values \\(\\omega\\), we can define causality between statement \\(A(\\omega)\\) and \\(B(\\omega)\\) as : \\[\\forall \\omega, A(\\omega) \\implies B(\\omega)\\]"
  },
  {
    "objectID": "session_6/slides.html#how-do-we-define-causality-2",
    "href": "session_6/slides.html#how-do-we-define-causality-2",
    "title": "Introduction to Instrumental Variables",
    "section": "How do we define causality (2)",
    "text": "How do we define causality (2)\nBut causality in the real world is problematic\nUsually, we observe \\(A(\\omega)\\) only once…\n\n\n\nExample:\n\nstate of the world \\(\\omega\\): 2008, big financial crisis, …\nA: Ben Bernanke chairman of the Fed\nB: successful economic interventions\nWas Ben Bernanke a good central banker?\nImpossible to say.\n\n\n\n\n\n\n\n\nThen there is the uncertain concept of time… But let’s take it as granted to not overcomplicate…"
  },
  {
    "objectID": "session_6/slides.html#causality-in-statistics",
    "href": "session_6/slides.html#causality-in-statistics",
    "title": "Introduction to Instrumental Variables",
    "section": "Causality in Statistics",
    "text": "Causality in Statistics\n\n\n\n\n\n\nStatistical definition of causality\n\n\nVariable \\(A\\) causes \\(B\\) in a statistical sense if - \\(A\\) and \\(B\\) are correlated - \\(A\\) is known before \\(B\\) - correlation between \\(A\\) and \\(B\\) is unaffected by other variables\n\n\n\n\nThere are other related statistical definitions:\n\nlike Granger causality…\n… but not for this course"
  },
  {
    "objectID": "session_6/slides.html#factual-and-counterfactual",
    "href": "session_6/slides.html#factual-and-counterfactual",
    "title": "Introduction to Instrumental Variables",
    "section": "Factual and counterfactual",
    "text": "Factual and counterfactual\n\n\n \n\n\n\n\n\n\n\nSuppose we observe an event A\n\nA: a patient is administered a drug, government closes all schools during Covid\n\nWe observe a another event B\n\nB: the patient recovers, virus circulation decreases\n\n\n\n\n\nTo interpret B as a consequence of A, we would like to consider the counter-factual:\n\na patient is not administered a drug, government doesn’t close schools\npatient does not recover, virus circulation is stable\n\n\n\n\n\n\nAn important task in econometrics is to construct a counter-factual\n\nas the name suggests is it sometimes never observed!"
  },
  {
    "objectID": "session_6/slides.html#scientific-experiment",
    "href": "session_6/slides.html#scientific-experiment",
    "title": "Introduction to Instrumental Variables",
    "section": "Scientific Experiment",
    "text": "Scientific Experiment\n\n \nIn science we establish causality by performing experiments\n\nand create the counterfactual\n\nA good experiment is reproducible\n\nsame variables\nsame state of the world (other variables)\nreproduce several times (in case output is noisy or random)\n\nChange one factor at a time\n\nto create a counter-factual"
  },
  {
    "objectID": "session_6/slides.html#measuring-effect-of-treatment",
    "href": "session_6/slides.html#measuring-effect-of-treatment",
    "title": "Introduction to Instrumental Variables",
    "section": "Measuring effect of treatment",
    "text": "Measuring effect of treatment\n\n\n\n\n\n\n\n\n\n\n\nAssume we have discovered two medications: R and B\n\n\n\n\nGive one of them (R) to a patient and observe the outcome\n\n\n\n\nWould would have been the effect of (B) on the same patient?\n\n????\n\n\n\n\n\nWhat if we had many patients and let them choose the medication?\n\n\n\n\n\nMaybe the effect would be the consequence of the choice of patients rather than of the medication?"
  },
  {
    "objectID": "session_6/slides.html#an-exmple-from-behavioural-economics",
    "href": "session_6/slides.html#an-exmple-from-behavioural-economics",
    "title": "Introduction to Instrumental Variables",
    "section": "An exmple from behavioural economics",
    "text": "An exmple from behavioural economics\n\n\n\nExample: cognitive dissonance\n\nExperiment in GATE Lab\nVolunteers play an investment game.\nThey are asked beforehand whether they support OM, PSG, or none.\n\n\n\n\n\nExperiment 1:\n\nBefore the experiment, randomly selected volunteers are given a football shirt of their preferred team (treatment 1)\nOther volunteers receive nothing (treatment 0)\n\nResult:\n\nhaving a football shirt seems to boost investment performance…\n\n\n\n\n\nExperiment 2: subjects are given randomly a shirt of either Olympique de Marseille or PSG.\nResult:\n\nHaving the good shirt improves performance.\nHaving the wrong one deteriorates it badly.\n\n\n\n\n\nHow would you code up this experiment?\nCan we conclude on some form of causality?"
  },
  {
    "objectID": "session_6/slides.html#formalisation-of-the-problem",
    "href": "session_6/slides.html#formalisation-of-the-problem",
    "title": "Introduction to Instrumental Variables",
    "section": "Formalisation of the problem",
    "text": "Formalisation of the problem\n\n\n\n\n\n\nCause (A): two groups of people\n\nthose given a shirt (treatment 1)\nthose not given a shirt (treatment 0)\n\nPossible consequence (B): performance\nTake a given agent Alice: she performs well with a PSG shirt.\n\nmaybe she is a good investor?\nor maybe she is playing for her team?\n\nLet’s try to have her play again without the football shirt\n\nnow the experiment has changed: she has gained experience, is more tired, misses the shirt…\nit is impossible to get a perfect counterfactual (i.e. where only A changes)\n\n\n\n\nLet’s take somebody else then? Bob was really bad without a PSG shirt.\n\nhe might be a bad investor? or he didn’t understand the rules?\nsome other variables have changed, not only the treatment\n\nHow to make a perfect experiment?\n\nChoose randomly whether assigning a shirt or not\nby construction the treatment will not be correlated with other variables"
  },
  {
    "objectID": "session_6/slides.html#randomized-control-trial",
    "href": "session_6/slides.html#randomized-control-trial",
    "title": "Introduction to Instrumental Variables",
    "section": "Randomized Control Trial",
    "text": "Randomized Control Trial\n\n\n\n\n\n\nRandomized Control Trial (RCT)\n\n\nThe best way to ensure that treatment is independent from other factors is to randomize it.\n\n\n\n\n\n\nIn medecine\n\nsome patients receive the treatment (red pill)\nsome other receive the control treatment (blue pill / placebo)\n\nIn economics:\n\nrandomized field experiments\nrandomized phase-ins for new policies\n\nvery useful for policy evaluation\n\n\n\n\n\n\n\nEsther Duflo\n\n\n\n\n\n\nIt is common in economics, instead of assigning treatments randomly, we often say that we assign individuals randomly to the treatment and to the control group. It is equivalent."
  },
  {
    "objectID": "session_6/slides.html#natural-experiment",
    "href": "session_6/slides.html#natural-experiment",
    "title": "Introduction to Instrumental Variables",
    "section": "Natural experiment",
    "text": "Natural experiment\n\n\n\n\n\n\nNatural Experiment\n\n\nA natural experiment satisfies conditions that treatment is assigned randomly\n\nwithout interference by the econometrician\n\n\n\n\n\nAn exemple of a Natural Experiment:\n\ngender bias in french local elections (jean-pierre eymeoud, paul vertier) link\nare women discriminated against by voters in local elections?\n\n\n\n\nResult: yes, they get 1.5% less votes by right-wing voters\n\n\n\n\nWhat was the natural experiment"
  },
  {
    "objectID": "session_6/slides.html#example",
    "href": "session_6/slides.html#example",
    "title": "Introduction to Instrumental Variables",
    "section": "Example",
    "text": "Example\nLifetime Earnings and the Vietnam Era Draft Lottery, by JD Angrist\n\n\n\nFact:\n\nveterans of the vietnam war (55-75) earn (in the 80s) an income that is 15% less in average than those who didn’t go to the war.\nWhat can we conclude?\nHard to say: maybe those sent to the war came back with lower productivity (because of PTSD, public stigma, …)? maybe they were not the most productive in the first place (selection bias)?\n\nProblem (for the economist):\n\nwe didn’t sent people to war randomly\n\n\n\n\nGenius idea:\n\nhere is a variable which randomly affected whether people were sent: the Draft\n\n\nbetween 1947, and 1973, a lottery was run to determine who would go to war\n\nthe draft number was determined, based on date of birth, and first letters of name\n\nand was correlated with the probability that a given person would go to war\nand it was so to say random or at least independent from anything relevant to the problem\n\n\n\n\n\nCan we use the Draft to generate randomness ?"
  },
  {
    "objectID": "session_6/slides.html#problem",
    "href": "session_6/slides.html#problem",
    "title": "Introduction to Instrumental Variables",
    "section": "Problem",
    "text": "Problem\n\nTake the linear regression: \\[y = \\alpha + \\beta x + \\epsilon\\]\n\n\\(y\\): salary\n\\(x\\): went to war\n\nWe want to establish causality from x to y\n\nwe would like to interpret \\(x\\) as the “treatment”\n\nBut there can be confounding factors:\n\nvariable \\(z\\) which causes both x and y\nexemple: socio-economic background, IQ, …\n\nIf we could identify \\(z\\) we could control for it: \\[y = \\alpha + \\beta_1 x + \\beta_2 z + \\epsilon\\]\n\nwe would get a better predictor of \\(y\\) but more uncertainty about \\(\\beta_1\\) (\\(x\\) and \\(z\\) are correlated)"
  },
  {
    "objectID": "session_6/slides.html#reformulate-the-problem",
    "href": "session_6/slides.html#reformulate-the-problem",
    "title": "Introduction to Instrumental Variables",
    "section": "Reformulate the problem",
    "text": "Reformulate the problem\n\n\n\nLet’s assume treatment \\(x\\) is a binary variable \\(\\in{0,1}\\)\nWe want to estimate \\[y = \\alpha + \\beta x + z + \\epsilon\\] where \\(z\\) is potentially correlated to \\(x\\) and \\(y\\)\nThere are two groups:\n\nthose who receive the treatment \\[y = \\alpha + \\beta + z_{T=1} + \\epsilon\\]\nthe others \\[y = \\alpha + 0 +  z_{T=0} + \\epsilon\\]\n\n\n\n\nProblem:\n\nif \\(z\\) is higher in the treatment group, its effect can’t be separated from the treatment effect.\n\nIntuition: what if we make groups differently?\n\ncompletely independent from \\(z\\) (and \\(\\epsilon\\))\nnot independently from \\(x\\) so that one group will receive more treatment than the other\n\nTo make this group we need a new variable \\(q\\) that is:\n\ncorrelated with \\(x\\) so that it will correspond to some treatment effect\nuncorrelated to \\(z\\) or \\(\\epsilon\\) (exogenous)"
  },
  {
    "objectID": "session_6/slides.html#two-stage-regression",
    "href": "session_6/slides.html#two-stage-regression",
    "title": "Introduction to Instrumental Variables",
    "section": "Two stage regression",
    "text": "Two stage regression\n\n\n\nWe would like to redo the treatment groups in a way that is independent from \\(z\\) (and everything contained in \\(\\epsilon\\))\n\n\\(q\\) is a binary variable: drafted or not\n\n\nFirst stage: regress group assignment on the instrument: \\[x = \\alpha_0 + \\beta_0 q + \\eta\\]\n\nwe can now predict group assignment in a way that is independent from \\(z\\) (and everything in \\(\\epsilon\\)) \\[\\tilde{x} = \\alpha_0 + \\beta_0 q\\]\n\n\nSecond stage: use the predicted value instead of the original one \\[y = \\alpha + \\beta_1 \\tilde{x} + z + \\epsilon\\]\n\n\n\n\nResult:\n\nIf \\(\\beta_1\\) is significantly nonzero, there is a causal effect between \\(x\\) and \\(y\\).\nNote that \\(\\tilde{x}\\) is imperfectly correlated with the treatment: \\(\\beta_1\\) can’t be interpreted directly\nThe actual effect will be \\(\\frac{\\beta_1}{\\beta_0}\\) (in 1d)\n\n\nWe say that we instrument \\(x\\) by \\(q\\)."
  },
  {
    "objectID": "session_6/slides.html#choosing-a-good-instrument",
    "href": "session_6/slides.html#choosing-a-good-instrument",
    "title": "Introduction to Instrumental Variables",
    "section": "Choosing a good instrument",
    "text": "Choosing a good instrument\n\n\n\n\n\n\n\nChoosing an instrumental variable\n\n\nA good instrument when trying to explain y by x, is a variable that is correlated to the treatment (x) but does not have any effect on the outcome of interest (y), appart from its effect through x."
  },
  {
    "objectID": "session_6/slides.html#in-practice",
    "href": "session_6/slides.html#in-practice",
    "title": "Introduction to Instrumental Variables",
    "section": "In practice",
    "text": "In practice\n\nBoth statsmodels and linearmodels support instrumental variables\n\nlibrary (look for IV2SLS)\n\nLibrary linearmodels has a handy formula syntax: salary ~ 1 + [war ~ draft]\n\nAPI is similar but not exactly identical to statsmodels\nfor instance linearmodels does not include constants by default\n\nExample from the doc\n\nformula = (\n    \"np.log(drugexp) ~ 1 + totchr + age + linc + blhisp + [hi_empunion ~ ssiratio]\"\n)\nols = IV2SLS.from_formula(formula, data)\nols_res = ols.fit(cov_type=\"robust\")\nprint(ols_res)"
  },
  {
    "objectID": "session_6/index.html",
    "href": "session_6/index.html",
    "title": "Introduction to Instrumental Variables",
    "section": "",
    "text": "Our multilinear regression: \\[y = \\alpha + \\beta x_1 + \\cdots + \\beta x_n\\]\nSo far, we have only considered real variables: (\\(x_i \\in \\mathbb{R}\\)).\n\nex: \\(x_{\\text{gdp}} = \\alpha + \\beta_1 x_{\\text{unemployment}} + \\beta_2 x_{\\text{inflation}}\\)\n\nHow do we deal with the following cases?\n\nbinary variable: \\(x\\in \\{0,1\\}\\) (or \\(\\{True, False}\\))\n\nex: \\(\\text{gonetowar}\\), \\(\\text{hasdegree}\\)\n\ncategorical variable:\n\nex: survey result (0: I don’t know, 1: I strongly disagree, 2: I disagree, 3: I agree, 4: I strongly agree)\nthere is no ranking of answers\nwhen there is ranking: hierarchical index\n\nnonnumerical variables:\n\nex: (flower type: \\(x\\in \\text{myosotis}, \\text{rose}, ...\\))\n\n\n\n\n\n\n\nNothing to be done: just make sure variables take values 0 or 1. \\[y_\\text{salary} = \\alpha + \\beta x_{\\text{gonetowar}}\\]\nInterpretation:\n\nhaving gone to war is associated with a \\(\\beta\\) increase (or decrease?) in salary (still no causality)\n\n\n\n\n\n\nLook at the model: \\[y_{\\text{CO2 emission}} = \\alpha + \\beta x_{\\text{yellow vest support}} \\]\nWhere \\(y_{\\text{CO2 emission}}\\) is an individual’s CO2 emissions and \\(x_{\\text{yellow vest support}}\\) is the response the the question Are you in agreement with the yellow vests demands?.\nResponse is coded up as:\n\n0: Strongly disagree\n1: Disagree\n2: Neutral\n3: Agree\n4: Strongly agree\n\nIf the variable was used directly, how would you intepret the coefficient \\(\\beta\\) ?\n\nindex is hierarchical\nbut the distances between 1 and 2 or 2 and 3 are not comparable…\n\n\n\n\n\n\nWe use one dummy variable per possible answer.\n\n\n\n\n\n\n\n\n\n\n\n\\(D_{\\text{Strongly Disagree}}\\)\n\\(D_{\\text{Disagree}}\\)\n\\(D_{\\text{Neutral}}\\)\n\\(D_{\\text{Agree}}\\)\n\\(D_{\\text{Strongly Agree}}\\)\n\n\n\n\n1\n0\n0\n0\n0\n\n\n0\n1\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n1\n0\n\n\n0\n0\n0\n0\n1\n\n\n\n\nValues are linked by the specific dummy coding.\n\nthe choice of the reference group (with 0) is not completely neutral\n\nfor linear regressions, we can ignore its implications\n\nit must be frequent enough in the data\neffects coding: reference group takes -1 instead of 0\n\nNote that hierarchy is lost. The same treatment can be applied to non-hierachical variables\nNow our variables are perfectly colinear:\n\nwe can deduce one from all the others\nwe drop one from the regression: the reference group TODO\n\n\n\n\n\n\\[y_{\\text{CO2 emission}} = \\alpha + \\beta_1 x_{\\text{strdis}} + \\beta_2 x_{\\text{dis}} + \\beta_3 x_{\\text{agr}} + \\beta_4 x_{\\text{stragr}}\\]\n\nInterpretation:\n\nbeing in the group which strongly agrees to the yellow vest’s claim is associated with an additional \\(\\beta_4\\) increase in CO2 consumption compared with members of the neutral group\n\n\n\n\n\n\n\n\n\nWhat about nonnumerical variables?\n\nWhen variables take nonnumerical variables, we convert them to numerical variables.\n\nExample:\n\n\n\n\nactivity\ncode\n\n\n\n\nmassage therapist\n1\n\n\nmortician\n2\n\n\narcheologist\n3\n\n\nfinancial clerks\n4\n\n\n\n\n\n\nThen we convert to dummy variables exactly like hierarchical indices\n\nhere \\(\\text{massage therapist}\\) is taken as reference\n\n\n\n\n\n\n\n\n\n\n\\(D_{\\text{mortician}}\\)\n\\(D_{\\text{archeologist}}\\)\n\\(D_{\\text{financial clerks}}\\)\n\n\n\n\n1\n0\n0\n\n\n0\n1\n0\n\n\n0\n0\n1\n\n\n\n\n\n\n\n\nUse statsmodels to create dummy variables with formula API.\n\nReplace\n\nsalary ~ activity\n\nby:\n\nsalary ~ C(activity)\nThere is an options to choose the reference group\nsalary ~ C(activity, Treatment(reference=\"archeologist\"))"
  },
  {
    "objectID": "session_6/index.html#data",
    "href": "session_6/index.html#data",
    "title": "Introduction to Instrumental Variables",
    "section": "",
    "text": "Our multilinear regression: \\[y = \\alpha + \\beta x_1 + \\cdots + \\beta x_n\\]\nSo far, we have only considered real variables: (\\(x_i \\in \\mathbb{R}\\)).\n\nex: \\(x_{\\text{gdp}} = \\alpha + \\beta_1 x_{\\text{unemployment}} + \\beta_2 x_{\\text{inflation}}\\)\n\nHow do we deal with the following cases?\n\nbinary variable: \\(x\\in \\{0,1\\}\\) (or \\(\\{True, False}\\))\n\nex: \\(\\text{gonetowar}\\), \\(\\text{hasdegree}\\)\n\ncategorical variable:\n\nex: survey result (0: I don’t know, 1: I strongly disagree, 2: I disagree, 3: I agree, 4: I strongly agree)\nthere is no ranking of answers\nwhen there is ranking: hierarchical index\n\nnonnumerical variables:\n\nex: (flower type: \\(x\\in \\text{myosotis}, \\text{rose}, ...\\))"
  },
  {
    "objectID": "session_6/index.html#binary-variable",
    "href": "session_6/index.html#binary-variable",
    "title": "Introduction to Instrumental Variables",
    "section": "",
    "text": "Nothing to be done: just make sure variables take values 0 or 1. \\[y_\\text{salary} = \\alpha + \\beta x_{\\text{gonetowar}}\\]\nInterpretation:\n\nhaving gone to war is associated with a \\(\\beta\\) increase (or decrease?) in salary (still no causality)"
  },
  {
    "objectID": "session_6/index.html#categorical-variable",
    "href": "session_6/index.html#categorical-variable",
    "title": "Introduction to Instrumental Variables",
    "section": "",
    "text": "Look at the model: \\[y_{\\text{CO2 emission}} = \\alpha + \\beta x_{\\text{yellow vest support}} \\]\nWhere \\(y_{\\text{CO2 emission}}\\) is an individual’s CO2 emissions and \\(x_{\\text{yellow vest support}}\\) is the response the the question Are you in agreement with the yellow vests demands?.\nResponse is coded up as:\n\n0: Strongly disagree\n1: Disagree\n2: Neutral\n3: Agree\n4: Strongly agree\n\nIf the variable was used directly, how would you intepret the coefficient \\(\\beta\\) ?\n\nindex is hierarchical\nbut the distances between 1 and 2 or 2 and 3 are not comparable…"
  },
  {
    "objectID": "session_6/index.html#hierarchical-index-2",
    "href": "session_6/index.html#hierarchical-index-2",
    "title": "Introduction to Instrumental Variables",
    "section": "",
    "text": "We use one dummy variable per possible answer.\n\n\n\n\n\n\n\n\n\n\n\n\\(D_{\\text{Strongly Disagree}}\\)\n\\(D_{\\text{Disagree}}\\)\n\\(D_{\\text{Neutral}}\\)\n\\(D_{\\text{Agree}}\\)\n\\(D_{\\text{Strongly Agree}}\\)\n\n\n\n\n1\n0\n0\n0\n0\n\n\n0\n1\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n1\n0\n\n\n0\n0\n0\n0\n1\n\n\n\n\nValues are linked by the specific dummy coding.\n\nthe choice of the reference group (with 0) is not completely neutral\n\nfor linear regressions, we can ignore its implications\n\nit must be frequent enough in the data\neffects coding: reference group takes -1 instead of 0\n\nNote that hierarchy is lost. The same treatment can be applied to non-hierachical variables\nNow our variables are perfectly colinear:\n\nwe can deduce one from all the others\nwe drop one from the regression: the reference group TODO"
  },
  {
    "objectID": "session_6/index.html#hierarchical-index-3",
    "href": "session_6/index.html#hierarchical-index-3",
    "title": "Introduction to Instrumental Variables",
    "section": "",
    "text": "\\[y_{\\text{CO2 emission}} = \\alpha + \\beta_1 x_{\\text{strdis}} + \\beta_2 x_{\\text{dis}} + \\beta_3 x_{\\text{agr}} + \\beta_4 x_{\\text{stragr}}\\]\n\nInterpretation:\n\nbeing in the group which strongly agrees to the yellow vest’s claim is associated with an additional \\(\\beta_4\\) increase in CO2 consumption compared with members of the neutral group"
  },
  {
    "objectID": "session_6/index.html#nonnumerical-variables",
    "href": "session_6/index.html#nonnumerical-variables",
    "title": "Introduction to Instrumental Variables",
    "section": "",
    "text": "What about nonnumerical variables?\n\nWhen variables take nonnumerical variables, we convert them to numerical variables.\n\nExample:\n\n\n\n\nactivity\ncode\n\n\n\n\nmassage therapist\n1\n\n\nmortician\n2\n\n\narcheologist\n3\n\n\nfinancial clerks\n4\n\n\n\n\n\n\nThen we convert to dummy variables exactly like hierarchical indices\n\nhere \\(\\text{massage therapist}\\) is taken as reference\n\n\n\n\n\n\n\n\n\n\n\\(D_{\\text{mortician}}\\)\n\\(D_{\\text{archeologist}}\\)\n\\(D_{\\text{financial clerks}}\\)\n\n\n\n\n1\n0\n0\n\n\n0\n1\n0\n\n\n0\n0\n1"
  },
  {
    "objectID": "session_6/index.html#hands-on",
    "href": "session_6/index.html#hands-on",
    "title": "Introduction to Instrumental Variables",
    "section": "",
    "text": "Use statsmodels to create dummy variables with formula API.\n\nReplace\n\nsalary ~ activity\n\nby:\n\nsalary ~ C(activity)\nThere is an options to choose the reference group\nsalary ~ C(activity, Treatment(reference=\"archeologist\"))"
  },
  {
    "objectID": "session_6/index.html#what-is-causality",
    "href": "session_6/index.html#what-is-causality",
    "title": "Introduction to Instrumental Variables",
    "section": "What is causality?",
    "text": "What is causality?\n. . .\n\n\n\nGroucho Marx\n\n\nClear? Huh! Why a four-year-old child could understand this report! Run out and find me a four-year-old child, I can’t make head or tail of it."
  },
  {
    "objectID": "session_6/index.html#spurious-correlation",
    "href": "session_6/index.html#spurious-correlation",
    "title": "Introduction to Instrumental Variables",
    "section": "Spurious correlation",
    "text": "Spurious correlation\n\n\n\nSpurious Correlation\n\n\n\nWe have seen spurious correlation before\n\nit happens when two series comove without being actually correlated\n\nAlso, two series might be correlated without one causing the other\n\nex: countries eating more chocolate have more nobel prices…"
  },
  {
    "objectID": "session_6/index.html#definitions",
    "href": "session_6/index.html#definitions",
    "title": "Introduction to Instrumental Variables",
    "section": "Definitions?",
    "text": "Definitions?\nBut how do we define\n\ncorrelation\ncausality\n\n?\nBoth concepts are actually hard to define:\n\nin statistics (and econometrices) they refer to the generating process\nif the data was generated again, would you observe the same relations?\n\nFor instance correlation between \\(X\\) and \\(Y\\) is just the average correlation taken over many draws \\(\\omega\\) of the data: \\[E_{\\omega}\\left[ (X-E[X])(Y-E[Y])\\right]\\]"
  },
  {
    "objectID": "session_6/index.html#how-do-we-define-causality-1",
    "href": "session_6/index.html#how-do-we-define-causality-1",
    "title": "Introduction to Instrumental Variables",
    "section": "How do we define causality (1)",
    "text": "How do we define causality (1)\n\nIn math, we have implication: \\(A \\implies B\\)\n\napplies to statements that can be either true or false\ngiven \\(A\\) and \\(B\\), \\(A\\) implies \\(B\\) unless \\(A\\) is true and \\(B\\) is false\nparadox of the drinker: at any time, there exists a person such that: if this person drinks, then everybody drinks\n\nIn a mathematical universe taking values \\(\\omega\\), we can define causality between statement \\(A(\\omega)\\) and \\(B(\\omega)\\) as : \\[\\forall \\omega, A(\\omega) \\implies B(\\omega)\\]"
  },
  {
    "objectID": "session_6/index.html#how-do-we-define-causality-2",
    "href": "session_6/index.html#how-do-we-define-causality-2",
    "title": "Introduction to Instrumental Variables",
    "section": "How do we define causality (2)",
    "text": "How do we define causality (2)\nBut causality in the real world is problematic\nUsually, we observe \\(A(\\omega)\\) only once…\n. . .\n\n\nExample:\n\nstate of the world \\(\\omega\\): 2008, big financial crisis, …\nA: Ben Bernanke chairman of the Fed\nB: successful economic interventions\nWas Ben Bernanke a good central banker?\nImpossible to say.\n\n\n\n\n\n\n\nThen there is the uncertain concept of time… But let’s take it as granted to not overcomplicate…"
  },
  {
    "objectID": "session_6/index.html#causality-in-statistics",
    "href": "session_6/index.html#causality-in-statistics",
    "title": "Introduction to Instrumental Variables",
    "section": "Causality in Statistics",
    "text": "Causality in Statistics\n\n\n\n\n\n\nStatistical definition of causality\n\n\n\nVariable \\(A\\) causes \\(B\\) in a statistical sense if - \\(A\\) and \\(B\\) are correlated - \\(A\\) is known before \\(B\\) - correlation between \\(A\\) and \\(B\\) is unaffected by other variables\n\n\n\nThere are other related statistical definitions:\n\nlike Granger causality…\n… but not for this course"
  },
  {
    "objectID": "session_6/index.html#factual-and-counterfactual",
    "href": "session_6/index.html#factual-and-counterfactual",
    "title": "Introduction to Instrumental Variables",
    "section": "Factual and counterfactual",
    "text": "Factual and counterfactual\n\n\n \n\n\n\n\n\n\n\nSuppose we observe an event A\n\nA: a patient is administered a drug, government closes all schools during Covid\n\nWe observe a another event B\n\nB: the patient recovers, virus circulation decreases\n\n\n\n\n\nTo interpret B as a consequence of A, we would like to consider the counter-factual:\n\na patient is not administered a drug, government doesn’t close schools\npatient does not recover, virus circulation is stable\n\n\n\n\n\n. . .\nAn important task in econometrics is to construct a counter-factual\n\nas the name suggests is it sometimes never observed!"
  },
  {
    "objectID": "session_6/index.html#scientific-experiment",
    "href": "session_6/index.html#scientific-experiment",
    "title": "Introduction to Instrumental Variables",
    "section": "Scientific Experiment",
    "text": "Scientific Experiment\n\n\n\n \n\n\n\nIn science we establish causality by performing experiments\n\nand create the counterfactual\n\nA good experiment is reproducible\n\nsame variables\nsame state of the world (other variables)\nreproduce several times (in case output is noisy or random)\n\nChange one factor at a time\n\nto create a counter-factual"
  },
  {
    "objectID": "session_6/index.html#measuring-effect-of-treatment",
    "href": "session_6/index.html#measuring-effect-of-treatment",
    "title": "Introduction to Instrumental Variables",
    "section": "Measuring effect of treatment",
    "text": "Measuring effect of treatment\n\n\n\n\n\n\n\n\n\n\n\nAssume we have discovered two medications: R and B\n\n\n\n\nGive one of them (R) to a patient and observe the outcome\n\n\n\n\nWould would have been the effect of (B) on the same patient?\n\n????\n\n\n\n\n\nWhat if we had many patients and let them choose the medication?\n\n\n\n\n. . .\nMaybe the effect would be the consequence of the choice of patients rather than of the medication?"
  },
  {
    "objectID": "session_6/index.html#an-exmple-from-behavioural-economics",
    "href": "session_6/index.html#an-exmple-from-behavioural-economics",
    "title": "Introduction to Instrumental Variables",
    "section": "An exmple from behavioural economics",
    "text": "An exmple from behavioural economics\n\n\n\nExample: cognitive dissonance\n\nExperiment in GATE Lab\nVolunteers play an investment game.\nThey are asked beforehand whether they support OM, PSG, or none.\n\n\n\n\n\nExperiment 1:\n\nBefore the experiment, randomly selected volunteers are given a football shirt of their preferred team (treatment 1)\nOther volunteers receive nothing (treatment 0)\n\nResult:\n\nhaving a football shirt seems to boost investment performance…\n\n\n\n\n\nExperiment 2: subjects are given randomly a shirt of either Olympique de Marseille or PSG.\nResult:\n\nHaving the good shirt improves performance.\nHaving the wrong one deteriorates it badly.\n\n\n\n\n\nHow would you code up this experiment?\nCan we conclude on some form of causality?"
  },
  {
    "objectID": "session_6/index.html#formalisation-of-the-problem",
    "href": "session_6/index.html#formalisation-of-the-problem",
    "title": "Introduction to Instrumental Variables",
    "section": "Formalisation of the problem",
    "text": "Formalisation of the problem\n\n\n\n\n\n\nCause (A): two groups of people\n\nthose given a shirt (treatment 1)\nthose not given a shirt (treatment 0)\n\nPossible consequence (B): performance\nTake a given agent Alice: she performs well with a PSG shirt.\n\nmaybe she is a good investor?\nor maybe she is playing for her team?\n\nLet’s try to have her play again without the football shirt\n\nnow the experiment has changed: she has gained experience, is more tired, misses the shirt…\nit is impossible to get a perfect counterfactual (i.e. where only A changes)\n\n\n\n\nLet’s take somebody else then? Bob was really bad without a PSG shirt.\n\nhe might be a bad investor? or he didn’t understand the rules?\nsome other variables have changed, not only the treatment\n\nHow to make a perfect experiment?\n\nChoose randomly whether assigning a shirt or not\nby construction the treatment will not be correlated with other variables"
  },
  {
    "objectID": "session_6/index.html#randomized-control-trial",
    "href": "session_6/index.html#randomized-control-trial",
    "title": "Introduction to Instrumental Variables",
    "section": "Randomized Control Trial",
    "text": "Randomized Control Trial\n\n\n\n\n\n\nRandomized Control Trial (RCT)\n\n\n\nThe best way to ensure that treatment is independent from other factors is to randomize it.\n\n\n\n\n\nIn medecine\n\nsome patients receive the treatment (red pill)\nsome other receive the control treatment (blue pill / placebo)\n\nIn economics:\n\nrandomized field experiments\nrandomized phase-ins for new policies\n\nvery useful for policy evaluation\n\n\n\n\n\n\n\nEsther Duflo\n\n\n\n\n\n\nIt is common in economics, instead of assigning treatments randomly, we often say that we assign individuals randomly to the treatment and to the control group. It is equivalent."
  },
  {
    "objectID": "session_6/index.html#natural-experiment",
    "href": "session_6/index.html#natural-experiment",
    "title": "Introduction to Instrumental Variables",
    "section": "Natural experiment",
    "text": "Natural experiment\n\n\n\n\n\n\nNatural Experiment\n\n\n\nA natural experiment satisfies conditions that treatment is assigned randomly\n\nwithout interference by the econometrician\n\n\n\n\nAn exemple of a Natural Experiment:\n\ngender bias in french local elections (jean-pierre eymeoud, paul vertier) link\nare women discriminated against by voters in local elections?\n\n\n. . .\n\nResult: yes, they get 1.5% less votes by right-wing voters\n\n. . .\n\nWhat was the natural experiment"
  },
  {
    "objectID": "session_6/index.html#example",
    "href": "session_6/index.html#example",
    "title": "Introduction to Instrumental Variables",
    "section": "Example",
    "text": "Example\nLifetime Earnings and the Vietnam Era Draft Lottery, by JD Angrist\n\n\n\nFact:\n\nveterans of the vietnam war (55-75) earn (in the 80s) an income that is 15% less in average than those who didn’t go to the war.\nWhat can we conclude?\nHard to say: maybe those sent to the war came back with lower productivity (because of PTSD, public stigma, …)? maybe they were not the most productive in the first place (selection bias)?\n\nProblem (for the economist):\n\nwe didn’t sent people to war randomly\n\n\n\n\nGenius idea:\n\nhere is a variable which randomly affected whether people were sent: the Draft\n\n\nbetween 1947, and 1973, a lottery was run to determine who would go to war\n\nthe draft number was determined, based on date of birth, and first letters of name\n\nand was correlated with the probability that a given person would go to war\nand it was so to say random or at least independent from anything relevant to the problem\n\n\n\n\n. . .\nCan we use the Draft to generate randomness ?"
  },
  {
    "objectID": "session_6/index.html#problem",
    "href": "session_6/index.html#problem",
    "title": "Introduction to Instrumental Variables",
    "section": "Problem",
    "text": "Problem\n\nTake the linear regression: \\[y = \\alpha + \\beta x + \\epsilon\\]\n\n\\(y\\): salary\n\\(x\\): went to war\n\nWe want to establish causality from x to y\n\nwe would like to interpret \\(x\\) as the “treatment”\n\nBut there can be confounding factors:\n\nvariable \\(z\\) which causes both x and y\nexemple: socio-economic background, IQ, …\n\nIf we could identify \\(z\\) we could control for it: \\[y = \\alpha + \\beta_1 x + \\beta_2 z + \\epsilon\\]\n\nwe would get a better predictor of \\(y\\) but more uncertainty about \\(\\beta_1\\) (\\(x\\) and \\(z\\) are correlated)"
  },
  {
    "objectID": "session_6/index.html#reformulate-the-problem",
    "href": "session_6/index.html#reformulate-the-problem",
    "title": "Introduction to Instrumental Variables",
    "section": "Reformulate the problem",
    "text": "Reformulate the problem\n\n\n\nLet’s assume treatment \\(x\\) is a binary variable \\(\\in{0,1}\\)\nWe want to estimate \\[y = \\alpha + \\beta x + z + \\epsilon\\] where \\(z\\) is potentially correlated to \\(x\\) and \\(y\\)\nThere are two groups:\n\nthose who receive the treatment \\[y = \\alpha + \\beta + z_{T=1} + \\epsilon\\]\nthe others \\[y = \\alpha + 0 +  z_{T=0} + \\epsilon\\]\n\n\n\n\nProblem:\n\nif \\(z\\) is higher in the treatment group, its effect can’t be separated from the treatment effect.\n\nIntuition: what if we make groups differently?\n\ncompletely independent from \\(z\\) (and \\(\\epsilon\\))\nnot independently from \\(x\\) so that one group will receive more treatment than the other\n\nTo make this group we need a new variable \\(q\\) that is:\n\ncorrelated with \\(x\\) so that it will correspond to some treatment effect\nuncorrelated to \\(z\\) or \\(\\epsilon\\) (exogenous)"
  },
  {
    "objectID": "session_6/index.html#two-stage-regression",
    "href": "session_6/index.html#two-stage-regression",
    "title": "Introduction to Instrumental Variables",
    "section": "Two stage regression",
    "text": "Two stage regression\n\n\n\nWe would like to redo the treatment groups in a way that is independent from \\(z\\) (and everything contained in \\(\\epsilon\\))\n\n\\(q\\) is a binary variable: drafted or not\n\n\nFirst stage: regress group assignment on the instrument: \\[x = \\alpha_0 + \\beta_0 q + \\eta\\]\n\nwe can now predict group assignment in a way that is independent from \\(z\\) (and everything in \\(\\epsilon\\)) \\[\\tilde{x} = \\alpha_0 + \\beta_0 q\\]\n\n\nSecond stage: use the predicted value instead of the original one \\[y = \\alpha + \\beta_1 \\tilde{x} + z + \\epsilon\\]\n\n\n\n\nResult:\n\nIf \\(\\beta_1\\) is significantly nonzero, there is a causal effect between \\(x\\) and \\(y\\).\nNote that \\(\\tilde{x}\\) is imperfectly correlated with the treatment: \\(\\beta_1\\) can’t be interpreted directly\nThe actual effect will be \\(\\frac{\\beta_1}{\\beta_0}\\) (in 1d)\n\n\nWe say that we instrument \\(x\\) by \\(q\\)."
  },
  {
    "objectID": "session_6/index.html#choosing-a-good-instrument",
    "href": "session_6/index.html#choosing-a-good-instrument",
    "title": "Introduction to Instrumental Variables",
    "section": "Choosing a good instrument",
    "text": "Choosing a good instrument\n\n\n\n\n\n\n\nChoosing an instrumental variable\n\n\n\nA good instrument when trying to explain y by x, is a variable that is correlated to the treatment (x) but does not have any effect on the outcome of interest (y), appart from its effect through x."
  },
  {
    "objectID": "session_6/index.html#in-practice",
    "href": "session_6/index.html#in-practice",
    "title": "Introduction to Instrumental Variables",
    "section": "In practice",
    "text": "In practice\n\nBoth statsmodels and linearmodels support instrumental variables\n\nlibrary (look for IV2SLS)\n\nLibrary linearmodels has a handy formula syntax: salary ~ 1 + [war ~ draft]\n\nAPI is similar but not exactly identical to statsmodels\nfor instance linearmodels does not include constants by default\n\nExample from the doc\n\nformula = (\n    \"np.log(drugexp) ~ 1 + totchr + age + linc + blhisp + [hi_empunion ~ ssiratio]\"\n)\nols = IV2SLS.from_formula(formula, data)\nols_res = ols.fit(cov_type=\"robust\")\nprint(ols_res)"
  },
  {
    "objectID": "session_6/instrumental_variables.html",
    "href": "session_6/instrumental_variables.html",
    "title": "Instrumental variables",
    "section": "",
    "text": "Go back to the first notebook. Learn how to write functions and loops.\n\nimport pandas\n\n\nfname = \"dataset.csv\"\ndf = pandas.read_csv(fname)\ndisplay(df.describe())\ndf\n\n\n\n\n\n\n\n\ngdp\ndate\n\n\n\n\ncount\n4.000000\n4.00000\n\n\nmean\n552.500000\n2000.50000\n\n\nstd\n519.623261\n0.57735\n\n\nmin\n100.000000\n2000.00000\n\n\n25%\n103.750000\n2000.00000\n\n\n50%\n552.500000\n2000.50000\n\n\n75%\n1001.250000\n2001.00000\n\n\nmax\n1005.000000\n2001.00000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncountry\ngdp\ndate\n\n\n\n\n0\nusa\n1000\n2000\n\n\n1\nusa\n1005\n2001\n\n\n2\nfrance\n100\n2000\n\n\n3\nfrance\n105\n2001\n\n\n\n\n\n\n\n\ndef import_and_print(fname, print_statistics=True):\n    # here is the body of the function\n    df = pandas.read_csv(fname)\n    if print_statistics==True:\n        display(\"Summary Statistics\")\n        display(df.describe())\n    return df\n\n\nimport_and_print(\"dataset.csv\")\n\n'Summary Statistics'\n\n\n\n\n\n\n\n\n\ngdp\ndate\n\n\n\n\ncount\n4.000000\n4.00000\n\n\nmean\n552.500000\n2000.50000\n\n\nstd\n519.623261\n0.57735\n\n\nmin\n100.000000\n2000.00000\n\n\n25%\n103.750000\n2000.00000\n\n\n50%\n552.500000\n2000.50000\n\n\n75%\n1001.250000\n2001.00000\n\n\nmax\n1005.000000\n2001.00000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncountry\ngdp\ndate\n\n\n\n\n0\nusa\n1000\n2000\n\n\n1\nusa\n1005\n2001\n\n\n2\nfrance\n100\n2000\n\n\n3\nfrance\n105\n2001\n\n\n\n\n\n\n\n\nimport_and_print(\"dataset_2.csv\", False)\n\n\n\n\n\n\n\n\ncountry\ngdp\ndate\n\n\n\n\n0\nusa\n1000\n2000\n\n\n1\nusa\n1005\n2001\n\n\n2\nfrance\n100\n2000\n\n\n3\nfrance\n105\n2001\n\n\n\n\n\n\n\n\ndef f(x): # no side effect\n    return x**2 + 1\n\n\nimport time\n\n\ndef g(x): # that one has side effects\n    print(\"Calculating...\")\n    time.sleep(10)\n    return x**2 + 1\n\n\ng(1)\n\nCalculating...\n\n\n2\n\n\n\ndef h(x): # that one has only side effects\n    print(\"Calculating...\")\n    time.sleep(10)\n    y =  x**2 + 1\n    print(f\"Found it! {y}\")\n\n\nh(1)\n\nCalculating...\nFound it! 2\n\n\nDocumenting code:\n\nadding comments: # ...\nadd docstrings: just after the function name, a string explaining what the function does\n\n\ndef import_and_print(fname, print_statistics=True):\n    \"Import a dataframe from a filename, print the main statistics and return the dataframe.\"\n    \n    \n    # import the file to get a dataframe\n    df = pandas.read_csv(fname)\n    \n    if print_statistics==True:\n        display(\"Summary Statistics\")\n        # we print default summary statistics computed by pandas\n        display(df.describe())\n        \n    return df\n\n\nimport_and_print?\n\n\nSignature: import_and_print(fname, print_statistics=True)\nDocstring: Import a dataframe from a filename, print the main statistics and return the dataframe.\nFile:      ~/Teaching/dbe/session_6/&lt;ipython-input-32-fb18d0d82cec&gt;\nType:      function\n\n\n\n\n\ndef import_and_print(fname, print_statistics=True):\n    \"\"\"Import a dataframe from a filename.\n    \n    fname (string): filename\n    print_statistics (boolean): if True print summary statistics\n    \n    \"\"\"\n    \n    \n    # import the file to get a dataframe\n    df = pandas.read_csv(fname)\n    \n    if print_statistics==True:\n        display(\"Summary Statistics\")\n        # we print default summary statistics computed by pandas\n        display(df.describe())\n        \n    return df\n\n\nimport_and_print?\n\n\nSignature: import_and_print(fname, print_statistics=True)\nDocstring:\nImport a dataframe from a filename.\nfname (string): filename\nprint_statistics (boolean): if True print summary statistics\nFile:      ~/Teaching/dbe/session_6/&lt;ipython-input-35-97c3580f10df&gt;\nType:      function\n\n\n\n\n\n(lambda x: x**2-1)   (3)\n\n8\n\n\n\nf = (lambda x: x**2-1)\nf(3)\n\n8\n\n\n\n## anonymous functions are useful for the groupby pandas function\n\n\ndf\n\n\n\n\n\n\n\n\ncountry\ngdp\ndate\n\n\n\n\n0\nusa\n1000\n2000\n\n\n1\nusa\n1005\n2001\n\n\n2\nfrance\n100\n2000\n\n\n3\nfrance\n105\n2001\n\n\n\n\n\n\n\n\n# naive approach\nl = []\nfor country in df[\"country\"].unique():\n    print(f\"Selecting country {country}\")\n    sel = df[\"country\"]==country\n    sdf = df[sel] # sub dataframe with the right country\n    print( sdf.mean() )\n    l.append(df_s.mean())\n\nSelecting country usa\n gdp     1002.5\n date    2000.5\ndtype: float64\nSelecting country france\n gdp      102.5\n date    2000.5\ndtype: float64\n\n\n\ndef todo(sdf): print( sdf.mean())\n\n\ndf.groupby(\"country\").apply( todo )\n\n gdp      102.5\n date    2000.5\ndtype: float64\n gdp     1002.5\n date    2000.5\ndtype: float64\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf.groupby(\"country\").apply( lambda sdf: (sdf.mean()) )\n\n\n\n\n\n\n\n\ngdp\ndate\n\n\ncountry\n\n\n\n\n\n\nfrance\n102.5\n2000.5\n\n\nusa\n1002.5\n2000.5"
  },
  {
    "objectID": "session_6/instrumental_variables.html#functions-and-loops",
    "href": "session_6/instrumental_variables.html#functions-and-loops",
    "title": "Instrumental variables",
    "section": "",
    "text": "Go back to the first notebook. Learn how to write functions and loops.\n\nimport pandas\n\n\nfname = \"dataset.csv\"\ndf = pandas.read_csv(fname)\ndisplay(df.describe())\ndf\n\n\n\n\n\n\n\n\ngdp\ndate\n\n\n\n\ncount\n4.000000\n4.00000\n\n\nmean\n552.500000\n2000.50000\n\n\nstd\n519.623261\n0.57735\n\n\nmin\n100.000000\n2000.00000\n\n\n25%\n103.750000\n2000.00000\n\n\n50%\n552.500000\n2000.50000\n\n\n75%\n1001.250000\n2001.00000\n\n\nmax\n1005.000000\n2001.00000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncountry\ngdp\ndate\n\n\n\n\n0\nusa\n1000\n2000\n\n\n1\nusa\n1005\n2001\n\n\n2\nfrance\n100\n2000\n\n\n3\nfrance\n105\n2001\n\n\n\n\n\n\n\n\ndef import_and_print(fname, print_statistics=True):\n    # here is the body of the function\n    df = pandas.read_csv(fname)\n    if print_statistics==True:\n        display(\"Summary Statistics\")\n        display(df.describe())\n    return df\n\n\nimport_and_print(\"dataset.csv\")\n\n'Summary Statistics'\n\n\n\n\n\n\n\n\n\ngdp\ndate\n\n\n\n\ncount\n4.000000\n4.00000\n\n\nmean\n552.500000\n2000.50000\n\n\nstd\n519.623261\n0.57735\n\n\nmin\n100.000000\n2000.00000\n\n\n25%\n103.750000\n2000.00000\n\n\n50%\n552.500000\n2000.50000\n\n\n75%\n1001.250000\n2001.00000\n\n\nmax\n1005.000000\n2001.00000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncountry\ngdp\ndate\n\n\n\n\n0\nusa\n1000\n2000\n\n\n1\nusa\n1005\n2001\n\n\n2\nfrance\n100\n2000\n\n\n3\nfrance\n105\n2001\n\n\n\n\n\n\n\n\nimport_and_print(\"dataset_2.csv\", False)\n\n\n\n\n\n\n\n\ncountry\ngdp\ndate\n\n\n\n\n0\nusa\n1000\n2000\n\n\n1\nusa\n1005\n2001\n\n\n2\nfrance\n100\n2000\n\n\n3\nfrance\n105\n2001\n\n\n\n\n\n\n\n\ndef f(x): # no side effect\n    return x**2 + 1\n\n\nimport time\n\n\ndef g(x): # that one has side effects\n    print(\"Calculating...\")\n    time.sleep(10)\n    return x**2 + 1\n\n\ng(1)\n\nCalculating...\n\n\n2\n\n\n\ndef h(x): # that one has only side effects\n    print(\"Calculating...\")\n    time.sleep(10)\n    y =  x**2 + 1\n    print(f\"Found it! {y}\")\n\n\nh(1)\n\nCalculating...\nFound it! 2\n\n\nDocumenting code:\n\nadding comments: # ...\nadd docstrings: just after the function name, a string explaining what the function does\n\n\ndef import_and_print(fname, print_statistics=True):\n    \"Import a dataframe from a filename, print the main statistics and return the dataframe.\"\n    \n    \n    # import the file to get a dataframe\n    df = pandas.read_csv(fname)\n    \n    if print_statistics==True:\n        display(\"Summary Statistics\")\n        # we print default summary statistics computed by pandas\n        display(df.describe())\n        \n    return df\n\n\nimport_and_print?\n\n\nSignature: import_and_print(fname, print_statistics=True)\nDocstring: Import a dataframe from a filename, print the main statistics and return the dataframe.\nFile:      ~/Teaching/dbe/session_6/&lt;ipython-input-32-fb18d0d82cec&gt;\nType:      function\n\n\n\n\n\ndef import_and_print(fname, print_statistics=True):\n    \"\"\"Import a dataframe from a filename.\n    \n    fname (string): filename\n    print_statistics (boolean): if True print summary statistics\n    \n    \"\"\"\n    \n    \n    # import the file to get a dataframe\n    df = pandas.read_csv(fname)\n    \n    if print_statistics==True:\n        display(\"Summary Statistics\")\n        # we print default summary statistics computed by pandas\n        display(df.describe())\n        \n    return df\n\n\nimport_and_print?\n\n\nSignature: import_and_print(fname, print_statistics=True)\nDocstring:\nImport a dataframe from a filename.\nfname (string): filename\nprint_statistics (boolean): if True print summary statistics\nFile:      ~/Teaching/dbe/session_6/&lt;ipython-input-35-97c3580f10df&gt;\nType:      function\n\n\n\n\n\n(lambda x: x**2-1)   (3)\n\n8\n\n\n\nf = (lambda x: x**2-1)\nf(3)\n\n8\n\n\n\n## anonymous functions are useful for the groupby pandas function\n\n\ndf\n\n\n\n\n\n\n\n\ncountry\ngdp\ndate\n\n\n\n\n0\nusa\n1000\n2000\n\n\n1\nusa\n1005\n2001\n\n\n2\nfrance\n100\n2000\n\n\n3\nfrance\n105\n2001\n\n\n\n\n\n\n\n\n# naive approach\nl = []\nfor country in df[\"country\"].unique():\n    print(f\"Selecting country {country}\")\n    sel = df[\"country\"]==country\n    sdf = df[sel] # sub dataframe with the right country\n    print( sdf.mean() )\n    l.append(df_s.mean())\n\nSelecting country usa\n gdp     1002.5\n date    2000.5\ndtype: float64\nSelecting country france\n gdp      102.5\n date    2000.5\ndtype: float64\n\n\n\ndef todo(sdf): print( sdf.mean())\n\n\ndf.groupby(\"country\").apply( todo )\n\n gdp      102.5\n date    2000.5\ndtype: float64\n gdp     1002.5\n date    2000.5\ndtype: float64\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf.groupby(\"country\").apply( lambda sdf: (sdf.mean()) )\n\n\n\n\n\n\n\n\ngdp\ndate\n\n\ncountry\n\n\n\n\n\n\nfrance\n102.5\n2000.5\n\n\nusa\n1002.5\n2000.5"
  },
  {
    "objectID": "session_6/instrumental_variables.html#baby-example-on-mock-dataset",
    "href": "session_6/instrumental_variables.html#baby-example-on-mock-dataset",
    "title": "Instrumental variables",
    "section": "Baby example on mock dataset",
    "text": "Baby example on mock dataset\n\nConstructing the dataset\nCreate four random series of length \\(N=1000\\)\n\n\\(x\\): education\n\\(y\\): salary\n\\(z\\): ambition\n\\(q\\): early smoking\n\nsuch that:\n\n\\(x\\) and \\(z\\) cause \\(y\\)\n\\(z\\) causes \\(x\\)\n\\(q\\) is correlated with \\(x\\), not with \\(z\\)\n\n(all relations are linear, add random shocks where needed)\nCreate a dataset df\n\nimport numpy\n\n\nN = 100000\n\n\nϵ_z = numpy.random.randn(N)*0.01\nϵ_x = numpy.random.randn(N)*0.01\nϵ_q = numpy.random.randn(N)*0.01\nϵ_y = numpy.random.randn(N)*0.01\n\n\nz = 0.1 + ϵ_z\nx = 0.1 + z + ϵ_x\nq = 0.5 + 0.1234*ϵ_x + ϵ_q\ny  = 1.0 + 0.9*x + 0.4*z + ϵ_y\n\n\ndf = pandas.DataFrame({\n    \"x\": x,\n    \"y\": y,\n    \"z\": z,\n    \"q\": q\n})\n\n\ndf.corr()\n\n\n\n\n\n\n\n\nx\ny\nz\nq\n\n\n\n\nx\n1.000000\n0.831152\n0.708497\n0.079905\n\n\ny\n0.831152\n1.000000\n0.694133\n0.054070\n\n\nz\n0.708497\n0.694133\n1.000000\n-0.006022\n\n\nq\n0.079905\n0.054070\n-0.006022\n1.000000\n\n\n\n\n\n\n\n\n\nNaive approach\nRun a regression to estimate the effect of \\(x\\) on \\(y\\). Control by \\(z\\). What happens ?\n\nimport linearmodels\nfrom statsmodels.formula import api\n\n\nmodel = api.ols(\"y ~ x\", df)\nres = model.fit()\nres.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\ny\nR-squared:\n0.691\n\n\nModel:\nOLS\nAdj. R-squared:\n0.691\n\n\nMethod:\nLeast Squares\nF-statistic:\n2.234e+05\n\n\nDate:\nWed, 09 Mar 2022\nProb (F-statistic):\n0.00\n\n\nTime:\n12:14:59\nLog-Likelihood:\n3.1488e+05\n\n\nNo. Observations:\n100000\nAIC:\n-6.298e+05\n\n\nDf Residuals:\n99998\nBIC:\n-6.297e+05\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n1.0005\n0.000\n2150.251\n0.000\n1.000\n1.001\n\n\nx\n1.0974\n0.002\n472.679\n0.000\n1.093\n1.102\n\n\n\n\n\n\nOmnibus:\n2.727\nDurbin-Watson:\n1.991\n\n\nProb(Omnibus):\n0.256\nJarque-Bera (JB):\n2.727\n\n\nSkew:\n0.008\nProb(JB):\n0.256\n\n\nKurtosis:\n3.019\nCond. No.\n73.5\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nmodel = api.ols(\"y ~ x + z\", df)\nres = model.fit()\nres.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\ny\nR-squared:\n0.713\n\n\nModel:\nOLS\nAdj. R-squared:\n0.713\n\n\nMethod:\nLeast Squares\nF-statistic:\n1.243e+05\n\n\nDate:\nWed, 09 Mar 2022\nProb (F-statistic):\n0.00\n\n\nTime:\n12:15:00\nLog-Likelihood:\n3.1862e+05\n\n\nNo. Observations:\n100000\nAIC:\n-6.372e+05\n\n\nDf Residuals:\n99997\nBIC:\n-6.372e+05\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n1.0007\n0.000\n2232.389\n0.000\n1.000\n1.002\n\n\nx\n0.8997\n0.003\n283.880\n0.000\n0.893\n0.906\n\n\nz\n0.3938\n0.004\n88.054\n0.000\n0.385\n0.403\n\n\n\n\n\n\nOmnibus:\n2.380\nDurbin-Watson:\n1.990\n\n\nProb(Omnibus):\n0.304\nJarque-Bera (JB):\n2.379\n\n\nSkew:\n0.007\nProb(JB):\n0.304\n\n\nKurtosis:\n3.019\nCond. No.\n166.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\nInstrumental variable\nUse \\(q\\) to instrument the effect of x on y. Comment.\n\n# difference between linearmodels and statsmodels:\n# linearmodels does not include the constant by defulat\n\n\nfrom linearmodels import IV2SLS\n\n\nfrom linearmodels import IV2SLS\nformula = (\n    \"y ~ 1 + [x ~ q]\"\n)\nmod = IV2SLS.from_formula(formula, df)\nres = mod.fit()\nres\n\n\nIV-2SLS Estimation Summary\n\n\nDep. Variable:\ny\nR-squared:\n0.6670\n\n\nEstimator:\nIV-2SLS\nAdj. R-squared:\n0.6669\n\n\nNo. Observations:\n100000\nF-statistic:\n883.43\n\n\nDate:\nWed, Mar 09 2022\nP-value (F-stat)\n0.0000\n\n\nTime:\n12:15:00\nDistribution:\nchi2(1)\n\n\nCov. Estimator:\nrobust\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\nIntercept\n1.0413\n0.0060\n173.27\n0.0000\n1.0295\n1.0531\n\n\nx\n0.8934\n0.0301\n29.723\n0.0000\n0.8345\n0.9523\n\n\n\nEndogenous: xInstruments: qRobust Covariance (Heteroskedastic)Debiased: Falseid: 0x7f09adece490"
  },
  {
    "objectID": "session_6/instrumental_variables.html#return-on-education",
    "href": "session_6/instrumental_variables.html#return-on-education",
    "title": "Instrumental variables",
    "section": "Return on Education",
    "text": "Return on Education\nWe follow the excellent R tutorial from the (excellent) Econometrics with R book.\nThe goal is to measure the effect of schooling on earnings, while correcting the endogeneity bias by using distance to college as an instrument.\nDownload the college distance and make a nice dataframe. Describe the dataset. Plot an histogram of distance.\nhttps://vincentarelbundock.github.io/Rdatasets/datasets.html\n\nimport statsmodels.api as sm\n\n\nsm.datasets.get_rdataset?\n\n\nSignature: sm.datasets.get_rdataset(dataname, package='datasets', cache=False)\nDocstring:\ndownload and return R dataset\nParameters\n----------\ndataname : str\n    The name of the dataset you want to download\npackage : str\n    The package in which the dataset is found. The default is the core\n    'datasets' package.\ncache : bool or str\n    If True, will download this data into the STATSMODELS_DATA folder.\n    The default location is a folder called statsmodels_data in the\n    user home folder. Otherwise, you can specify a path to a folder to\n    use for caching the data. If False, the data will not be cached.\nReturns\n-------\ndataset : Dataset\n    A `statsmodels.data.utils.Dataset` instance. This objects has\n    attributes:\n    * data - A pandas DataFrame containing the data\n    * title - The dataset title\n    * package - The package from which the data came\n    * from_cache - Whether not cached data was retrieved\n    * __doc__ - The verbatim R documentation.\nNotes\n-----\nIf the R dataset has an integer index. This is reset to be zero-based.\nOtherwise the index is preserved. The caching facilities are dumb. That\nis, no download dates, e-tags, or otherwise identifying information\nis checked to see if the data should be downloaded again or not. If the\ndataset is in the cache, it's used.\nFile:      ~/.local/opt/miniconda/lib/python3.8/site-packages/statsmodels/datasets/utils.py\nType:      function\n\n\n\n\n\nds = sm.datasets.get_rdataset(\"CollegeDistance\", \"AER\")\n\n\ndf = ds.data\n\n\nds.title\n\n'College Distance Data'\n\n\n\ndf.head()\n\n\n\n\n\n\n\n\ngender\nethnicity\nscore\nfcollege\nmcollege\nhome\nurban\nunemp\nwage\ndistance\ntuition\neducation\nincome\nregion\n\n\n\n\n1\nmale\nother\n39.150002\nyes\nno\nyes\nyes\n6.2\n8.09\n0.2\n0.88915\n12\nhigh\nother\n\n\n2\nfemale\nother\n48.869999\nno\nno\nyes\nyes\n6.2\n8.09\n0.2\n0.88915\n12\nlow\nother\n\n\n3\nmale\nother\n48.740002\nno\nno\nyes\nyes\n6.2\n8.09\n0.2\n0.88915\n12\nlow\nother\n\n\n4\nmale\nafam\n40.400002\nno\nno\nyes\nyes\n6.2\n8.09\n0.2\n0.88915\n12\nlow\nother\n\n\n5\nfemale\nother\n40.480000\nno\nno\nno\nyes\n5.6\n8.09\n0.4\n0.88915\n13\nlow\nother\n\n\n\n\n\n\n\n\n# wage looks constant. Check there is actually some variability\ndf.describe()\n\n\n\n\n\n\n\n\nscore\nunemp\nwage\ndistance\ntuition\neducation\n\n\n\n\ncount\n4739.000000\n4739.000000\n4739.000000\n4739.000000\n4739.000000\n4739.000000\n\n\nmean\n50.889029\n7.597215\n9.500506\n1.802870\n0.814608\n13.807765\n\n\nstd\n8.701910\n2.763581\n1.343067\n2.297128\n0.339504\n1.789107\n\n\nmin\n28.950001\n1.400000\n6.590000\n0.000000\n0.257510\n12.000000\n\n\n25%\n43.924999\n5.900000\n8.850000\n0.400000\n0.484990\n12.000000\n\n\n50%\n51.189999\n7.100000\n9.680000\n1.000000\n0.824480\n13.000000\n\n\n75%\n57.769999\n8.900000\n10.150000\n2.500000\n1.127020\n16.000000\n\n\nmax\n72.809998\n24.900000\n12.960000\n20.000000\n1.404160\n18.000000\n\n\n\n\n\n\n\n\nfrom matplotlib import pyplot as plt\n\n\nplt.hist(df['distance'], bins=50)\n\n(array([1091.,  923.,  601.,  534.,   92.,  277.,  219.,  126.,  173.,\n          36.,  168.,   88.,  126.,   17.,    0.,   43.,   47.,   48.,\n          10.,    8.,   19.,    0.,   26.,    0.,    0.,    6.,    4.,\n           7.,    0.,    0.,   10.,    0.,    0.,    0.,    0.,    4.,\n           0.,   15.,    0.,    0.,   17.,    0.,    0.,    0.,    0.,\n           0.,    0.,    0.,    0.,    4.]),\n array([ 0. ,  0.4,  0.8,  1.2,  1.6,  2. ,  2.4,  2.8,  3.2,  3.6,  4. ,\n         4.4,  4.8,  5.2,  5.6,  6. ,  6.4,  6.8,  7.2,  7.6,  8. ,  8.4,\n         8.8,  9.2,  9.6, 10. , 10.4, 10.8, 11.2, 11.6, 12. , 12.4, 12.8,\n        13.2, 13.6, 14. , 14.4, 14.8, 15.2, 15.6, 16. , 16.4, 16.8, 17.2,\n        17.6, 18. , 18.4, 18.8, 19.2, 19.6, 20. ]),\n &lt;BarContainer object of 50 artists&gt;)\n\n\n\n\n\nRun the naive regression \\(income=\\beta_0 + \\beta_1 \\text{education} + u\\)\n\nimport statsmodels.formula.api as api\n\n\napi.ols(\"C(income) ~ education\", df)\n\nValueError: endog has evaluated to an array with multiple columns that has shape (4739, 2). This occurs when the variable converted to endog is non-numeric (e.g., bool or str).\n\n\n\n# education variable takes string values (\"high\" or \"low\"). \n# we need to convert them into 1 and 0 first\ndf['income_binary'] = (df['income'] == \"high\")*1\n\n\nmodel = api.ols(\"income_binary ~ education\", df)\nresult = model.fit()\n\n\nresult.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nincome_binary\nR-squared:\n0.048\n\n\nModel:\nOLS\nAdj. R-squared:\n0.048\n\n\nMethod:\nLeast Squares\nF-statistic:\n239.0\n\n\nDate:\nTue, 15 Mar 2022\nProb (F-statistic):\n1.22e-52\n\n\nTime:\n09:34:48\nLog-Likelihood:\n-2853.5\n\n\nNo. Observations:\n4739\nAIC:\n5711.\n\n\nDf Residuals:\n4737\nBIC:\n5724.\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n-0.4780\n0.050\n-9.567\n0.000\n-0.576\n-0.380\n\n\neducation\n0.0555\n0.004\n15.460\n0.000\n0.048\n0.063\n\n\n\n\n\n\nOmnibus:\n1510.859\nDurbin-Watson:\n1.791\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n795.036\n\n\nSkew:\n0.871\nProb(JB):\n2.29e-173\n\n\nKurtosis:\n2.003\nCond. No.\n109.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe p-value associated to the Fisher statistics is negligible: the model is globally significant. Predictive power is very low (R^2 ~ 5%): the effect of education on income is small w.r.t. to the effect of other factors.\nCoefficients for interecept and education terms are significant at the 0.1% threshold.\n\ndf.head()\n\n\n\n\n\n\n\n\ngender\nethnicity\nscore\nfcollege\nmcollege\nhome\nurban\nunemp\nwage\ndistance\ntuition\neducation\nincome\nregion\nincome_binary\n\n\n\n\n1\nmale\nother\n39.150002\nyes\nno\nyes\nyes\n6.2\n8.09\n0.2\n0.88915\n12\nhigh\nother\n1\n\n\n2\nfemale\nother\n48.869999\nno\nno\nyes\nyes\n6.2\n8.09\n0.2\n0.88915\n12\nlow\nother\n0\n\n\n3\nmale\nother\n48.740002\nno\nno\nyes\nyes\n6.2\n8.09\n0.2\n0.88915\n12\nlow\nother\n0\n\n\n4\nmale\nafam\n40.400002\nno\nno\nyes\nyes\n6.2\n8.09\n0.2\n0.88915\n12\nlow\nother\n0\n\n\n5\nfemale\nother\n40.480000\nno\nno\nno\nyes\n5.6\n8.09\n0.4\n0.88915\n13\nlow\nother\n0\n\n\n\n\n\n\n\n\ndf['ethnicity'].unique()\n\narray(['other', 'afam', 'hispanic'], dtype=object)\n\n\nAugment the regression with unemp, hispanic, af-am, female and urban\n\nfrom patsy import Treatment\n\n\nmodel = api.ols(\"income_binary ~ education + C(gender,Treatment(reference='male')) + C(ethnicity,Treatment(reference='other')) + urban + unemp\", df)\nresult = model.fit()\n\n\nresult.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nincome_binary\nR-squared:\n0.083\n\n\nModel:\nOLS\nAdj. R-squared:\n0.082\n\n\nMethod:\nLeast Squares\nF-statistic:\n71.34\n\n\nDate:\nTue, 15 Mar 2022\nProb (F-statistic):\n2.00e-85\n\n\nTime:\n09:51:02\nLog-Likelihood:\n-2764.9\n\n\nNo. Observations:\n4739\nAIC:\n5544.\n\n\nDf Residuals:\n4732\nBIC:\n5589.\n\n\nDf Model:\n6\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n-0.2433\n0.054\n-4.528\n0.000\n-0.349\n-0.138\n\n\nC(gender, Treatment(reference='male'))[T.female]\n-0.0490\n0.013\n-3.861\n0.000\n-0.074\n-0.024\n\n\nC(ethnicity, Treatment(reference='other'))[T.afam]\n-0.1235\n0.018\n-6.920\n0.000\n-0.159\n-0.089\n\n\nC(ethnicity, Treatment(reference='other'))[T.hispanic]\n-0.1532\n0.017\n-9.197\n0.000\n-0.186\n-0.121\n\n\nurban[T.yes]\n-0.0470\n0.015\n-3.073\n0.002\n-0.077\n-0.017\n\n\neducation\n0.0511\n0.004\n14.422\n0.000\n0.044\n0.058\n\n\nunemp\n-0.0115\n0.002\n-5.006\n0.000\n-0.016\n-0.007\n\n\n\n\n\n\nOmnibus:\n1212.294\nDurbin-Watson:\n1.836\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n697.989\n\n\nSkew:\n0.813\nProb(JB):\n2.71e-152\n\n\nKurtosis:\n2.055\nCond. No.\n136.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nComment the results and explain the selection problem\nAll coefficients are significant at the 1% level. Prediction power is higher : R^2 about 8%.\nExplain why distance to college might be used to instrument the effect of schooling.\nWe need an instrument that:\n\nis correlated with schooling:\n\ndistance to college affects chances to go to university hence schooling\n\nindependent from other factors (gender, ethnicity, …)\n\nThe effect of “distance to college” on income, is only through its effect on education.\nRun an IV regression, where distance is used to instrument schooling.\nlook at: https://bashtage.github.io/linearmodels/ (two-stage least squares)\n\n# remember that linearmodels does not include constants by default\n# we take the same formula and add the constant\n\n\nfrom linearmodels import IV2SLS\nformula = (\n\"income_binary ~ 1 + [education~distance] + C(gender,Treatment(reference='male')) + C(ethnicity,Treatment(reference='other')) + urban + unemp\"\n)\nmod = IV2SLS.from_formula(formula, df)\nres = mod.fit()\nres\n\n\nIV-2SLS Estimation Summary\n\n\nDep. Variable:\nincome_binary\nR-squared:\n-0.2734\n\n\nEstimator:\nIV-2SLS\nAdj. R-squared:\n-0.2750\n\n\nNo. Observations:\n4739\nF-statistic:\n213.68\n\n\nDate:\nTue, Mar 15 2022\nP-value (F-stat)\n0.0000\n\n\nTime:\n10:12:51\nDistribution:\nchi2(6)\n\n\nCov. Estimator:\nrobust\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\nIntercept\n-2.3763\n0.5316\n-4.4699\n0.0000\n-3.4182\n-1.3343\n\n\nC(gender, Treatment(reference='male'))[T.female]\n-0.0456\n0.0150\n-3.0301\n0.0024\n-0.0750\n-0.0161\n\n\nC(ethnicity, Treatment(reference='other'))[T.afam]\n-0.0456\n0.0283\n-1.6123\n0.1069\n-0.1011\n0.0098\n\n\nC(ethnicity, Treatment(reference='other'))[T.hispanic]\n-0.1075\n0.0223\n-4.8322\n0.0000\n-0.1511\n-0.0639\n\n\nurban[T.yes]\n-0.0527\n0.0182\n-2.8947\n0.0038\n-0.0884\n-0.0170\n\n\nunemp\n-0.0101\n0.0027\n-3.7771\n0.0002\n-0.0153\n-0.0048\n\n\neducation\n0.2032\n0.0378\n5.3800\n0.0000\n0.1292\n0.2773\n\n\n\nEndogenous: educationInstruments: distanceRobust Covariance (Heteroskedastic)Debiased: Falseid: 0x7f3f2323f0d0\n\n\nComment the results. Compare with the R tutorials.\nR^2 is negative, but we can’t compare it with the non-IV regression.\nAll coefficients are significant at the 1% level, save for ethnicity (for category “afam”). With the instrumentation strategy, the effect of education on salary, is 4 times higher than without it."
  },
  {
    "objectID": "session_6/instrumental_variables_correction.html",
    "href": "session_6/instrumental_variables_correction.html",
    "title": "Instrumental variables",
    "section": "",
    "text": "Create four random series of length \\(N=1000\\)\n\n\\(x\\): education\n\\(y\\): salary\n\\(z\\): ambition\n\\(q\\): early smoking\n\nsuch that:\n\n\\(x\\) and \\(z\\) cause \\(y\\)\n\\(z\\) causes \\(x\\)\n\\(q\\) is correlated with \\(x\\), not with \\(z\\)\n\n(all relations are linear, add random shocks where needed)\nWe want to study the effect of \\(x\\) on \\(y\\).\nRun the following code to create a mock dataset.\n\nimport numpy\nimport pandas\nN = 100000\n\n\nϵ_z = numpy.random.randn(N)*0.1\nϵ_x = numpy.random.randn(N)*0.1\nϵ_q = numpy.random.randn(N)*0.1\nϵ_y = numpy.random.randn(N)*0.1\n\n\nz = 0.1 + ϵ_z\nx = 0.1 + 0.5*z + ϵ_x\nq = 0.5 + 0.1234*ϵ_x + ϵ_q\ny  = 1.0 + 0.9*x + 0.4*z + ϵ_y\n\n\ndf = pandas.DataFrame({\n    \"x\": x,\n    \"y\": y,\n    \"z\": z,\n    \"q\": q\n})\n\nDescribe the data. Compute the correlations between series.\nHere are the results from the database:\n\ndf.describe()\n\n\n\n\n\n\n\n\nx\ny\nz\nq\n\n\n\n\ncount\n100000.000000\n100000.000000\n100000.000000\n100000.000000\n\n\nmean\n0.149838\n1.174453\n0.099544\n0.499990\n\n\nstd\n0.111716\n0.158766\n0.099791\n0.100681\n\n\nmin\n-0.346680\n0.487024\n-0.307697\n0.100623\n\n\n25%\n0.074457\n1.067474\n0.032258\n0.431809\n\n\n50%\n0.149622\n1.174309\n0.099331\n0.500190\n\n\n75%\n0.225062\n1.280778\n0.166741\n0.567860\n\n\nmax\n0.711602\n1.836777\n0.528661\n0.924587\n\n\n\n\n\n\n\n\ndf.corr()\n\n\n\n\n\n\n\n\nx\ny\nz\nq\n\n\n\n\nx\n1.000000\n0.745872\n0.446562\n0.106993\n\n\ny\n0.745872\n1.000000\n0.534009\n0.067097\n\n\nz\n0.446562\n0.534009\n1.000000\n-0.003369\n\n\nq\n0.106993\n0.067097\n-0.003369\n1.000000\n\n\n\n\n\n\n\nWe observe: - cor(q, x) non zero: the instrument is relevant - close to zero: might be a weak instrument (we would need to check significance) - cor(q, z) = 0 : the instrument is really exogenous\n\n\n\nRun a regression to estimate the effect of \\(x\\) on \\(y\\). Compare the result using statsmodels and linearmodels.\nWhat is the problem with this regression? How can it be detected?\n\nfrom linearmodels.iv import IV2SLS\n\nformula = \"y ~ x\"\nmodel = IV2SLS.from_formula(formula, df)\nresult = model.fit()\nresult\n\n\nOLS Estimation Summary\n\n\nDep. Variable:\ny\nR-squared:\n0.5563\n\n\nEstimator:\nOLS\nAdj. R-squared:\n0.5563\n\n\nNo. Observations:\n100000\nF-statistic:\n1.234e+05\n\n\nDate:\nMon, Mar 27 2023\nP-value (F-stat)\n0.0000\n\n\nTime:\n21:54:41\nDistribution:\nchi2(1)\n\n\nCov. Estimator:\nrobust\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\nIntercept\n1.0156\n0.0006\n1800.8\n0.0000\n1.0145\n1.0167\n\n\nx\n1.0600\n0.0030\n351.22\n0.0000\n1.0541\n1.0659\n\n\n\nid: 0x7f5789b06260\n\n\nRegression is globally significant (p-value for Fisher test &lt; 0.00001). The coefficient \\(\\beta=1.0999\\) in front of \\(x\\) is also very significant at a 0.001% level but does not match the model (y  = 1.0 + 0.9*x + 0.4*z + ϵ_y)\n\n\n\n\nfrom linearmodels.iv import IV2SLS\n\nformula = \"y ~ x + z\"\nmodel = IV2SLS.from_formula(formula, df)\nresult = model.fit()\nresult\n\n\nOLS Estimation Summary\n\n\nDep. Variable:\ny\nR-squared:\n0.6068\n\n\nEstimator:\nOLS\nAdj. R-squared:\n0.6067\n\n\nNo. Observations:\n100000\nF-statistic:\n1.518e+05\n\n\nDate:\nMon, Mar 27 2023\nP-value (F-stat)\n0.0000\n\n\nTime:\n21:54:44\nDistribution:\nchi2(2)\n\n\nCov. Estimator:\nrobust\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\nIntercept\n0.9997\n0.0005\n1818.9\n0.0000\n0.9987\n1.0008\n\n\nx\n0.9007\n0.0032\n283.90\n0.0000\n0.8945\n0.9069\n\n\nz\n0.3993\n0.0035\n113.00\n0.0000\n0.3924\n0.4062\n\n\n\nid: 0x7f5788c9ea10\n\n\nNow we see that the coefficient in front of x is the correct one (that is 0.9).\n\n\n\nMake a causality graph, summarizing what you know from the equations.\nUse \\(q\\) to instrument the effect of x on y. Comment.\n\nfrom linearmodels.iv import IV2SLS\n\nformula = \"y ~ 1 + [x ~ q]\"\nmodel = IV2SLS.from_formula(formula, df)\nresult = model.fit()\nresult\n\n\nIV-2SLS Estimation Summary\n\n\nDep. Variable:\ny\nR-squared:\n0.5422\n\n\nEstimator:\nIV-2SLS\nAdj. R-squared:\n0.5422\n\n\nNo. Observations:\n100000\nF-statistic:\n979.40\n\n\nDate:\nMon, Mar 27 2023\nP-value (F-stat)\n0.0000\n\n\nTime:\n21:56:10\nDistribution:\nchi2(1)\n\n\nCov. Estimator:\nrobust\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\nIntercept\n1.0409\n0.0043\n243.27\n0.0000\n1.0325\n1.0493\n\n\nx\n0.8912\n0.0285\n31.295\n0.0000\n0.8354\n0.9470\n\n\n\nEndogenous: xInstruments: qRobust Covariance (Heteroskedastic)Debiased: Falseid: 0x7f5784cd9d80\n\n\nWe observe that the result is, again, the correct one. This is especially impressive since we didn’t have access to the confounding factor z and couldn’t add it to the regression. Instead, we had another source of randomness q that we used to instrument the regression."
  },
  {
    "objectID": "session_6/instrumental_variables_correction.html#baby-example-on-mock-dataset",
    "href": "session_6/instrumental_variables_correction.html#baby-example-on-mock-dataset",
    "title": "Instrumental variables",
    "section": "",
    "text": "Create four random series of length \\(N=1000\\)\n\n\\(x\\): education\n\\(y\\): salary\n\\(z\\): ambition\n\\(q\\): early smoking\n\nsuch that:\n\n\\(x\\) and \\(z\\) cause \\(y\\)\n\\(z\\) causes \\(x\\)\n\\(q\\) is correlated with \\(x\\), not with \\(z\\)\n\n(all relations are linear, add random shocks where needed)\nWe want to study the effect of \\(x\\) on \\(y\\).\nRun the following code to create a mock dataset.\n\nimport numpy\nimport pandas\nN = 100000\n\n\nϵ_z = numpy.random.randn(N)*0.1\nϵ_x = numpy.random.randn(N)*0.1\nϵ_q = numpy.random.randn(N)*0.1\nϵ_y = numpy.random.randn(N)*0.1\n\n\nz = 0.1 + ϵ_z\nx = 0.1 + 0.5*z + ϵ_x\nq = 0.5 + 0.1234*ϵ_x + ϵ_q\ny  = 1.0 + 0.9*x + 0.4*z + ϵ_y\n\n\ndf = pandas.DataFrame({\n    \"x\": x,\n    \"y\": y,\n    \"z\": z,\n    \"q\": q\n})\n\nDescribe the data. Compute the correlations between series.\nHere are the results from the database:\n\ndf.describe()\n\n\n\n\n\n\n\n\nx\ny\nz\nq\n\n\n\n\ncount\n100000.000000\n100000.000000\n100000.000000\n100000.000000\n\n\nmean\n0.149838\n1.174453\n0.099544\n0.499990\n\n\nstd\n0.111716\n0.158766\n0.099791\n0.100681\n\n\nmin\n-0.346680\n0.487024\n-0.307697\n0.100623\n\n\n25%\n0.074457\n1.067474\n0.032258\n0.431809\n\n\n50%\n0.149622\n1.174309\n0.099331\n0.500190\n\n\n75%\n0.225062\n1.280778\n0.166741\n0.567860\n\n\nmax\n0.711602\n1.836777\n0.528661\n0.924587\n\n\n\n\n\n\n\n\ndf.corr()\n\n\n\n\n\n\n\n\nx\ny\nz\nq\n\n\n\n\nx\n1.000000\n0.745872\n0.446562\n0.106993\n\n\ny\n0.745872\n1.000000\n0.534009\n0.067097\n\n\nz\n0.446562\n0.534009\n1.000000\n-0.003369\n\n\nq\n0.106993\n0.067097\n-0.003369\n1.000000\n\n\n\n\n\n\n\nWe observe: - cor(q, x) non zero: the instrument is relevant - close to zero: might be a weak instrument (we would need to check significance) - cor(q, z) = 0 : the instrument is really exogenous\n\n\n\nRun a regression to estimate the effect of \\(x\\) on \\(y\\). Compare the result using statsmodels and linearmodels.\nWhat is the problem with this regression? How can it be detected?\n\nfrom linearmodels.iv import IV2SLS\n\nformula = \"y ~ x\"\nmodel = IV2SLS.from_formula(formula, df)\nresult = model.fit()\nresult\n\n\nOLS Estimation Summary\n\n\nDep. Variable:\ny\nR-squared:\n0.5563\n\n\nEstimator:\nOLS\nAdj. R-squared:\n0.5563\n\n\nNo. Observations:\n100000\nF-statistic:\n1.234e+05\n\n\nDate:\nMon, Mar 27 2023\nP-value (F-stat)\n0.0000\n\n\nTime:\n21:54:41\nDistribution:\nchi2(1)\n\n\nCov. Estimator:\nrobust\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\nIntercept\n1.0156\n0.0006\n1800.8\n0.0000\n1.0145\n1.0167\n\n\nx\n1.0600\n0.0030\n351.22\n0.0000\n1.0541\n1.0659\n\n\n\nid: 0x7f5789b06260\n\n\nRegression is globally significant (p-value for Fisher test &lt; 0.00001). The coefficient \\(\\beta=1.0999\\) in front of \\(x\\) is also very significant at a 0.001% level but does not match the model (y  = 1.0 + 0.9*x + 0.4*z + ϵ_y)\n\n\n\n\nfrom linearmodels.iv import IV2SLS\n\nformula = \"y ~ x + z\"\nmodel = IV2SLS.from_formula(formula, df)\nresult = model.fit()\nresult\n\n\nOLS Estimation Summary\n\n\nDep. Variable:\ny\nR-squared:\n0.6068\n\n\nEstimator:\nOLS\nAdj. R-squared:\n0.6067\n\n\nNo. Observations:\n100000\nF-statistic:\n1.518e+05\n\n\nDate:\nMon, Mar 27 2023\nP-value (F-stat)\n0.0000\n\n\nTime:\n21:54:44\nDistribution:\nchi2(2)\n\n\nCov. Estimator:\nrobust\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\nIntercept\n0.9997\n0.0005\n1818.9\n0.0000\n0.9987\n1.0008\n\n\nx\n0.9007\n0.0032\n283.90\n0.0000\n0.8945\n0.9069\n\n\nz\n0.3993\n0.0035\n113.00\n0.0000\n0.3924\n0.4062\n\n\n\nid: 0x7f5788c9ea10\n\n\nNow we see that the coefficient in front of x is the correct one (that is 0.9).\n\n\n\nMake a causality graph, summarizing what you know from the equations.\nUse \\(q\\) to instrument the effect of x on y. Comment.\n\nfrom linearmodels.iv import IV2SLS\n\nformula = \"y ~ 1 + [x ~ q]\"\nmodel = IV2SLS.from_formula(formula, df)\nresult = model.fit()\nresult\n\n\nIV-2SLS Estimation Summary\n\n\nDep. Variable:\ny\nR-squared:\n0.5422\n\n\nEstimator:\nIV-2SLS\nAdj. R-squared:\n0.5422\n\n\nNo. Observations:\n100000\nF-statistic:\n979.40\n\n\nDate:\nMon, Mar 27 2023\nP-value (F-stat)\n0.0000\n\n\nTime:\n21:56:10\nDistribution:\nchi2(1)\n\n\nCov. Estimator:\nrobust\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\nIntercept\n1.0409\n0.0043\n243.27\n0.0000\n1.0325\n1.0493\n\n\nx\n0.8912\n0.0285\n31.295\n0.0000\n0.8354\n0.9470\n\n\n\nEndogenous: xInstruments: qRobust Covariance (Heteroskedastic)Debiased: Falseid: 0x7f5784cd9d80\n\n\nWe observe that the result is, again, the correct one. This is especially impressive since we didn’t have access to the confounding factor z and couldn’t add it to the regression. Instead, we had another source of randomness q that we used to instrument the regression."
  },
  {
    "objectID": "session_6/instrumental_variables_correction.html#return-on-education",
    "href": "session_6/instrumental_variables_correction.html#return-on-education",
    "title": "Instrumental variables",
    "section": "Return on Education",
    "text": "Return on Education\nWe follow the excellent R tutorial from the (excellent) Econometrics with R book.\nThe goal is to measure the effect of schooling on earnings, while correcting the endogeneity bias by using distance to college as an instrument.\nDownload the college distance using get_dataset function and make a nice dataframe. Describe the dataset. Plot a histogram of distance (you can use matplotlib’s hist function or seaborn).\nhttps://vincentarelbundock.github.io/Rdatasets/datasets.html\n\nimport statsmodels.api as sm\nds = sm.datasets.get_rdataset(\"CollegeDistance\", \"AER\")\ndf = ds.data\n\n\ndf.head()\n\n\n\n\n\n\n\n\ngender\nethnicity\nscore\nfcollege\nmcollege\nhome\nurban\nunemp\nwage\ndistance\ntuition\neducation\nincome\nregion\n\n\n\n\n1\nmale\nother\n39.150002\nyes\nno\nyes\nyes\n6.2\n8.09\n0.2\n0.88915\n12\nhigh\nother\n\n\n2\nfemale\nother\n48.869999\nno\nno\nyes\nyes\n6.2\n8.09\n0.2\n0.88915\n12\nlow\nother\n\n\n3\nmale\nother\n48.740002\nno\nno\nyes\nyes\n6.2\n8.09\n0.2\n0.88915\n12\nlow\nother\n\n\n4\nmale\nafam\n40.400002\nno\nno\nyes\nyes\n6.2\n8.09\n0.2\n0.88915\n12\nlow\nother\n\n\n5\nfemale\nother\n40.480000\nno\nno\nno\nyes\n5.6\n8.09\n0.4\n0.88915\n13\nlow\nother\n\n\n\n\n\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nscore\nunemp\nwage\ndistance\ntuition\neducation\n\n\n\n\ncount\n4739.000000\n4739.000000\n4739.000000\n4739.000000\n4739.000000\n4739.000000\n\n\nmean\n50.889029\n7.597215\n9.500506\n1.802870\n0.814608\n13.807765\n\n\nstd\n8.701910\n2.763581\n1.343067\n2.297128\n0.339504\n1.789107\n\n\nmin\n28.950001\n1.400000\n6.590000\n0.000000\n0.257510\n12.000000\n\n\n25%\n43.924999\n5.900000\n8.850000\n0.400000\n0.484990\n12.000000\n\n\n50%\n51.189999\n7.100000\n9.680000\n1.000000\n0.824480\n13.000000\n\n\n75%\n57.769999\n8.900000\n10.150000\n2.500000\n1.127020\n16.000000\n\n\nmax\n72.809998\n24.900000\n12.960000\n20.000000\n1.404160\n18.000000\n\n\n\n\n\n\n\nCreate a binary variable incomeb which equals 1 if income is high, 0 otherwise.\n\ndf['income'].unique()\n\narray(['high', 'low'], dtype=object)\n\n\n\n# option 1\ndf['incomeb'] = df['income'].map({'high' : 1, 'low': 0})\n\n\n# option 2\ndf['incomeb'] = (df['income'] == 'high')*1\n\nRun the naive regression \\(\\text{incomeb}=\\beta_0 + \\beta_1 \\text{education} + u\\) using linearmodels. Comment.\n\nfrom linearmodels.iv import IV2SLS\n\nformula = \"incomeb ~ education\"\nmodel = IV2SLS.from_formula(formula, df)\nresult = model.fit()\nresult\n\n\nOLS Estimation Summary\n\n\nDep. Variable:\nincomeb\nR-squared:\n0.0480\n\n\nEstimator:\nOLS\nAdj. R-squared:\n0.0478\n\n\nNo. Observations:\n4739\nF-statistic:\n227.43\n\n\nDate:\nMon, Mar 27 2023\nP-value (F-stat)\n0.0000\n\n\nTime:\n21:58:28\nDistribution:\nchi2(1)\n\n\nCov. Estimator:\nrobust\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\nIntercept\n-0.4780\n0.0499\n-9.5702\n0.0000\n-0.5759\n-0.3801\n\n\neducation\n0.0555\n0.0037\n15.081\n0.0000\n0.0483\n0.0627\n\n\n\nid: 0x7f57819dd9c0\n\n\nWe find that education explains higher income with a significant, but low coefficient 0.05.\nAugment the regression with unemp, hispanic, af-am, female and urban. (Hint: you can use the C() function to deal with categorical data.\n\ndf['gender']\n\n1         male\n2       female\n3         male\n4         male\n5       female\n         ...  \n9391      male\n9401      male\n9411      male\n9421      male\n9431      male\nName: gender, Length: 4739, dtype: object\n\n\n\ndf.columns\n\nIndex(['gender', 'ethnicity', 'score', 'fcollege', 'mcollege', 'home', 'urban',\n       'unemp', 'wage', 'distance', 'tuition', 'education', 'income', 'region',\n       'incomeb'],\n      dtype='object')\n\n\n\n# needed only if you use the function Treatment in the formulas\nfrom patsy import Treatment\n\nfrom linearmodels.iv import IV2SLS\n\nformula = \"incomeb ~ education + unemp + C(gender) + C(ethnicity)\"\nmodel = IV2SLS.from_formula(formula, df)\nresult = model.fit()\nresult\n\n\nOLS Estimation Summary\n\n\nDep. Variable:\nincomeb\nR-squared:\n0.0811\n\n\nEstimator:\nOLS\nAdj. R-squared:\n0.0802\n\n\nNo. Observations:\n4739\nF-statistic:\n443.38\n\n\nDate:\nMon, Mar 27 2023\nP-value (F-stat)\n0.0000\n\n\nTime:\n22:00:32\nDistribution:\nchi2(5)\n\n\nCov. Estimator:\nrobust\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\nIntercept\n-0.4361\n0.0533\n-8.1797\n0.0000\n-0.5406\n-0.3316\n\n\nC(ethnicity)[T.hispanic]\n-0.0249\n0.0185\n-1.3425\n0.1794\n-0.0612\n0.0114\n\n\nC(ethnicity)[T.other]\n0.1347\n0.0163\n8.2871\n0.0000\n0.1029\n0.1666\n\n\nC(gender)[T.male]\n0.0484\n0.0128\n3.7866\n0.0002\n0.0234\n0.0735\n\n\neducation\n0.0511\n0.0037\n13.982\n0.0000\n0.0439\n0.0582\n\n\nunemp\n-0.0111\n0.0022\n-4.9609\n0.0000\n-0.0155\n-0.0067\n\n\n\nid: 0x7f5781306440\n\n\n\ndf['ethnicity'].unique()\n\narray(['other', 'afam', 'hispanic'], dtype=object)\n\n\nComment the results and explain the endogeneity problem\nAdding additional regressors has increased the fit (adj. R^2 from 0.04 to 0.08) without changing the coefficient on the education level. This would imply that regression is robust.\nHowever, we might have an endogeneity issue with some potential other factors explaining both income level and salary (cf many discussions in the course).\nExplain why distance to college might be used to instrument the effect of schooling.\nAssuming that the decision to live in a given county does not depend on the presence of a college nearby, the distance to college should be exogenous.\nThe distance to college is probably correlated with the decision to go so the instrument should have some power (opposite of weak)\nRun an IV regression, where distance is used to instrument schooling.\nlook at: https://bashtage.github.io/linearmodels/ (two-stage least squares)\n\n# needed only if you use the function Treatment in the formulas\nfrom patsy import Treatment\n\nfrom linearmodels.iv import IV2SLS\n \nformula = \"incomeb ~ [education ~ distance] + unemp + C(gender) + C(ethnicity)\"\nmodel = IV2SLS.from_formula(formula, df)\nresult = model.fit()\nresult\n\n\nIV-2SLS Estimation Summary\n\n\nDep. Variable:\nincomeb\nR-squared:\n-0.1339\n\n\nEstimator:\nIV-2SLS\nAdj. R-squared:\n-0.1351\n\n\nNo. Observations:\n4739\nF-statistic:\n1748.5\n\n\nDate:\nWed, Mar 01 2023\nP-value (F-stat)\n0.0000\n\n\nTime:\n11:06:52\nDistribution:\nchi2(6)\n\n\nCov. Estimator:\nrobust\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\nC(ethnicity)[T.afam]\n-2.0318\n0.4856\n-4.1842\n0.0000\n-2.9836\n-1.0801\n\n\nC(ethnicity)[T.hispanic]\n-2.0813\n0.4929\n-4.2226\n0.0000\n-3.0474\n-1.1152\n\n\nC(ethnicity)[T.other]\n-1.9566\n0.5039\n-3.8830\n0.0001\n-2.9441\n-0.9690\n\n\nC(gender)[T.male]\n0.0457\n0.0142\n3.2195\n0.0013\n0.0179\n0.0735\n\n\nunemp\n-0.0100\n0.0025\n-3.9704\n0.0001\n-0.0149\n-0.0051\n\n\neducation\n0.1692\n0.0358\n4.7261\n0.0000\n0.0990\n0.2393\n\n\n\nEndogenous: educationInstruments: distanceRobust Covariance (Heteroskedastic)Debiased: Falseid: 0x7f0acc0f8a30\n\n\nComment the results. Compare with the R tutorials.\nThe estimate we get for the return on education is three times higher and highly significant."
  },
  {
    "objectID": "session_6/Session_5_Correction.html",
    "href": "session_6/Session_5_Correction.html",
    "title": "Regressions 2",
    "section": "",
    "text": "We will analyse those data to find relationships between the happiness score and economy, family, health, freedom, trust, perception of corruption, generosity…\nThe dataset contains the following variables:\n\nCountry : Country name\nOverall rank : Country ranking based on happiness score\nScore : Individual personal happiness rating from 0 to 10.\nGDP per capita : GDP per capita of each country in terms of purchasing power parity (PPP) (in USD)\nSocial support : Individual rating that determines whether, when you have problems, your family or friends would help you. Binary responses (0 or 1).\nHealthy life expectancy : Healthy life expectancy at birth is based on data from the World Health Organization (WHO)\nFreedom to make life choices : Individual rating that determines whether you are atisfied or dissatisfied with your freedom to choose hat you do with your life. Binary responses (0 or 1).\nGenerosity : Generosity is the residual from the regression of the national mean of responses to the question “Have you donated money to a charity in the last month?” on GDP per capita.\nPerceptions of corruption : Average of binary responses to two GWP questions: corruption in government and corruption in business."
  },
  {
    "objectID": "session_6/Session_5_Correction.html#ii-linear-regression",
    "href": "session_6/Session_5_Correction.html#ii-linear-regression",
    "title": "Regressions 2",
    "section": "II) Linear regression",
    "text": "II) Linear regression"
  },
  {
    "objectID": "session_6/Session_5_Correction.html#a-simple-linear-regression",
    "href": "session_6/Session_5_Correction.html#a-simple-linear-regression",
    "title": "Regressions 2",
    "section": "A) Simple linear regression",
    "text": "A) Simple linear regression\nPerform various linear regressions to predict the Happiness score using one of the variables available in the dataset.\n\ndf.columns\n\nIndex(['Overall_rank', 'Country_or_region', 'Score', 'GDP_per_capita',\n       'Social_support', 'Healthy_life_expectancy',\n       'Freedom_to_make_life_choices', 'Generosity',\n       'Perceptions_of_corruption'],\n      dtype='object')\n\n\n\nmodel_1 = smf.ols(\"Score ~ GDP_per_capita\", df)\nres_1 = model_1.fit()\nres_1.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nScore\nR-squared:\n0.630\n\n\nModel:\nOLS\nAdj. R-squared:\n0.628\n\n\nMethod:\nLeast Squares\nF-statistic:\n262.5\n\n\nDate:\nWed, 15 Feb 2023\nProb (F-statistic):\n4.32e-35\n\n\nTime:\n11:00:56\nLog-Likelihood:\n-159.97\n\n\nNo. Observations:\n156\nAIC:\n323.9\n\n\nDf Residuals:\n154\nBIC:\n330.0\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n3.3993\n0.135\n25.120\n0.000\n3.132\n3.667\n\n\nGDP_per_capita\n2.2181\n0.137\n16.202\n0.000\n1.948\n2.489\n\n\n\n\n\n\nOmnibus:\n1.139\nDurbin-Watson:\n1.378\n\n\nProb(Omnibus):\n0.566\nJarque-Bera (JB):\n1.244\n\n\nSkew:\n-0.177\nProb(JB):\n0.537\n\n\nKurtosis:\n2.742\nCond. No.\n4.77\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nmodel_1 = smf.ols(\"Score ~ GDP_per_capita + Healthy_life_expectancy + Freedom_to_make_life_choices\", df)\nres_1 = model_1.fit()\n\n\nres_1.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nScore\nR-squared:\n0.742\n\n\nModel:\nOLS\nAdj. R-squared:\n0.737\n\n\nMethod:\nLeast Squares\nF-statistic:\n146.1\n\n\nDate:\nWed, 15 Feb 2023\nProb (F-statistic):\n1.42e-44\n\n\nTime:\n11:00:13\nLog-Likelihood:\n-131.75\n\n\nNo. Observations:\n156\nAIC:\n271.5\n\n\nDf Residuals:\n152\nBIC:\n283.7\n\n\nDf Model:\n3\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n2.4201\n0.167\n14.519\n0.000\n2.091\n2.749\n\n\nGDP_per_capita\n1.1781\n0.210\n5.599\n0.000\n0.762\n1.594\n\n\nHealthy_life_expectancy\n1.4578\n0.348\n4.189\n0.000\n0.770\n2.145\n\n\nFreedom_to_make_life_choices\n2.1993\n0.349\n6.298\n0.000\n1.509\n2.889\n\n\n\n\n\n\nOmnibus:\n11.927\nDurbin-Watson:\n1.497\n\n\nProb(Omnibus):\n0.003\nJarque-Bera (JB):\n12.394\n\n\nSkew:\n-0.665\nProb(JB):\n0.00204\n\n\nKurtosis:\n3.369\nCond. No.\n14.2\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nsns.regplot(x='GDP_per_capita', y='Score', data=df)\n\n&lt;AxesSubplot: xlabel='GDP_per_capita', ylabel='Score'&gt;\n\n\n\n\n\n\nmodel_2 = smf.ols(\"Score ~ Healthy_life_expectancy\", df)\nres_2 = model_2.fit()\nres_2.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nScore\nR-squared:\n0.608\n\n\nModel:\nOLS\nAdj. R-squared:\n0.606\n\n\nMethod:\nLeast Squares\nF-statistic:\n239.1\n\n\nDate:\nTue, 14 Feb 2023\nProb (F-statistic):\n3.79e-33\n\n\nTime:\n20:44:17\nLog-Likelihood:\n-164.48\n\n\nNo. Observations:\n156\nAIC:\n333.0\n\n\nDf Residuals:\n154\nBIC:\n339.1\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n2.8068\n0.177\n15.837\n0.000\n2.457\n3.157\n\n\nHealthy_life_expectancy\n3.5854\n0.232\n15.462\n0.000\n3.127\n4.043\n\n\n\n\n\n\nOmnibus:\n6.324\nDurbin-Watson:\n1.140\n\n\nProb(Omnibus):\n0.042\nJarque-Bera (JB):\n3.543\n\n\nSkew:\n-0.148\nProb(JB):\n0.170\n\n\nKurtosis:\n2.324\nCond. No.\n6.41\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nsns.regplot(x='Healthy_life_expectancy', y='Score', data=df)\n\n&lt;AxesSubplot: xlabel='Healthy_life_expectancy', ylabel='Score'&gt;"
  },
  {
    "objectID": "session_6/Session_5_Correction.html#b-multiple-linear-regression",
    "href": "session_6/Session_5_Correction.html#b-multiple-linear-regression",
    "title": "Regressions 2",
    "section": "B) Multiple linear regression",
    "text": "B) Multiple linear regression\n\nMultiple linear regression\n\nmodel_3 = smf.ols(\"Score ~ Healthy_life_expectancy + Freedom_to_make_life_choices + GDP_per_capita +Social_support\", df)\nres_3= model_3.fit()\nres_3.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nScore\nR-squared:\n0.771\n\n\nModel:\nOLS\nAdj. R-squared:\n0.765\n\n\nMethod:\nLeast Squares\nF-statistic:\n127.0\n\n\nDate:\nTue, 14 Feb 2023\nProb (F-statistic):\n2.82e-47\n\n\nTime:\n20:44:20\nLog-Likelihood:\n-122.62\n\n\nNo. Observations:\n156\nAIC:\n255.2\n\n\nDf Residuals:\n151\nBIC:\n270.5\n\n\nDf Model:\n4\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n1.8921\n0.199\n9.491\n0.000\n1.498\n2.286\n\n\nHealthy_life_expectancy\n1.1414\n0.337\n3.384\n0.001\n0.475\n1.808\n\n\nFreedom_to_make_life_choices\n1.8458\n0.340\n5.423\n0.000\n1.173\n2.518\n\n\nGDP_per_capita\n0.8105\n0.216\n3.745\n0.000\n0.383\n1.238\n\n\nSocial_support\n1.0166\n0.235\n4.331\n0.000\n0.553\n1.480\n\n\n\n\n\n\nOmnibus:\n5.077\nDurbin-Watson:\n1.641\n\n\nProb(Omnibus):\n0.079\nJarque-Bera (JB):\n4.685\n\n\nSkew:\n-0.413\nProb(JB):\n0.0961\n\n\nKurtosis:\n3.198\nCond. No.\n17.8\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n### By comparing adjusted $R^2$, find the regression which explains best happiness.\n\n\n\nPolynomial regression\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\n\n\nx=df.Score.values.reshape(-1,1)\ny=df[\"Healthy_life_expectancy\"].values.reshape(-1,1)\nplt.scatter(df[\"Score\"],df[\"Healthy_life_expectancy\"],color=\"green\")\n\npolynomial_regression=PolynomialFeatures(degree=4)\nx_polynomial=polynomial_regression.fit_transform(x)\n\n#%% fit\nlinear_regression2=LinearRegression()\nlinear_regression2.fit(x_polynomial,y)\n\n#%%\ny_head2=linear_regression2.predict(x_polynomial)\n\nplt.plot(x,y_head2,color=\"red\",label=\"poly\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "session_6/transcript.html",
    "href": "session_6/transcript.html",
    "title": "Introduction to Instrumental Variables",
    "section": "",
    "text": "Our multilinear regression: \\[y = \\alpha + \\beta x_1 + \\cdots + \\beta x_n\\]\nSo far, we have only considered real variables: (\\(x_i \\in \\mathbb{R}\\)).\n\nex: \\(x_{\\text{gdp}} = \\alpha + \\beta_1 x_{\\text{unemployment}} + \\beta_2 x_{\\text{inflation}}\\)\n\nHow do we deal with the following cases?\n\nbinary variable: \\(x\\in \\{0,1\\}\\) (or \\(\\{True, False}\\))\n\nex: \\(\\text{gonetowar}\\), \\(\\text{hasdegree}\\)\n\ncategorical variable:\n\nex: survey result (0: I don’t know, 1: I strongly disagree, 2: I disagree, 3: I agree, 4: I strongly agree)\nthere is no ranking of answers\nwhen there is ranking: hierarchical index\n\nnonnumerical variables:\n\nex: (flower type: \\(x\\in \\text{myosotis}, \\text{rose}, ...\\))\n\n\n\n\n\n\n\nNothing to be done: just make sure variables take values 0 or 1. \\[y_\\text{salary} = \\alpha + \\beta x_{\\text{gonetowar}}\\]\nInterpretation:\n\nhaving gone to war is associated with a \\(\\beta\\) increase (or decrease?) in salary (still no causality)\n\n\n\n\n\n\nLook at the model: \\[y_{\\text{CO2 emission}} = \\alpha + \\beta x_{\\text{yellow vest support}} \\]\nWhere \\(y_{\\text{CO2 emission}}\\) is an individual’s CO2 emissions and \\(x_{\\text{yellow vest support}}\\) is the response the the question Are you in agreement with the yellow vests demands?.\nResponse is coded up as:\n\n0: Strongly disagree\n1: Disagree\n2: Neutral\n3: Agree\n4: Strongly agree\n\nIf the variable was used directly, how would you intepret the coefficient \\(\\beta\\) ?\n\nindex is hierarchical\nbut the distances between 1 and 2 or 2 and 3 are not comparable…\n\n\n\n\n\n\nWe use one dummy variable per possible answer.\n\n\n\n\n\n\n\n\n\n\n\n\\(D_{\\text{Strongly Disagree}}\\)\n\\(D_{\\text{Disagree}}\\)\n\\(D_{\\text{Neutral}}\\)\n\\(D_{\\text{Agree}}\\)\n\\(D_{\\text{Strongly Agree}}\\)\n\n\n\n\n1\n0\n0\n0\n0\n\n\n0\n1\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n1\n0\n\n\n0\n0\n0\n0\n1\n\n\n\n\nValues are linked by the specific dummy coding.\n\nthe choice of the reference group (with 0) is not completely neutral\n\nfor linear regressions, we can ignore its implications\n\nit must be frequent enough in the data\neffects coding: reference group takes -1 instead of 0\n\nNote that hierarchy is lost. The same treatment can be applied to non-hierachical variables\nNow our variables are perfectly colinear:\n\nwe can deduce one from all the others\nwe drop one from the regression: the reference group TODO\n\n\n\n\n\n\\[y_{\\text{CO2 emission}} = \\alpha + \\beta_1 x_{\\text{strdis}} + \\beta_2 x_{\\text{dis}} + \\beta_3 x_{\\text{agr}} + \\beta_4 x_{\\text{stragr}}\\]\n\nInterpretation:\n\nbeing in the group which strongly agrees to the yellow vest’s claim is associated with an additional \\(\\beta_4\\) increase in CO2 consumption compared with members of the neutral group\n\n\n\n\n\n\n\n\n\nWhat about nonnumerical variables?\n\nWhen variables take nonnumerical variables, we convert them to numerical variables.\n\nExample:\n\n\n\n\nactivity\ncode\n\n\n\n\nmassage therapist\n1\n\n\nmortician\n2\n\n\narcheologist\n3\n\n\nfinancial clerks\n4\n\n\n\n\n\n\nThen we convert to dummy variables exactly like hierarchical indices\n\nhere \\(\\text{massage therapist}\\) is taken as reference\n\n\n\n\n\n\n\n\n\n\n\\(D_{\\text{mortician}}\\)\n\\(D_{\\text{archeologist}}\\)\n\\(D_{\\text{financial clerks}}\\)\n\n\n\n\n1\n0\n0\n\n\n0\n1\n0\n\n\n0\n0\n1\n\n\n\n\n\n\n\n\nUse statsmodels to create dummy variables with formula API.\n\nReplace\n\nsalary ~ activity\n\nby:\n\nsalary ~ C(activity)\nThere is an options to choose the reference group\nsalary ~ C(activity, Treatment(reference=\"archeologist\"))"
  },
  {
    "objectID": "session_6/transcript.html#data",
    "href": "session_6/transcript.html#data",
    "title": "Introduction to Instrumental Variables",
    "section": "",
    "text": "Our multilinear regression: \\[y = \\alpha + \\beta x_1 + \\cdots + \\beta x_n\\]\nSo far, we have only considered real variables: (\\(x_i \\in \\mathbb{R}\\)).\n\nex: \\(x_{\\text{gdp}} = \\alpha + \\beta_1 x_{\\text{unemployment}} + \\beta_2 x_{\\text{inflation}}\\)\n\nHow do we deal with the following cases?\n\nbinary variable: \\(x\\in \\{0,1\\}\\) (or \\(\\{True, False}\\))\n\nex: \\(\\text{gonetowar}\\), \\(\\text{hasdegree}\\)\n\ncategorical variable:\n\nex: survey result (0: I don’t know, 1: I strongly disagree, 2: I disagree, 3: I agree, 4: I strongly agree)\nthere is no ranking of answers\nwhen there is ranking: hierarchical index\n\nnonnumerical variables:\n\nex: (flower type: \\(x\\in \\text{myosotis}, \\text{rose}, ...\\))"
  },
  {
    "objectID": "session_6/transcript.html#binary-variable",
    "href": "session_6/transcript.html#binary-variable",
    "title": "Introduction to Instrumental Variables",
    "section": "",
    "text": "Nothing to be done: just make sure variables take values 0 or 1. \\[y_\\text{salary} = \\alpha + \\beta x_{\\text{gonetowar}}\\]\nInterpretation:\n\nhaving gone to war is associated with a \\(\\beta\\) increase (or decrease?) in salary (still no causality)"
  },
  {
    "objectID": "session_6/transcript.html#categorical-variable",
    "href": "session_6/transcript.html#categorical-variable",
    "title": "Introduction to Instrumental Variables",
    "section": "",
    "text": "Look at the model: \\[y_{\\text{CO2 emission}} = \\alpha + \\beta x_{\\text{yellow vest support}} \\]\nWhere \\(y_{\\text{CO2 emission}}\\) is an individual’s CO2 emissions and \\(x_{\\text{yellow vest support}}\\) is the response the the question Are you in agreement with the yellow vests demands?.\nResponse is coded up as:\n\n0: Strongly disagree\n1: Disagree\n2: Neutral\n3: Agree\n4: Strongly agree\n\nIf the variable was used directly, how would you intepret the coefficient \\(\\beta\\) ?\n\nindex is hierarchical\nbut the distances between 1 and 2 or 2 and 3 are not comparable…"
  },
  {
    "objectID": "session_6/transcript.html#hierarchical-index-2",
    "href": "session_6/transcript.html#hierarchical-index-2",
    "title": "Introduction to Instrumental Variables",
    "section": "",
    "text": "We use one dummy variable per possible answer.\n\n\n\n\n\n\n\n\n\n\n\n\\(D_{\\text{Strongly Disagree}}\\)\n\\(D_{\\text{Disagree}}\\)\n\\(D_{\\text{Neutral}}\\)\n\\(D_{\\text{Agree}}\\)\n\\(D_{\\text{Strongly Agree}}\\)\n\n\n\n\n1\n0\n0\n0\n0\n\n\n0\n1\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n1\n0\n\n\n0\n0\n0\n0\n1\n\n\n\n\nValues are linked by the specific dummy coding.\n\nthe choice of the reference group (with 0) is not completely neutral\n\nfor linear regressions, we can ignore its implications\n\nit must be frequent enough in the data\neffects coding: reference group takes -1 instead of 0\n\nNote that hierarchy is lost. The same treatment can be applied to non-hierachical variables\nNow our variables are perfectly colinear:\n\nwe can deduce one from all the others\nwe drop one from the regression: the reference group TODO"
  },
  {
    "objectID": "session_6/transcript.html#hierarchical-index-3",
    "href": "session_6/transcript.html#hierarchical-index-3",
    "title": "Introduction to Instrumental Variables",
    "section": "",
    "text": "\\[y_{\\text{CO2 emission}} = \\alpha + \\beta_1 x_{\\text{strdis}} + \\beta_2 x_{\\text{dis}} + \\beta_3 x_{\\text{agr}} + \\beta_4 x_{\\text{stragr}}\\]\n\nInterpretation:\n\nbeing in the group which strongly agrees to the yellow vest’s claim is associated with an additional \\(\\beta_4\\) increase in CO2 consumption compared with members of the neutral group"
  },
  {
    "objectID": "session_6/transcript.html#nonnumerical-variables",
    "href": "session_6/transcript.html#nonnumerical-variables",
    "title": "Introduction to Instrumental Variables",
    "section": "",
    "text": "What about nonnumerical variables?\n\nWhen variables take nonnumerical variables, we convert them to numerical variables.\n\nExample:\n\n\n\n\nactivity\ncode\n\n\n\n\nmassage therapist\n1\n\n\nmortician\n2\n\n\narcheologist\n3\n\n\nfinancial clerks\n4\n\n\n\n\n\n\nThen we convert to dummy variables exactly like hierarchical indices\n\nhere \\(\\text{massage therapist}\\) is taken as reference\n\n\n\n\n\n\n\n\n\n\n\\(D_{\\text{mortician}}\\)\n\\(D_{\\text{archeologist}}\\)\n\\(D_{\\text{financial clerks}}\\)\n\n\n\n\n1\n0\n0\n\n\n0\n1\n0\n\n\n0\n0\n1"
  },
  {
    "objectID": "session_6/transcript.html#hands-on",
    "href": "session_6/transcript.html#hands-on",
    "title": "Introduction to Instrumental Variables",
    "section": "",
    "text": "Use statsmodels to create dummy variables with formula API.\n\nReplace\n\nsalary ~ activity\n\nby:\n\nsalary ~ C(activity)\nThere is an options to choose the reference group\nsalary ~ C(activity, Treatment(reference=\"archeologist\"))"
  },
  {
    "objectID": "session_6/transcript.html#what-is-causality",
    "href": "session_6/transcript.html#what-is-causality",
    "title": "Introduction to Instrumental Variables",
    "section": "What is causality?",
    "text": "What is causality?\n. . .\n\n\n\nGroucho Marx\n\n\nClear? Huh! Why a four-year-old child could understand this report! Run out and find me a four-year-old child, I can’t make head or tail of it."
  },
  {
    "objectID": "session_6/transcript.html#spurious-correlation",
    "href": "session_6/transcript.html#spurious-correlation",
    "title": "Introduction to Instrumental Variables",
    "section": "Spurious correlation",
    "text": "Spurious correlation\n\n\n\nSpurious Correlation\n\n\n\nWe have seen spurious correlation before\n\nit happens when two series comove without being actually correlated\n\nAlso, two series might be correlated without one causing the other\n\nex: countries eating more chocolate have more nobel prices…"
  },
  {
    "objectID": "session_6/transcript.html#definitions",
    "href": "session_6/transcript.html#definitions",
    "title": "Introduction to Instrumental Variables",
    "section": "Definitions?",
    "text": "Definitions?\nBut how do we define\n\ncorrelation\ncausality\n\n?\nBoth concepts are actually hard to define:\n\nin statistics (and econometrices) they refer to the generating process\nif the data was generated again, would you observe the same relations?\n\nFor instance correlation between \\(X\\) and \\(Y\\) is just the average correlation taken over many draws \\(\\omega\\) of the data: \\[E_{\\omega}\\left[ (X-E[X])(Y-E[Y])\\right]\\]"
  },
  {
    "objectID": "session_6/transcript.html#how-do-we-define-causality-1",
    "href": "session_6/transcript.html#how-do-we-define-causality-1",
    "title": "Introduction to Instrumental Variables",
    "section": "How do we define causality (1)",
    "text": "How do we define causality (1)\n\nIn math, we have implication: \\(A \\implies B\\)\n\napplies to statements that can be either true or false\ngiven \\(A\\) and \\(B\\), \\(A\\) implies \\(B\\) unless \\(A\\) is true and \\(B\\) is false\nparadox of the drinker: at any time, there exists a person such that: if this person drinks, then everybody drinks\n\nIn a mathematical universe taking values \\(\\omega\\), we can define causality between statement \\(A(\\omega)\\) and \\(B(\\omega)\\) as : \\[\\forall \\omega, A(\\omega) \\implies B(\\omega)\\]"
  },
  {
    "objectID": "session_6/transcript.html#how-do-we-define-causality-2",
    "href": "session_6/transcript.html#how-do-we-define-causality-2",
    "title": "Introduction to Instrumental Variables",
    "section": "How do we define causality (2)",
    "text": "How do we define causality (2)\nBut causality in the real world is problematic\nUsually, we observe \\(A(\\omega)\\) only once…\n. . .\n\n\nExample:\n\nstate of the world \\(\\omega\\): 2008, big financial crisis, …\nA: Ben Bernanke chairman of the Fed\nB: successful economic interventions\nWas Ben Bernanke a good central banker?\nImpossible to say.\n\n\n\n\n\n\n\nThen there is the uncertain concept of time… But let’s take it as granted to not overcomplicate…"
  },
  {
    "objectID": "session_6/transcript.html#causality-in-statistics",
    "href": "session_6/transcript.html#causality-in-statistics",
    "title": "Introduction to Instrumental Variables",
    "section": "Causality in Statistics",
    "text": "Causality in Statistics\n\n\n\n\n\n\nStatistical definition of causality\n\n\n\nVariable \\(A\\) causes \\(B\\) in a statistical sense if - \\(A\\) and \\(B\\) are correlated - \\(A\\) is known before \\(B\\) - correlation between \\(A\\) and \\(B\\) is unaffected by other variables\n\n\n\nThere are other related statistical definitions:\n\nlike Granger causality…\n… but not for this course"
  },
  {
    "objectID": "session_6/transcript.html#factual-and-counterfactual",
    "href": "session_6/transcript.html#factual-and-counterfactual",
    "title": "Introduction to Instrumental Variables",
    "section": "Factual and counterfactual",
    "text": "Factual and counterfactual\n\n\n \n\n\n\n\n\n\n\nSuppose we observe an event A\n\nA: a patient is administered a drug, government closes all schools during Covid\n\nWe observe a another event B\n\nB: the patient recovers, virus circulation decreases\n\n\n\n\n\nTo interpret B as a consequence of A, we would like to consider the counter-factual:\n\na patient is not administered a drug, government doesn’t close schools\npatient does not recover, virus circulation is stable\n\n\n\n\n\n. . .\nAn important task in econometrics is to construct a counter-factual\n\nas the name suggests is it sometimes never observed!"
  },
  {
    "objectID": "session_6/transcript.html#scientific-experiment",
    "href": "session_6/transcript.html#scientific-experiment",
    "title": "Introduction to Instrumental Variables",
    "section": "Scientific Experiment",
    "text": "Scientific Experiment\n\n\n\n \n\n\n\nIn science we establish causality by performing experiments\n\nand create the counterfactual\n\nA good experiment is reproducible\n\nsame variables\nsame state of the world (other variables)\nreproduce several times (in case output is noisy or random)\n\nChange one factor at a time\n\nto create a counter-factual"
  },
  {
    "objectID": "session_6/transcript.html#measuring-effect-of-treatment",
    "href": "session_6/transcript.html#measuring-effect-of-treatment",
    "title": "Introduction to Instrumental Variables",
    "section": "Measuring effect of treatment",
    "text": "Measuring effect of treatment\n\n\n\n\n\n\n\n\n\n\n\nAssume we have discovered two medications: R and B\n\n\n\n\nGive one of them (R) to a patient and observe the outcome\n\n\n\n\nWould would have been the effect of (B) on the same patient?\n\n????\n\n\n\n\n\nWhat if we had many patients and let them choose the medication?\n\n\n\n\n. . .\nMaybe the effect would be the consequence of the choice of patients rather than of the medication?"
  },
  {
    "objectID": "session_6/transcript.html#an-exmple-from-behavioural-economics",
    "href": "session_6/transcript.html#an-exmple-from-behavioural-economics",
    "title": "Introduction to Instrumental Variables",
    "section": "An exmple from behavioural economics",
    "text": "An exmple from behavioural economics\n\n\n\nExample: cognitive dissonance\n\nExperiment in GATE Lab\nVolunteers play an investment game.\nThey are asked beforehand whether they support OM, PSG, or none.\n\n\n\n\n\nExperiment 1:\n\nBefore the experiment, randomly selected volunteers are given a football shirt of their preferred team (treatment 1)\nOther volunteers receive nothing (treatment 0)\n\nResult:\n\nhaving a football shirt seems to boost investment performance…\n\n\n\n\n\nExperiment 2: subjects are given randomly a shirt of either Olympique de Marseille or PSG.\nResult:\n\nHaving the good shirt improves performance.\nHaving the wrong one deteriorates it badly.\n\n\n\n\n\nHow would you code up this experiment?\nCan we conclude on some form of causality?"
  },
  {
    "objectID": "session_6/transcript.html#formalisation-of-the-problem",
    "href": "session_6/transcript.html#formalisation-of-the-problem",
    "title": "Introduction to Instrumental Variables",
    "section": "Formalisation of the problem",
    "text": "Formalisation of the problem\n\n\n\n\n\n\nCause (A): two groups of people\n\nthose given a shirt (treatment 1)\nthose not given a shirt (treatment 0)\n\nPossible consequence (B): performance\nTake a given agent Alice: she performs well with a PSG shirt.\n\nmaybe she is a good investor?\nor maybe she is playing for her team?\n\nLet’s try to have her play again without the football shirt\n\nnow the experiment has changed: she has gained experience, is more tired, misses the shirt…\nit is impossible to get a perfect counterfactual (i.e. where only A changes)\n\n\n\n\nLet’s take somebody else then? Bob was really bad without a PSG shirt.\n\nhe might be a bad investor? or he didn’t understand the rules?\nsome other variables have changed, not only the treatment\n\nHow to make a perfect experiment?\n\nChoose randomly whether assigning a shirt or not\nby construction the treatment will not be correlated with other variables"
  },
  {
    "objectID": "session_6/transcript.html#randomized-control-trial",
    "href": "session_6/transcript.html#randomized-control-trial",
    "title": "Introduction to Instrumental Variables",
    "section": "Randomized Control Trial",
    "text": "Randomized Control Trial\n\n\n\n\n\n\nRandomized Control Trial (RCT)\n\n\n\nThe best way to ensure that treatment is independent from other factors is to randomize it.\n\n\n\n\n\nIn medecine\n\nsome patients receive the treatment (red pill)\nsome other receive the control treatment (blue pill / placebo)\n\nIn economics:\n\nrandomized field experiments\nrandomized phase-ins for new policies\n\nvery useful for policy evaluation\n\n\n\n\n\n\n\nEsther Duflo\n\n\n\n\n\n\nIt is common in economics, instead of assigning treatments randomly, we often say that we assign individuals randomly to the treatment and to the control group. It is equivalent."
  },
  {
    "objectID": "session_6/transcript.html#natural-experiment",
    "href": "session_6/transcript.html#natural-experiment",
    "title": "Introduction to Instrumental Variables",
    "section": "Natural experiment",
    "text": "Natural experiment\n\n\n\n\n\n\nNatural Experiment\n\n\n\nA natural experiment satisfies conditions that treatment is assigned randomly\n\nwithout interference by the econometrician\n\n\n\n\nAn exemple of a Natural Experiment:\n\ngender bias in french local elections (jean-pierre eymeoud, paul vertier) link\nare women discriminated against by voters in local elections?\n\n\n. . .\n\nResult: yes, they get 1.5% less votes by right-wing voters\n\n. . .\n\nWhat was the natural experiment"
  },
  {
    "objectID": "session_6/transcript.html#example",
    "href": "session_6/transcript.html#example",
    "title": "Introduction to Instrumental Variables",
    "section": "Example",
    "text": "Example\nLifetime Earnings and the Vietnam Era Draft Lottery, by JD Angrist\n\n\n\nFact:\n\nveterans of the vietnam war (55-75) earn (in the 80s) an income that is 15% less in average than those who didn’t go to the war.\nWhat can we conclude?\nHard to say: maybe those sent to the war came back with lower productivity (because of PTSD, public stigma, …)? maybe they were not the most productive in the first place (selection bias)?\n\nProblem (for the economist):\n\nwe didn’t sent people to war randomly\n\n\n\n\nGenius idea:\n\nhere is a variable which randomly affected whether people were sent: the Draft\n\n\nbetween 1947, and 1973, a lottery was run to determine who would go to war\n\nthe draft number was determined, based on date of birth, and first letters of name\n\nand was correlated with the probability that a given person would go to war\nand it was so to say random or at least independent from anything relevant to the problem\n\n\n\n\n. . .\nCan we use the Draft to generate randomness ?"
  },
  {
    "objectID": "session_6/transcript.html#problem",
    "href": "session_6/transcript.html#problem",
    "title": "Introduction to Instrumental Variables",
    "section": "Problem",
    "text": "Problem\n\nTake the linear regression: \\[y = \\alpha + \\beta x + \\epsilon\\]\n\n\\(y\\): salary\n\\(x\\): went to war\n\nWe want to establish causality from x to y\n\nwe would like to interpret \\(x\\) as the “treatment”\n\nBut there can be confounding factors:\n\nvariable \\(z\\) which causes both x and y\nexemple: socio-economic background, IQ, …\n\nIf we could identify \\(z\\) we could control for it: \\[y = \\alpha + \\beta_1 x + \\beta_2 z + \\epsilon\\]\n\nwe would get a better predictor of \\(y\\) but more uncertainty about \\(\\beta_1\\) (\\(x\\) and \\(z\\) are correlated)"
  },
  {
    "objectID": "session_6/transcript.html#reformulate-the-problem",
    "href": "session_6/transcript.html#reformulate-the-problem",
    "title": "Introduction to Instrumental Variables",
    "section": "Reformulate the problem",
    "text": "Reformulate the problem\n\n\n\nLet’s assume treatment \\(x\\) is a binary variable \\(\\in{0,1}\\)\nWe want to estimate \\[y = \\alpha + \\beta x + z + \\epsilon\\] where \\(z\\) is potentially correlated to \\(x\\) and \\(y\\)\nThere are two groups:\n\nthose who receive the treatment \\[y = \\alpha + \\beta + z_{T=1} + \\epsilon\\]\nthe others \\[y = \\alpha + 0 +  z_{T=0} + \\epsilon\\]\n\n\n\n\nProblem:\n\nif \\(z\\) is higher in the treatment group, its effect can’t be separated from the treatment effect.\n\nIntuition: what if we make groups differently?\n\ncompletely independent from \\(z\\) (and \\(\\epsilon\\))\nnot independently from \\(x\\) so that one group will receive more treatment than the other\n\nTo make this group we need a new variable \\(q\\) that is:\n\ncorrelated with \\(x\\) so that it will correspond to some treatment effect\nuncorrelated to \\(z\\) or \\(\\epsilon\\) (exogenous)"
  },
  {
    "objectID": "session_6/transcript.html#two-stage-regression",
    "href": "session_6/transcript.html#two-stage-regression",
    "title": "Introduction to Instrumental Variables",
    "section": "Two stage regression",
    "text": "Two stage regression\n\n\n\nWe would like to redo the treatment groups in a way that is independent from \\(z\\) (and everything contained in \\(\\epsilon\\))\n\n\\(q\\) is a binary variable: drafted or not\n\n\nFirst stage: regress group assignment on the instrument: \\[x = \\alpha_0 + \\beta_0 q + \\eta\\]\n\nwe can now predict group assignment in a way that is independent from \\(z\\) (and everything in \\(\\epsilon\\)) \\[\\tilde{x} = \\alpha_0 + \\beta_0 q\\]\n\n\nSecond stage: use the predicted value instead of the original one \\[y = \\alpha + \\beta_1 \\tilde{x} + z + \\epsilon\\]\n\n\n\n\nResult:\n\nIf \\(\\beta_1\\) is significantly nonzero, there is a causal effect between \\(x\\) and \\(y\\).\nNote that \\(\\tilde{x}\\) is imperfectly correlated with the treatment: \\(\\beta_1\\) can’t be interpreted directly\nThe actual effect will be \\(\\frac{\\beta_1}{\\beta_0}\\) (in 1d)\n\n\nWe say that we instrument \\(x\\) by \\(q\\)."
  },
  {
    "objectID": "session_6/transcript.html#choosing-a-good-instrument",
    "href": "session_6/transcript.html#choosing-a-good-instrument",
    "title": "Introduction to Instrumental Variables",
    "section": "Choosing a good instrument",
    "text": "Choosing a good instrument\n\n\n\n\n\n\n\nChoosing an instrumental variable\n\n\n\nA good instrument when trying to explain y by x, is a variable that is correlated to the treatment (x) but does not have any effect on the outcome of interest (y), appart from its effect through x."
  },
  {
    "objectID": "session_6/transcript.html#in-practice",
    "href": "session_6/transcript.html#in-practice",
    "title": "Introduction to Instrumental Variables",
    "section": "In practice",
    "text": "In practice\n\nBoth statsmodels and linearmodels support instrumental variables\n\nlibrary (look for IV2SLS)\n\nLibrary linearmodels has a handy formula syntax: salary ~ 1 + [war ~ draft]\n\nAPI is similar but not exactly identical to statsmodels\nfor instance linearmodels does not include constants by default\n\nExample from the doc\n\nformula = (\n    \"np.log(drugexp) ~ 1 + totchr + age + linc + blhisp + [hi_empunion ~ ssiratio]\"\n)\nols = IV2SLS.from_formula(formula, data)\nols_res = ols.fit(cov_type=\"robust\")\nprint(ols_res)"
  },
  {
    "objectID": "session_7/graphs/inference.html",
    "href": "session_7/graphs/inference.html",
    "title": "Data-Based Economics",
    "section": "",
    "text": "from matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n\ndef generate_dataset(μ1, μ2, α, β, σ, N=10):\n    xvec = np.random.uniform(μ1, μ2, N)\n    yvec = α + β*xvec + np.random.normal(size=N)*σ\n    return pd.DataFrame({'x': xvec, 'y': yvec})\n\n\ndf = generate_dataset(0.0, 1.0, 0.1, 0.8, 0.1)\n\n\nplt.plot(df['x'], df['y'], 'o')\nplt.grid()\n\n\n\n\n\ndef plot_distribution(α, β, σ, N=100000, μ1=0.0, μ2=1.0):\n    xvec = np.random.uniform(μ1, μ2, N)\n    yvec = α + β*xvec + np.random.normal(size=N)*σ\n    plt.plot(xvec, yvec, '.r', alpha=0.005)\n    plt.plot(xvec, α + β*xvec, color='black')\n\n# missing ridge line\n\n\nimport statsmodels\n\n\nμ1 = 0\nμ2 = 1.0\nα = 0.1\nβ = 0.8\nσ = 0.2\nN = 20\nK = 1000\n\n\nimport statsmodels.formula.api as smf\n\n\ndf = generate_dataset(μ1, μ2, α, β, σ, N=N)\n\n\nres = smf.ols(formula='y ~ x + 1', data=df).fit()\nparams = res.params\nαhat = params['Intercept']\nβhat = params['x']\nσhat = res.resid.std()\n\n\nres.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\ny\nR-squared:\n0.692\n\n\nModel:\nOLS\nAdj. R-squared:\n0.675\n\n\nMethod:\nLeast Squares\nF-statistic:\n40.48\n\n\nDate:\nTue, 26 Jan 2021\nProb (F-statistic):\n5.41e-06\n\n\nTime:\n04:02:36\nLog-Likelihood:\n7.6662\n\n\nNo. Observations:\n20\nAIC:\n-11.33\n\n\nDf Residuals:\n18\nBIC:\n-9.341\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n0.1210\n0.077\n1.565\n0.135\n-0.041\n0.283\n\n\nx\n0.7941\n0.125\n6.362\n0.000\n0.532\n1.056\n\n\n\n\n\n\nOmnibus:\n1.410\nDurbin-Watson:\n1.507\n\n\nProb(Omnibus):\n0.494\nJarque-Bera (JB):\n0.890\n\n\nSkew:\n-0.081\nProb(JB):\n0.641\n\n\nKurtosis:\n1.979\nCond. No.\n4.20\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nres.predict(df['x'])\n\n0     0.326200\n1     0.211704\n2     0.798819\n3     0.603306\n4     0.573319\n5     0.823919\n6     0.740622\n7     0.503227\n8     0.292622\n9     0.489566\n10    0.138720\n11    0.355157\n12    0.594171\n13    0.883917\n14    0.266229\n15    0.827021\n16    0.912376\n17    0.163088\n18    0.684858\n19    0.732782\ndtype: float64\n\n\n\nfor i in [1,2,3]:\n    \n    fig = plt.figure(figsize=(10,14))\n    plt.subplot(311)\n    plot_distribution(0.1, 0.8, 0.2)\n    plt.grid()\n    plt.title(f\"True Distribution: $y = {α:.2f} + {β:.2f} x + {σ:.2f} u$\")\n    plt.xlim(0,1)\n    plt.ylim(-0.5, 1.5)\n\n    plt.subplot(312)\n    plt.xlim(0,1)\n    plt.ylim(-0.5, 1.5)\n    if i&gt;=2:\n        plt.plot(df['x'], df['y'], 'o')\n    if i&gt;=3:\n        plt.plot(df['x'], res.predict(), label=f'$\\hat{{α}}={αhat:.2f}; \\hat{{β}}={βhat:.2f}$')\n        plt.legend(loc='lower right')\n    plt.title(\"Random Draw\")\n    plt.grid()\n    \n    plt.savefig(f\"regression_uncertainty_{i}.png\")\n\n\n\n\n\n\n\n\n\n\n\nimport scipy.stats\n\n\ndatasets = [generate_dataset(μ1, μ2, αhat, βhat, σhat, N=N) for i in range(K)]\nall_params = [smf.ols(formula='x ~ y + 1', data=df).fit() for df in datasets]\nαvec = np.array( [e.params['Intercept'] for e in all_params] )\nβvec = np.array( [e.params['y'] for e in all_params] )\n\n\ngkd = scipy.stats.kde.gaussian_kde(βvec)\n\n\nfor i in [1,2,3,4,5,6,7,8,9,10,100]:\n\n    fig = plt.figure(figsize=(10,14))\n    plt.subplot(311)\n    plot_distribution(0.1, 0.8, 0.2)\n    plt.grid()\n    plt.title(f\"True Distribution: $y = {αhat:.2f} + {βhat:.2f} x + {σhat:.2f} u$\")\n    plt.xlim(0,1)\n    plt.ylim(-0.5, 1.5)\n    \n    plt.subplot(312)\n    plt.xlim(0,1)\n    plt.ylim(-0.5, 1.5)\n    df = datasets[i]\n    if i&gt;=2:\n        plt.plot(df['x'], df['y'], 'o')\n    plt.title(\"Random Draw\")\n    plt.grid()\n\n    plt.subplot(313)\n    if i==3:\n        plt.plot(βvec[i], βvec[i]*0, 'o')\n    if i&gt;4:\n        plt.plot(βvec[3:i], βvec[3:i]*0, 'o')\n    if i&gt;10:\n        xx = np.linspace(0.2, 1.4, 10000)\n        plt.plot( βvec, gkd.pdf(βvec), '.')\n    plt.title(\"Distribution of β\")\n    plt.xlim(0.2, 1.4)\n    plt.ylim(-0.1, 4)\n    plt.grid()\n\n    plt.tight_layout()\n\n    plt.savefig(f\"random_estimates_{i}.png\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplt.plot( βvec, βvec*0, 'o')"
  },
  {
    "objectID": "session_7/graphs/Untitled1.html",
    "href": "session_7/graphs/Untitled1.html",
    "title": "Data-Based Economics",
    "section": "",
    "text": "from matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n\nimport statsmodels.api as sm\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nincome\neducation\nprestige\n\n\n\n\ncount\n45.000000\n45.000000\n45.000000\n\n\nmean\n41.866667\n52.555556\n47.688889\n\n\nstd\n24.435072\n29.760831\n31.510332\n\n\nmin\n7.000000\n7.000000\n3.000000\n\n\n25%\n21.000000\n26.000000\n16.000000\n\n\n50%\n42.000000\n45.000000\n41.000000\n\n\n75%\n64.000000\n84.000000\n81.000000\n\n\nmax\n81.000000\n100.000000\n97.000000\n\n\n\n\n\n\n\n\ndf.cov()\n\n\n\n\n\n\n\n\nincome\neducation\nprestige\n\n\n\n\nincome\n597.072727\n526.871212\n645.071212\n\n\neducation\n526.871212\n885.707071\n798.904040\n\n\nprestige\n645.071212\n798.904040\n992.901010\n\n\n\n\n\n\n\n\nfrom matplotlib import pyplot as plt\n\n\nplt.figure(figsize=(8,6))\nplt.plot(df['education'],df['income'],'o')\nplt.grid()\nplt.xlabel(\"x (Education)\")\nplt.ylabel(\"y (Income)\")\nplt.savefig(\"data_description.png\")\n\n\n\n\n\nfor i in [1,2,3]:\n    xvec = np.linspace(10,100)\n\n    plt.figure(figsize=(12,8))\n    plt.plot(df['education'],df['income'],'o')\n\n    plt.plot(xvec, xvec * 0 + 50)\n    if i&gt;=2:\n        plt.plot(xvec, xvec )\n    if i&gt;=3:\n        plt.plot(xvec,  90- 0.6*xvec )\n\n    plt.grid()\n    plt.xlabel(\"x (Education)\")\n    plt.ylabel(\"y (Income)\")\n    plt.savefig(f\"which_line_{i}.png\")\n\n\n\n\n\n\n\n\n\n\n\nfrom ipywidgets import interact\n\n\nimport matplotlib.patches as patches\n\n\na = 0.1\nb = 1.0\nind = 23\n\n\napprox =  a + b*xvec\n\n# Create figure and axes\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\nplt.plot(df['education'],df['income'],'o')\nplt.plot(xvec, approx, color='red')\n\nx, y = df['education'][ind], df['income'][ind]\nplt.plot(x, y, 'o', color='red' )\np = a+b*x\nplt.grid(True)\nh = abs(p-y)\nplt.vlines(x, y+h, y, color='red')\n\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.savefig(f\"error_0.png\")\n\n\n\n\n\nplt.vlines?\n\n\nSignature:\nplt.vlines(\n    x,\n    ymin,\n    ymax,\n    colors=None,\n    linestyles='solid',\n    label='',\n    *,\n    data=None,\n    **kwargs,\n)\nDocstring:\nPlot vertical lines.\nPlot vertical lines at each *x* from *ymin* to *ymax*.\nParameters\n----------\nx : float or array-like\n    x-indexes where to plot the lines.\nymin, ymax : float or array-like\n    Respective beginning and end of each line. If scalars are\n    provided, all lines will have same length.\ncolors : list of colors, default: :rc:`lines.color`\nlinestyles : {'solid', 'dashed', 'dashdot', 'dotted'}, optional\nlabel : str, default: ''\nReturns\n-------\n`~matplotlib.collections.LineCollection`\nOther Parameters\n----------------\n**kwargs : `~matplotlib.collections.LineCollection` properties.\nSee Also\n--------\nhlines : horizontal lines\naxvline: vertical line across the axes\nNotes\n-----\n.. note::\n    In addition to the above described arguments, this function can take\n    a *data* keyword argument. If such a *data* argument is given,\n    the following arguments can also be string ``s``, which is\n    interpreted as ``data[s]`` (unless this raises an exception):\n    *x*, *ymin*, *ymax*, *colors*.\n    Objects passed as **data** must support item access (``data[s]``) and\n    membership test (``s in data``).\nFile:      ~/.local/opt/miniconda/lib/python3.8/site-packages/matplotlib/pyplot.py\nType:      function\n\n\n\n\n\na = 0.1\nb = 1.0\nind = 23\n\n\napprox =  a + b*xvec\n\n# Create figure and axes\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\nplt.plot(df['education'],df['income'],'o')\nplt.plot(xvec, approx, color='red')\n\nx, y = df['education'][ind], df['income'][ind]\nplt.plot(x, y, 'o', color='red' )\np = a+b*x\nplt.grid(True)\nh = abs(p-y)\nif p-y&gt;0:\n    # Create a Rectangle patch\n    rect = patches.Rectangle((x,y),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n    ax.add_patch(rect)\n    \nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.savefig(f\"errors_{1}.png\")\n\n\n\n\n\ndef L(a,b):\n    Δ = a + b*df['education'] - df['income']\n    return (Δ**2).sum()\n\n\na = 0.1\nb = 0.8\n\napprox =  a + b*xvec\n\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\n\n\nplt.plot(df['education'],df['income'],'o', label=f\"L({a,b})={L(a,b)}\")\nplt.plot(xvec, approx, color='red')\n\nplt.grid(True)\nfor ind in range(df.shape[0]):\n    \n    x, y = df['education'][ind], df['income'][ind]\n    p = a+b*x\n\n    h = abs(p-y)\n    if p-y&gt;0:\n        # Create a Rectangle patch\n        rect = patches.Rectangle((x,y),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\n    else:\n        rect = patches.Rectangle((x,y-h),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.legend(loc='upper right')\nplt.savefig(f\"errors_2.png\")\n\n\n\n\n\na = 90\nb = -0.6\n\napprox =  a + b*xvec\n\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\n\n\nplt.plot(df['education'],df['income'],'o', label=f\"L({a,b})={L(a,b)}\")\nplt.plot(xvec, approx, color='red')\n\nplt.grid(True)\nfor ind in range(df.shape[0]):\n    \n    x, y = df['education'][ind], df['income'][ind]\n    p = a+b*x\n\n    h = abs(p-y)\n    if p-y&gt;0:\n        # Create a Rectangle patch\n        rect = patches.Rectangle((x,y),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\n    else:\n        rect = patches.Rectangle((x,y-h),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.legend(loc='upper right')\nplt.savefig(f\"errors_3.png\")\n\n\n\n\n\nimport scipy.optimize\n\n\nscipy.optimize.minimize(lambda x: L(x[0], x[1]),np.array([0.5, 0.5]))\n\n      fun: 12480.970174488397\n hess_inv: array([[ 7.14169839e-09, -3.91281920e-09],\n       [-3.91281920e-09,  2.46663613e-09]])\n      jac: array([0.00024414, 0.00012207])\n  message: 'Desired error not necessarily achieved due to precision loss.'\n     nfev: 57\n      nit: 7\n     njev: 19\n   status: 2\n  success: False\n        x: array([10.60350224,  0.59485938])\n\n\n\na = 10\nb = 0.59\n\napprox =  a + b*xvec\n\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\n\n\nplt.plot(df['education'],df['income'],'o', label=f\"L({a,b})={L(a,b)}\")\nplt.plot(xvec, approx, color='red')\n\nplt.grid(True)\nfor ind in range(df.shape[0]):\n    \n    x, y = df['education'][ind], df['income'][ind]\n    p = a+b*x\n\n    h = abs(p-y)\n    if p-y&gt;0:\n        # Create a Rectangle patch\n        rect = patches.Rectangle((x,y),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\n    else:\n        rect = patches.Rectangle((x,y-h),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.legend(loc='upper right')\nplt.savefig(f\"errors_4.png\")\n\n\n\n\n\na = 10\nb = 0.59\n\napprox =  a + b*xvec\n\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\n\n\nplt.plot(df['education'],df['income'],'o', label=f\"L({a,b})={L(a,b)}\")\nplt.plot(xvec, approx, color='red', alpha=0.5)\n\nplt.plot(60, a + b*60, 'o', color='red',)\n\nprint(a+b*60)\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.legend(loc='upper right')\nplt.savefig(f\"prediction.png\")\n\n45.4\n\n\n\n\n\n\na = 10\nb = 0.59\n\napprox =  (a + b*df['education'] - df['income'])\n\nplt.figure(figsize=(12,6))\n\nplt.subplot(121)\nplt.plot(approx)\nplt.grid(False)\nplt.title(\"Residuals\")\n\n\nplt.subplot(122)\ndistplot(approx)\nplt.title(\"Distribution of residuals\")\nplt.grid()\n\nplt.savefig(\"residuals.png\")\n\n/home/pablo/.local/opt/miniconda/lib/python3.8/site-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n  warnings.warn(msg, FutureWarning)\n\n\n\n\n\n\n(a + b*df['education'] - df['income']).std()\n\n16.842782676352154\n\n\n\n\n\n/home/pablo/.local/opt/miniconda/lib/python3.8/site-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n  warnings.warn(msg, FutureWarning)\n\n\n&lt;AxesSubplot:ylabel='Density'&gt;\n\n\n\n\n\n\nfrom scipy.stats import f\n\n\nf(0.3)\n\nTypeError: _parse_args() missing 1 required positional argument: 'dfd'\n\n\n\nnp.rand\n\n\nK = 100\nxvec = np.linspace(0,1,K)\ne1 = np.random.randn(K)*0.1\nyvec = 0.1 + xvec*0.4 + e1\ne2 = np.random.randn(K)*0.05\nyvec2 = 0.1 + xvec*(xvec-1)/2 + e2\ne3 = np.random.randn(K)*xvec/2\nyvec3 = 0.1 + xvec + e3\n\nyvec4 = 0.1 + np.sin(xvec*6) + np.random.randn(K)*xvec/2\n\n\nfrom dolo.numeric.processes import VAR1\n\n\nsim = VAR1( ρ=0.8, Σ=0.001).simulate(N=1,T=100)\nyvec4 = 0.1 + xvec*0.4 + sim.ravel()\n\n\nplt.figure(figsize=(18,6))\nplt.subplot(241)\nplt.plot(xvec, yvec,'o')\nplt.plot(xvec, 0.1 + xvec*0.4 )\nplt.ylabel(\"Series\")\nplt.title(\"white noise\")\nplt.subplot(242)\nplt.plot(xvec, yvec2, 'o')\nplt.plot(xvec, yvec2*0)\nplt.title('nonlinear')\nplt.subplot(243)\nplt.plot(xvec, yvec3,'o')\nplt.plot(xvec, 0.1 + xvec)\nplt.title('heteroskedastic')\nplt.subplot(244)\nplt.plot(xvec, yvec4,'o')\nplt.plot(xvec, xvec*0.6)\n\nplt.title('correlated')\n\n\nplt.subplot(245)\nplt.plot(xvec, e1,'o')\nplt.ylabel(\"Residuals\")\nplt.subplot(246)\nplt.plot(xvec, yvec2-0.075, 'o')\n\nplt.subplot(247)\nplt.plot(xvec, e3,'o')\nplt.subplot(248)\nplt.plot(xvec, sim.ravel(),'o')\n\nplt.tight_layout()\n\nplt.savefig(\"residuals_circus.png\")"
  },
  {
    "objectID": "session_7/slides.html#regressions",
    "href": "session_7/slides.html#regressions",
    "title": "Introduction to Machine Learning",
    "section": "Regressions",
    "text": "Regressions"
  },
  {
    "objectID": "session_7/slides.html#what-is-machine-learning-1",
    "href": "session_7/slides.html#what-is-machine-learning-1",
    "title": "Introduction to Machine Learning",
    "section": "What is Machine learning?",
    "text": "What is Machine learning?\nDefinition Candidates:\nArthur Samuel: Field of study that gives computers the ability to learn without being explicitly programmed\nTom Mitchell: A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E."
  },
  {
    "objectID": "session_7/slides.html#what-about-artificial-intelligence",
    "href": "session_7/slides.html#what-about-artificial-intelligence",
    "title": "Introduction to Machine Learning",
    "section": "What about artificial intelligence ?",
    "text": "What about artificial intelligence ?\n\n\n\nAIs\n\nthink and learn\nmimmic human cognition"
  },
  {
    "objectID": "session_7/slides.html#econometrics-vs-machine-learning",
    "href": "session_7/slides.html#econometrics-vs-machine-learning",
    "title": "Introduction to Machine Learning",
    "section": "Econometrics vs Machine Learning",
    "text": "Econometrics vs Machine Learning\n\nEconometrics is essentially a subfield of machine learning with a different jargon and a focus on:\n\nstudying properties and validity of results\n\ndata is scarce\ninference\n\nsingling out effects of specific explanatory variables\nestablishing causality\n\nMachine learning:\n\nstructure data\nmake predictions (interpolate data)"
  },
  {
    "objectID": "session_7/slides.html#data-types",
    "href": "session_7/slides.html#data-types",
    "title": "Introduction to Machine Learning",
    "section": "Data types",
    "text": "Data types\n\nstructured:\n\ntabular\n\nlong\nwide\n\n\nunstructured:\n\nfiles\nnetworks\ntext, mails\nimages, sound"
  },
  {
    "objectID": "session_7/slides.html#tabular-data",
    "href": "session_7/slides.html#tabular-data",
    "title": "Introduction to Machine Learning",
    "section": "Tabular Data",
    "text": "Tabular Data\n\ntabular data"
  },
  {
    "objectID": "session_7/slides.html#networks",
    "href": "session_7/slides.html#networks",
    "title": "Introduction to Machine Learning",
    "section": "Networks",
    "text": "Networks\n\nBanking networks\nProduction network"
  },
  {
    "objectID": "session_7/slides.html#big-data-1",
    "href": "session_7/slides.html#big-data-1",
    "title": "Introduction to Machine Learning",
    "section": "Big Data",
    "text": "Big Data\n\nBig data:\n\nwide data (K&gt;&gt;N)\nlong data (N&gt;&gt;K)\nheterogenous, unstructured data\n\nMight not even fit in memory\n\nout of core computations\nlearn from a subset of the data"
  },
  {
    "objectID": "session_7/slides.html#big-subfields-of-machine-learning",
    "href": "session_7/slides.html#big-subfields-of-machine-learning",
    "title": "Introduction to Machine Learning",
    "section": "Big Subfields of Machine Learning",
    "text": "Big Subfields of Machine Learning\n\n\n\nTraditional classification\n\nsupervised (labelled data)\n\nregression: predict quantity\nclassification: predict index (categorical variable)\n\nunsupervised (no labels)\n\ndimension reduction\nclustering\n\nsemi-supervised / self-supervised\nreinforcement learning\n\nBazillions of different algorithms: https://scikit-learn.org/stable/user_guide.html\n\n\n\n\n\nregression:\n\nPredict: \\(y = f(x; \\theta)\\)\n\n\n\n\n\nsupervised: regression\n\n\n\n\n\n\nAge\n\n\nActivity\n\n\nSalary\n\n\n\n\n23\n\n\nExplorer\n\n\n1200\n\n\n\n\n40\n\n\nMortician\n\n\n2000\n\n\n\n\n45\n\n\nMortician\n\n\n2500\n\n\n\n\n33\n\n\nMovie Star\n\n\n3000\n\n\n\n\n35\n\n\nExplorer\n\n\n???\n\n\n\n\n\n\n\nsupervised: classification\n\nOutput is discrete\nRegular trick: \\(\\sigma(f(x; \\theta))\\) where \\(\\sigma(x)=\\frac{1}{1-e^{-x}}\\)\n\n\n\n\n\nclassification\n\n\n\n\n\n\nAge\n\n\nSalary\n\n\nActivity\n\n\n\n\n23\n\n\n1200\n\n\nExplorer\n\n\n\n\n40\n\n\n2000\n\n\nMortician\n\n\n\n\n45\n\n\n2500\n\n\nMortician\n\n\n\n\n33\n\n\n3000\n\n\nMovie Star\n\n\n\n\n35\n\n\n3000\n\n\n???\n\n\n\n\n\n\nunsupervised\n\norganize data without labels\n\ndimension reduction: describe data with less parameters\nclustering: sort data into “similar groups” (exemple)\n\n\n\n\n\n\nAge\n\n\nSalary\n\n\nActivity\n\n\n\n\n23\n\n\n1200\n\n\nExplorer\n\n\n\n\n40\n\n\n2000\n\n\nMortician\n\n\n\n\n45\n\n\n2500\n\n\nMortician\n\n\n\n\n33\n\n\n3000\n\n\nMovie Star\n\n\n\n\n35\n\n\n3000\n\n\nExplorer\n\n\n\n\n\n\nunsupervised: clustering\n\n\n\nkmeansclustering\n\n\n\n\nunsupervised: clustering\nWomen buying dresses during the year:"
  },
  {
    "objectID": "session_7/slides.html#difference-with-traditional-regression",
    "href": "session_7/slides.html#difference-with-traditional-regression",
    "title": "Introduction to Machine Learning",
    "section": "Difference with traditional regression",
    "text": "Difference with traditional regression\n\\[\\underbrace{y}_{\\text{explained variable}} = a \\underbrace{x}_{\\text{explanatory variable}} + b\\]"
  },
  {
    "objectID": "session_7/slides.html#difference-with-traditional-regression-1",
    "href": "session_7/slides.html#difference-with-traditional-regression-1",
    "title": "Introduction to Machine Learning",
    "section": "Difference with traditional regression",
    "text": "Difference with traditional regression\n\\[\\underbrace{y}_{\\text{labels}} = a \\underbrace{x}_{\\text{features}} + b\\]\n\n\n\n\n\n\n\n\nEconometrics\nMachine learning\n\n\n\n\nRegressand / independent variable / explanatory variable\nFeatures\n\n\nRegressor / dependent variable / explained variable\nLabels\n\n\nRegression\nModel Training"
  },
  {
    "objectID": "session_7/slides.html#difference-with-traditional-regression-2",
    "href": "session_7/slides.html#difference-with-traditional-regression-2",
    "title": "Introduction to Machine Learning",
    "section": "Difference with traditional regression",
    "text": "Difference with traditional regression\n\nBig data requires other means to process the data:\n\ndata is long: so many observations \\(x\\) doesn’t fit in the memory\n\nneed to use incremental training method to use only a subsample at a time\n\ndata is wide: so many features, the model is crudely overspecified\n\nneed to build dimension reduction into the objective\n\ndata is nonlinear:\n\nuse nonlinear model (and nonlinear training)\n\ndata is not a simple vector…\n\nsame as nonlinear"
  },
  {
    "objectID": "session_7/slides.html#long-data",
    "href": "session_7/slides.html#long-data",
    "title": "Introduction to Machine Learning",
    "section": "Long data",
    "text": "Long data"
  },
  {
    "objectID": "session_7/slides.html#long-data-1",
    "href": "session_7/slides.html#long-data-1",
    "title": "Introduction to Machine Learning",
    "section": "Long data",
    "text": "Long data\nLong data is characterized by a high number of observations."
  },
  {
    "objectID": "session_7/slides.html#long-data-2",
    "href": "session_7/slides.html#long-data-2",
    "title": "Introduction to Machine Learning",
    "section": "Long data",
    "text": "Long data\n\n\n\nTraditional regression:\n\nfull sample \\(X,Y=(x_i,y_i)_{i=1:N}\\)\nOLS: \\(\\min_{a,b} \\sum_{i=1}^N (a x_i + b - y_i)^2\\)\nclosed-form solution: \\(a = X^{\\prime}X Y\\) and \\(b= ...\\)\nhard to compute if \\(X\\) is very big\n\n\n\n\n\nIncremental learning:\n\ngiven initial \\(a_n\\), \\(b_n\\)\npick \\(N\\) random observations (the batch)\n\nregress them to get new estimate \\(a\\), \\(b\\)\nthis minimizes the square of errors\n\nupdate with learning rate \\(\\beta\\):\n\n\\(a_{n+1} \\leftarrow a_n (1-\\beta_n) + \\beta_n a\\)\n\\(b_{n+1} \\leftarrow b_n (1-\\beta_n) + \\beta_n b\\)\n\nprocess is not biased (that is \\(a\\) converges to the true value) as long as one decreases \\(\\beta\\) sufficiently fast over time (ex: \\(\\beta_n=\\frac{1}{n}\\))"
  },
  {
    "objectID": "session_7/slides.html#formalisation-a-typical-machine-learning-task",
    "href": "session_7/slides.html#formalisation-a-typical-machine-learning-task",
    "title": "Introduction to Machine Learning",
    "section": "Formalisation: a typical machine learning task",
    "text": "Formalisation: a typical machine learning task\n\nvector of unknowns: \\(\\theta=(a,b)\\)\ndataset \\(X,Y=(x_i,y_i)_{i=1:N}\\)\nfor a random draw \\(\\omega = (a_{\\sigma(i)}, b_{\\sigma(i)})_{i=[1,N]} \\subset (X,Y)\\)\n\n\\(\\omega\\) is just a random batch of size \\(N\\)\n\ndefine the empirical risk (or empirical cost) \\[\\xi(\\theta, \\omega) = \\sum_{(x,y) \\in \\omega} (y - (a x + b))^2\\]\nwe want to minimize theoretical risk: \\[\\Xi(\\theta) = \\mathbb{E} \\left[ \\xi(\\theta, \\omega)\\right]\\]"
  },
  {
    "objectID": "session_7/slides.html#training-gradient-descent",
    "href": "session_7/slides.html#training-gradient-descent",
    "title": "Introduction to Machine Learning",
    "section": "Training: Gradient Descent",
    "text": "Training: Gradient Descent\n\n\n\nHow do we minimize a function \\(f(a,b)\\)?\nGradient descent:\n\n\\(a_k, b_k\\) given\ncompute the gradient (slope) \\(\\nabla_{a,b} f = \\begin{bmatrix} \\frac{\\partial f}{\\partial a} \\\\\\\\ \\frac{\\partial f}{\\partial b}\\end{bmatrix}\\)\nfollow the steepest slope: (Newton Algorithm)\n\n\\[ \\begin{bmatrix} a_{k+1} \\\\\\\\ b_{k+1} \\end{bmatrix} \\leftarrow  \\begin{bmatrix} a_k \\\\\\\\ b_k \\end{bmatrix} - \\nabla_{a,b} f\\]\n\nbut not too fast: use learning rate \\(\\lambda\\): \\[ \\begin{bmatrix} a_{k+1} \\\\\\\\ b_{k+1} \\end{bmatrix} \\leftarrow  (1-\\lambda) \\begin{bmatrix} a_k \\\\\\\\ b_k \\end{bmatrix} + \\lambda (- \\nabla_{a,b} f )\\]"
  },
  {
    "objectID": "session_7/slides.html#not-everything-goes-wrong-all-the-time",
    "href": "session_7/slides.html#not-everything-goes-wrong-all-the-time",
    "title": "Introduction to Machine Learning",
    "section": "Not everything goes wrong all the time",
    "text": "Not everything goes wrong all the time\n \n\nIn practice, choosing the right learning rate \\(\\lambda\\) is crucial\n\\(\\lambda\\) is a metaparameter of the model training."
  },
  {
    "objectID": "session_7/slides.html#wide-data",
    "href": "session_7/slides.html#wide-data",
    "title": "Introduction to Machine Learning",
    "section": "Wide data",
    "text": "Wide data\n\nWide Data is characterized by a high number of features compared to the number of observations.\n\n\nProblem: - with many independent variables \\(x_1, ... x_K\\), \\(K&gt;&gt;N\\) and one dependent variable \\(y\\) the regression \\[y = a_1 x_1 + a_2 x_2 + \\cdots + a_N x_N + b\\] is grossly overidentified."
  },
  {
    "objectID": "session_7/slides.html#wide-data-regression",
    "href": "session_7/slides.html#wide-data-regression",
    "title": "Introduction to Machine Learning",
    "section": "Wide data regression",
    "text": "Wide data regression\n\nMain Idea: penalize non-zero coefficients to encourage scarcity\n\nRidge: \\[\\Xi(a,b) = \\min_{a,b} \\sum_{i=1}^N ( \\sum_j a_j x_j + b - y_i)^2 + \\mu \\sum_i |a_i|^2\\]\n\nshrinks parameters towards zero\nclosed form\n\nLasso: \\[\\Xi(a,b) = \\min_{a,b} \\sum_{i=1}^N (\\sum_j a_j x_j + b - y_i)^2 + \\mu \\sum_i |a_i|\\]\n\neliminates zero coefficients\n\nElastic: Ridge + Lasso\n\nRemarks:\n\n\\(\\mu\\) is called a regularization term.\nit is a hyperparameter\n\\(\\mu \\uparrow\\), bias increases, variance decreases"
  },
  {
    "objectID": "session_7/slides.html#training",
    "href": "session_7/slides.html#training",
    "title": "Introduction to Machine Learning",
    "section": "Training",
    "text": "Training\nTo perform Lasso and ridge regression:\n\nAI approach:\n\nminimize objective \\(\\Xi(a,b)\\) directly.\napproach is known as (stochastic) Gradient Descent\n\nUse special algorithms"
  },
  {
    "objectID": "session_7/slides.html#example-imf-challenge",
    "href": "session_7/slides.html#example-imf-challenge",
    "title": "Introduction to Machine Learning",
    "section": "Example: IMF challenge",
    "text": "Example: IMF challenge\n\nAn internal IMF challenge to predict crises in countries\nLots of different approaches\nLots of data:\n\nwhich one is relevant\nmachine must select relevant informations\n\nExample: Lasso Regressions and Forecasting Models in Applied Stress Testing by Jorge A. Chan-Lau\n\nin a given developing country\ntries to predict probability of default in various sectors"
  },
  {
    "objectID": "session_7/slides.html#nonlinear-regression-1",
    "href": "session_7/slides.html#nonlinear-regression-1",
    "title": "Introduction to Machine Learning",
    "section": "Nonlinear Regression",
    "text": "Nonlinear Regression\n\nSo far, we have assumed,\n\n\\(y_i = a + b x_i\\)\n\\(y_i = a + b x_i + μ_1 (a^2 + b^2) + μ_2 (|a| + |b|)\\)\ndefined \\(\\Xi(a,b)\\) and tried to minimize it\n\nSame approach works for fully nonlinear models\n\n\\(y_i = a x_i + a^2 x_i^2 + c\\)\n\\(y_i = \\varphi(x; \\theta)\\) ()\n\nSpecial case: neural network:\n\nprimer tensor playground"
  },
  {
    "objectID": "session_7/slides.html#how-to-evaluate-the-machine-learning",
    "href": "session_7/slides.html#how-to-evaluate-the-machine-learning",
    "title": "Introduction to Machine Learning",
    "section": "how to evaluate the machine learning",
    "text": "how to evaluate the machine learning\nIn machine learning we can’t perform statistical inference easily. How do we assess the validity of a model?\n\nBasic idea (independent of how complex the algorithm is)\n\nseparate data in\n\ntraining set (in-sample)\ntest set (out of sample)\n\ntrain using only the training set\nevaluate performance on the test set\n\nPerformance can be:\n\nfitness, number of classification errors (false positive, false negative)"
  },
  {
    "objectID": "session_7/slides.html#how-to-evaluate-the-machine-learning-1",
    "href": "session_7/slides.html#how-to-evaluate-the-machine-learning-1",
    "title": "Introduction to Machine Learning",
    "section": "how to evaluate the machine learning",
    "text": "how to evaluate the machine learning\nIn case the training method depends itself on many parameters (the hyperparameters) we make three samples instead:\n\ntraining set (in-sample)\nvalidation set (to update hyperparameters)\ntest set (out of sample)\n\nGolden Rule: the test set should not be used to estimate the model, and should not affect the choice any training parameter (hyperparameter)."
  },
  {
    "objectID": "session_7/slides.html#section",
    "href": "session_7/slides.html#section",
    "title": "Introduction to Machine Learning",
    "section": "",
    "text": "Traintest\nThe test set reveals that orange model is overfitting."
  },
  {
    "objectID": "session_7/slides.html#how-to-choose-the-validation-set",
    "href": "session_7/slides.html#how-to-choose-the-validation-set",
    "title": "Introduction to Machine Learning",
    "section": "How to choose the validation set?",
    "text": "How to choose the validation set?\n\nHoldout validation approach:\n\nkeeps x% of the data for the training, (100-x)% for the test\n\nHow to choose the sizes of the subsets?\n\nsmall dataset: 90-10\nbig data set: 70-30 (we can afford to waste more training data for the test)\n\n\n\n\nProblem:\n\nare we sure the validation size is correct? Are the results determined by an (un-) lucky draw?\na problem for smaller datasets"
  },
  {
    "objectID": "session_7/slides.html#how-to-choose-the-validation-set-1",
    "href": "session_7/slides.html#how-to-choose-the-validation-set-1",
    "title": "Introduction to Machine Learning",
    "section": "How to choose the validation set?",
    "text": "How to choose the validation set?\nA more robust solution: \\(k\\)-fold validation\n\n\n\nsplit dataset randomly in \\(K\\) subsets of equal size \\(S_1, ... S_K\\)\nuse subset \\(S_i\\) as test set, the rest as training set, compute the score\ncompare the scores obtained for all \\(i\\in[1,K]\\)\n\nthey should be similar (compute standard deviation)\n\naverage them"
  },
  {
    "objectID": "session_7/slides.html#wait",
    "href": "session_7/slides.html#wait",
    "title": "Introduction to Machine Learning",
    "section": "Wait",
    "text": "Wait\n\nAnother library to do regression ?\nstatsmodels:\n\nexplanatory analysis\nstatistical tests\nformula interface for many estimation algorithms\n\nstateless approach (model.fit() returns another object)\n\n\nlinearmodels\n\nextends statsmodels (very similar interface)\n\n(panel models, IV, systems…)\n\n\nsklearn:\n\nprediction\nfaster for big datasets\ncommon interface for several machine learning tasks\n\nstateful approach (model is modified by .fit operation)\n\ndefacto standard for machine learning"
  },
  {
    "objectID": "session_7/slides.html#in-practice",
    "href": "session_7/slides.html#in-practice",
    "title": "Introduction to Machine Learning",
    "section": "In practice",
    "text": "In practice\n\n\nBasic sklearn workflow:\n\n\nimport data\n\nfeatures: a matrix X (2d numpy array)\nlabels: a vector y (1d numpy array)\n\nsplit the data, between training and test datasets\n\nsplit needs to be random to avoid any bias\n\nnormalize the data\n\nmost ML algorithm are sensitive to scale\n\ncreate a model (independent from data)\ntrain the model on training dataset\nevaluate accuracy on test dataset (here \\(R^2\\))\nuse the model to make predictions\n\n\nThe workflow is always the same, no matter what the model is\n\ntry sklearn.linear_model.Lasso instead of LinearRegression\n\n\nfrom sklearn.datasets import load_diabetes\ndataset = load_diabetes()\nX = dataset['data']\ny = dataset['target']\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.1)\n\n#Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\nmodel.score(X_test, y_test)\nmodel.predict(X_new)"
  },
  {
    "objectID": "session_7/slides.html#k-fold-validation-with-sklearn",
    "href": "session_7/slides.html#k-fold-validation-with-sklearn",
    "title": "Introduction to Machine Learning",
    "section": "\\(k\\)-fold validation with sklearn",
    "text": "\\(k\\)-fold validation with sklearn\nfrom sklearn.model_selection import KFold\nkf = KFold(n_splits=10)\n\nfor train_index, test_index in kf.split(X):\n   X_train, X_test = X[train_index], X[test_index]\n   y_train, y_test = y[train_index], y[test_index]\n\n   ## train a model in X_train, y_train\n   ## test it on X_test, y_test"
  },
  {
    "objectID": "session_7/index.html#what-is-machine-learning-1",
    "href": "session_7/index.html#what-is-machine-learning-1",
    "title": "Introduction to Machine Learning",
    "section": "What is Machine learning?",
    "text": "What is Machine learning?\nDefinition Candidates:\nArthur Samuel: Field of study that gives computers the ability to learn without being explicitly programmed\nTom Mitchell: A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E."
  },
  {
    "objectID": "session_7/index.html#what-about-artificial-intelligence",
    "href": "session_7/index.html#what-about-artificial-intelligence",
    "title": "Introduction to Machine Learning",
    "section": "What about artificial intelligence ?",
    "text": "What about artificial intelligence ?\n\n\n\nAIs\n\nthink and learn\nmimmic human cognition"
  },
  {
    "objectID": "session_7/index.html#econometrics-vs-machine-learning",
    "href": "session_7/index.html#econometrics-vs-machine-learning",
    "title": "Introduction to Machine Learning",
    "section": "Econometrics vs Machine Learning",
    "text": "Econometrics vs Machine Learning\n\nEconometrics is essentially a subfield of machine learning with a different jargon and a focus on:\n\nstudying properties and validity of results\n\ndata is scarce\ninference\n\nsingling out effects of specific explanatory variables\nestablishing causality\n\nMachine learning:\n\nstructure data\nmake predictions (interpolate data)"
  },
  {
    "objectID": "session_7/index.html#data-types",
    "href": "session_7/index.html#data-types",
    "title": "Introduction to Machine Learning",
    "section": "Data types",
    "text": "Data types\n\nstructured:\n\ntabular\n\nlong\nwide\n\n\nunstructured:\n\nfiles\nnetworks\ntext, mails\nimages, sound"
  },
  {
    "objectID": "session_7/index.html#tabular-data",
    "href": "session_7/index.html#tabular-data",
    "title": "Introduction to Machine Learning",
    "section": "Tabular Data",
    "text": "Tabular Data\n\n\n\ntabular data"
  },
  {
    "objectID": "session_7/index.html#networks",
    "href": "session_7/index.html#networks",
    "title": "Introduction to Machine Learning",
    "section": "Networks",
    "text": "Networks\n\nBanking networks\nProduction network"
  },
  {
    "objectID": "session_7/index.html#big-data-1",
    "href": "session_7/index.html#big-data-1",
    "title": "Introduction to Machine Learning",
    "section": "Big Data",
    "text": "Big Data\n\nBig data:\n\nwide data (K&gt;&gt;N)\nlong data (N&gt;&gt;K)\nheterogenous, unstructured data\n\nMight not even fit in memory\n\nout of core computations\nlearn from a subset of the data"
  },
  {
    "objectID": "session_7/index.html#big-subfields-of-machine-learning",
    "href": "session_7/index.html#big-subfields-of-machine-learning",
    "title": "Introduction to Machine Learning",
    "section": "Big Subfields of Machine Learning",
    "text": "Big Subfields of Machine Learning\n\n\n\nTraditional classification\n\nsupervised (labelled data)\n\nregression: predict quantity\nclassification: predict index (categorical variable)\n\nunsupervised (no labels)\n\ndimension reduction\nclustering\n\nsemi-supervised / self-supervised\nreinforcement learning\n\nBazillions of different algorithms: https://scikit-learn.org/stable/user_guide.html\n\n\n\n\n\nregression:\n\nPredict: \\(y = f(x; \\theta)\\)\n\n\n\n\n\nsupervised: regression\n\n\n\n\n\n\nAge\n\n\nActivity\n\n\nSalary\n\n\n\n\n23\n\n\nExplorer\n\n\n1200\n\n\n\n\n40\n\n\nMortician\n\n\n2000\n\n\n\n\n45\n\n\nMortician\n\n\n2500\n\n\n\n\n33\n\n\nMovie Star\n\n\n3000\n\n\n\n\n35\n\n\nExplorer\n\n\n???\n\n\n\n\n\n\n\nsupervised: classification\n\nOutput is discrete\nRegular trick: \\(\\sigma(f(x; \\theta))\\) where \\(\\sigma(x)=\\frac{1}{1-e^{-x}}\\)\n\n\n\n\n\nclassification\n\n\n\n\n\n\nAge\n\n\nSalary\n\n\nActivity\n\n\n\n\n23\n\n\n1200\n\n\nExplorer\n\n\n\n\n40\n\n\n2000\n\n\nMortician\n\n\n\n\n45\n\n\n2500\n\n\nMortician\n\n\n\n\n33\n\n\n3000\n\n\nMovie Star\n\n\n\n\n35\n\n\n3000\n\n\n???\n\n\n\n\n\n\nunsupervised\n\norganize data without labels\n\ndimension reduction: describe data with less parameters\nclustering: sort data into “similar groups” (exemple)\n\n\n\n\n\n\nAge\n\n\nSalary\n\n\nActivity\n\n\n\n\n23\n\n\n1200\n\n\nExplorer\n\n\n\n\n40\n\n\n2000\n\n\nMortician\n\n\n\n\n45\n\n\n2500\n\n\nMortician\n\n\n\n\n33\n\n\n3000\n\n\nMovie Star\n\n\n\n\n35\n\n\n3000\n\n\nExplorer\n\n\n\n\n\n\nunsupervised: clustering\n\n\n\nkmeansclustering\n\n\n\n\nunsupervised: clustering\nWomen buying dresses during the year:"
  },
  {
    "objectID": "session_7/index.html#difference-with-traditional-regression",
    "href": "session_7/index.html#difference-with-traditional-regression",
    "title": "Introduction to Machine Learning",
    "section": "Difference with traditional regression",
    "text": "Difference with traditional regression\n\\[\\underbrace{y}_{\\text{explained variable}} = a \\underbrace{x}_{\\text{explanatory variable}} + b\\]"
  },
  {
    "objectID": "session_7/index.html#difference-with-traditional-regression-1",
    "href": "session_7/index.html#difference-with-traditional-regression-1",
    "title": "Introduction to Machine Learning",
    "section": "Difference with traditional regression",
    "text": "Difference with traditional regression\n\\[\\underbrace{y}_{\\text{labels}} = a \\underbrace{x}_{\\text{features}} + b\\]\n. . .\n\n\n\n\n\n\n\nEconometrics\nMachine learning\n\n\n\n\nRegressand / independent variable / explanatory variable\nFeatures\n\n\nRegressor / dependent variable / explained variable\nLabels\n\n\nRegression\nModel Training"
  },
  {
    "objectID": "session_7/index.html#difference-with-traditional-regression-2",
    "href": "session_7/index.html#difference-with-traditional-regression-2",
    "title": "Introduction to Machine Learning",
    "section": "Difference with traditional regression",
    "text": "Difference with traditional regression\n\nBig data requires other means to process the data:\n\ndata is long: so many observations \\(x\\) doesn’t fit in the memory\n\nneed to use incremental training method to use only a subsample at a time\n\ndata is wide: so many features, the model is crudely overspecified\n\nneed to build dimension reduction into the objective\n\ndata is nonlinear:\n\nuse nonlinear model (and nonlinear training)\n\ndata is not a simple vector…\n\nsame as nonlinear"
  },
  {
    "objectID": "session_7/index.html#long-data",
    "href": "session_7/index.html#long-data",
    "title": "Introduction to Machine Learning",
    "section": "Long data",
    "text": "Long data"
  },
  {
    "objectID": "session_7/index.html#long-data-1",
    "href": "session_7/index.html#long-data-1",
    "title": "Introduction to Machine Learning",
    "section": "Long data",
    "text": "Long data\nLong data is characterized by a high number of observations."
  },
  {
    "objectID": "session_7/index.html#long-data-2",
    "href": "session_7/index.html#long-data-2",
    "title": "Introduction to Machine Learning",
    "section": "Long data",
    "text": "Long data\n\n\n\nTraditional regression:\n\nfull sample \\(X,Y=(x_i,y_i)_{i=1:N}\\)\nOLS: \\(\\min_{a,b} \\sum_{i=1}^N (a x_i + b - y_i)^2\\)\nclosed-form solution: \\(a = X^{\\prime}X Y\\) and \\(b= ...\\)\nhard to compute if \\(X\\) is very big\n\n\n\n\n\nIncremental learning:\n\ngiven initial \\(a_n\\), \\(b_n\\)\npick \\(N\\) random observations (the batch)\n\nregress them to get new estimate \\(a\\), \\(b\\)\nthis minimizes the square of errors\n\nupdate with learning rate \\(\\beta\\):\n\n\\(a_{n+1} \\leftarrow a_n (1-\\beta_n) + \\beta_n a\\)\n\\(b_{n+1} \\leftarrow b_n (1-\\beta_n) + \\beta_n b\\)\n\nprocess is not biased (that is \\(a\\) converges to the true value) as long as one decreases \\(\\beta\\) sufficiently fast over time (ex: \\(\\beta_n=\\frac{1}{n}\\))"
  },
  {
    "objectID": "session_7/index.html#formalisation-a-typical-machine-learning-task",
    "href": "session_7/index.html#formalisation-a-typical-machine-learning-task",
    "title": "Introduction to Machine Learning",
    "section": "Formalisation: a typical machine learning task",
    "text": "Formalisation: a typical machine learning task\n\nvector of unknowns: \\(\\theta=(a,b)\\)\ndataset \\(X,Y=(x_i,y_i)_{i=1:N}\\)\nfor a random draw \\(\\omega = (a_{\\sigma(i)}, b_{\\sigma(i)})_{i=[1,N]} \\subset (X,Y)\\)\n\n\\(\\omega\\) is just a random batch of size \\(N\\)\n\ndefine the empirical risk (or empirical cost) \\[\\xi(\\theta, \\omega) = \\sum_{(x,y) \\in \\omega} (y - (a x + b))^2\\]\nwe want to minimize theoretical risk: \\[\\Xi(\\theta) = \\mathbb{E} \\left[ \\xi(\\theta, \\omega)\\right]\\]"
  },
  {
    "objectID": "session_7/index.html#training-gradient-descent",
    "href": "session_7/index.html#training-gradient-descent",
    "title": "Introduction to Machine Learning",
    "section": "Training: Gradient Descent",
    "text": "Training: Gradient Descent\n\n\n\nHow do we minimize a function \\(f(a,b)\\)?\nGradient descent:\n\n\\(a_k, b_k\\) given\ncompute the gradient (slope) \\(\\nabla_{a,b} f = \\begin{bmatrix} \\frac{\\partial f}{\\partial a} \\\\\\\\ \\frac{\\partial f}{\\partial b}\\end{bmatrix}\\)\nfollow the steepest slope: (Newton Algorithm)\n\n\\[ \\begin{bmatrix} a_{k+1} \\\\\\\\ b_{k+1} \\end{bmatrix} \\leftarrow  \\begin{bmatrix} a_k \\\\\\\\ b_k \\end{bmatrix} - \\nabla_{a,b} f\\]\n\nbut not too fast: use learning rate \\(\\lambda\\): \\[ \\begin{bmatrix} a_{k+1} \\\\\\\\ b_{k+1} \\end{bmatrix} \\leftarrow  (1-\\lambda) \\begin{bmatrix} a_k \\\\\\\\ b_k \\end{bmatrix} + \\lambda (- \\nabla_{a,b} f )\\]"
  },
  {
    "objectID": "session_7/index.html#not-everything-goes-wrong-all-the-time",
    "href": "session_7/index.html#not-everything-goes-wrong-all-the-time",
    "title": "Introduction to Machine Learning",
    "section": "Not everything goes wrong all the time",
    "text": "Not everything goes wrong all the time\n \n\nIn practice, choosing the right learning rate \\(\\lambda\\) is crucial\n\\(\\lambda\\) is a metaparameter of the model training."
  },
  {
    "objectID": "session_7/index.html#wide-data",
    "href": "session_7/index.html#wide-data",
    "title": "Introduction to Machine Learning",
    "section": "Wide data",
    "text": "Wide data\n\nWide Data is characterized by a high number of features compared to the number of observations.\n\n. . .\nProblem: - with many independent variables \\(x_1, ... x_K\\), \\(K&gt;&gt;N\\) and one dependent variable \\(y\\) the regression \\[y = a_1 x_1 + a_2 x_2 + \\cdots + a_N x_N + b\\] is grossly overidentified."
  },
  {
    "objectID": "session_7/index.html#wide-data-regression",
    "href": "session_7/index.html#wide-data-regression",
    "title": "Introduction to Machine Learning",
    "section": "Wide data regression",
    "text": "Wide data regression\n\nMain Idea: penalize non-zero coefficients to encourage scarcity\n\nRidge: \\[\\Xi(a,b) = \\min_{a,b} \\sum_{i=1}^N ( \\sum_j a_j x_j + b - y_i)^2 + \\mu \\sum_i |a_i|^2\\]\n\nshrinks parameters towards zero\nclosed form\n\nLasso: \\[\\Xi(a,b) = \\min_{a,b} \\sum_{i=1}^N (\\sum_j a_j x_j + b - y_i)^2 + \\mu \\sum_i |a_i|\\]\n\neliminates zero coefficients\n\nElastic: Ridge + Lasso\n\nRemarks:\n\n\\(\\mu\\) is called a regularization term.\nit is a hyperparameter\n\\(\\mu \\uparrow\\), bias increases, variance decreases"
  },
  {
    "objectID": "session_7/index.html#training",
    "href": "session_7/index.html#training",
    "title": "Introduction to Machine Learning",
    "section": "Training",
    "text": "Training\nTo perform Lasso and ridge regression:\n\nAI approach:\n\nminimize objective \\(\\Xi(a,b)\\) directly.\napproach is known as (stochastic) Gradient Descent\n\nUse special algorithms"
  },
  {
    "objectID": "session_7/index.html#example-imf-challenge",
    "href": "session_7/index.html#example-imf-challenge",
    "title": "Introduction to Machine Learning",
    "section": "Example: IMF challenge",
    "text": "Example: IMF challenge\n\nAn internal IMF challenge to predict crises in countries\nLots of different approaches\nLots of data:\n\nwhich one is relevant\nmachine must select relevant informations\n\nExample: Lasso Regressions and Forecasting Models in Applied Stress Testing by Jorge A. Chan-Lau\n\nin a given developing country\ntries to predict probability of default in various sectors"
  },
  {
    "objectID": "session_7/index.html#nonlinear-regression-1",
    "href": "session_7/index.html#nonlinear-regression-1",
    "title": "Introduction to Machine Learning",
    "section": "Nonlinear Regression",
    "text": "Nonlinear Regression\n\nSo far, we have assumed,\n\n\\(y_i = a + b x_i\\)\n\\(y_i = a + b x_i + μ_1 (a^2 + b^2) + μ_2 (|a| + |b|)\\)\ndefined \\(\\Xi(a,b)\\) and tried to minimize it\n\nSame approach works for fully nonlinear models\n\n\\(y_i = a x_i + a^2 x_i^2 + c\\)\n\\(y_i = \\varphi(x; \\theta)\\) ()\n\nSpecial case: neural network:\n\nprimer tensor playground"
  },
  {
    "objectID": "session_7/index.html#how-to-evaluate-the-machine-learning",
    "href": "session_7/index.html#how-to-evaluate-the-machine-learning",
    "title": "Introduction to Machine Learning",
    "section": "how to evaluate the machine learning",
    "text": "how to evaluate the machine learning\nIn machine learning we can’t perform statistical inference easily. How do we assess the validity of a model?\n\nBasic idea (independent of how complex the algorithm is)\n\nseparate data in\n\ntraining set (in-sample)\ntest set (out of sample)\n\ntrain using only the training set\nevaluate performance on the test set\n\nPerformance can be:\n\nfitness, number of classification errors (false positive, false negative)"
  },
  {
    "objectID": "session_7/index.html#how-to-evaluate-the-machine-learning-1",
    "href": "session_7/index.html#how-to-evaluate-the-machine-learning-1",
    "title": "Introduction to Machine Learning",
    "section": "how to evaluate the machine learning",
    "text": "how to evaluate the machine learning\nIn case the training method depends itself on many parameters (the hyperparameters) we make three samples instead:\n\ntraining set (in-sample)\nvalidation set (to update hyperparameters)\ntest set (out of sample)\n\nGolden Rule: the test set should not be used to estimate the model, and should not affect the choice any training parameter (hyperparameter)."
  },
  {
    "objectID": "session_7/index.html#section",
    "href": "session_7/index.html#section",
    "title": "Introduction to Machine Learning",
    "section": "",
    "text": "Traintest\n\n\n. . .\nThe test set reveals that orange model is overfitting."
  },
  {
    "objectID": "session_7/index.html#how-to-choose-the-validation-set",
    "href": "session_7/index.html#how-to-choose-the-validation-set",
    "title": "Introduction to Machine Learning",
    "section": "How to choose the validation set?",
    "text": "How to choose the validation set?\n\nHoldout validation approach:\n\nkeeps x% of the data for the training, (100-x)% for the test\n\nHow to choose the sizes of the subsets?\n\nsmall dataset: 90-10\nbig data set: 70-30 (we can afford to waste more training data for the test)\n\n\n. . .\n\nProblem:\n\nare we sure the validation size is correct? Are the results determined by an (un-) lucky draw?\na problem for smaller datasets"
  },
  {
    "objectID": "session_7/index.html#how-to-choose-the-validation-set-1",
    "href": "session_7/index.html#how-to-choose-the-validation-set-1",
    "title": "Introduction to Machine Learning",
    "section": "How to choose the validation set?",
    "text": "How to choose the validation set?\nA more robust solution: \\(k\\)-fold validation\n\n\n\nsplit dataset randomly in \\(K\\) subsets of equal size \\(S_1, ... S_K\\)\nuse subset \\(S_i\\) as test set, the rest as training set, compute the score\ncompare the scores obtained for all \\(i\\in[1,K]\\)\n\nthey should be similar (compute standard deviation)\n\naverage them"
  },
  {
    "objectID": "session_7/index.html#wait",
    "href": "session_7/index.html#wait",
    "title": "Introduction to Machine Learning",
    "section": "Wait",
    "text": "Wait\n\nAnother library to do regression ?\nstatsmodels:\n\nexplanatory analysis\nstatistical tests\nformula interface for many estimation algorithms\n\nstateless approach (model.fit() returns another object)\n\n\nlinearmodels\n\nextends statsmodels (very similar interface)\n\n(panel models, IV, systems…)\n\n\nsklearn:\n\nprediction\nfaster for big datasets\ncommon interface for several machine learning tasks\n\nstateful approach (model is modified by .fit operation)\n\ndefacto standard for machine learning"
  },
  {
    "objectID": "session_7/index.html#in-practice",
    "href": "session_7/index.html#in-practice",
    "title": "Introduction to Machine Learning",
    "section": "In practice",
    "text": "In practice\n\n\nBasic sklearn workflow:\n\n\nimport data\n\nfeatures: a matrix X (2d numpy array)\nlabels: a vector y (1d numpy array)\n\nsplit the data, between training and test datasets\n\nsplit needs to be random to avoid any bias\n\nnormalize the data\n\nmost ML algorithm are sensitive to scale\n\ncreate a model (independent from data)\ntrain the model on training dataset\nevaluate accuracy on test dataset (here \\(R^2\\))\nuse the model to make predictions\n\n\nThe workflow is always the same, no matter what the model is\n\ntry sklearn.linear_model.Lasso instead of LinearRegression\n\n\nfrom sklearn.datasets import load_diabetes\ndataset = load_diabetes()\nX = dataset['data']\ny = dataset['target']\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.1)\n\n#Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\nmodel.score(X_test, y_test)\nmodel.predict(X_new)"
  },
  {
    "objectID": "session_7/index.html#k-fold-validation-with-sklearn",
    "href": "session_7/index.html#k-fold-validation-with-sklearn",
    "title": "Introduction to Machine Learning",
    "section": "\\(k\\)-fold validation with sklearn",
    "text": "\\(k\\)-fold validation with sklearn\nfrom sklearn.model_selection import KFold\nkf = KFold(n_splits=10)\n\nfor train_index, test_index in kf.split(X):\n   X_train, X_test = X[train_index], X[test_index]\n   y_train, y_test = y[train_index], y[test_index]\n\n   ## train a model in X_train, y_train\n   ## test it on X_test, y_test"
  },
  {
    "objectID": "session_7/machine_learning_regressions.html",
    "href": "session_7/machine_learning_regressions.html",
    "title": "Machine learning: regressions",
    "section": "",
    "text": "Objectives:"
  },
  {
    "objectID": "session_7/machine_learning_regressions.html#diabetes-dataset-basic-regression",
    "href": "session_7/machine_learning_regressions.html#diabetes-dataset-basic-regression",
    "title": "Machine learning: regressions",
    "section": "Diabetes dataset: basic regression",
    "text": "Diabetes dataset: basic regression\nImport the diabetes dataset from sklearn. Describe it.\nSplit the dataset into a training set (70%) and a test set (30%)\nTrain a linear model (with intercept) on the training set\nCompute the fitting score on the test set. (Bonus: compare with your own computation of \\(R^2\\))\nShould we adjust the size of the test set? What would be the problem?\nImplement \\(k\\)-fold model with \\(k=3\\).\nBonus: use statsmodels (or linearmodels) to estimate the same linear model on the full sample. Is it always a superior method?"
  },
  {
    "objectID": "session_7/machine_learning_regressions.html#sparse-regressions-on-the-boston-house-price-dataset",
    "href": "session_7/machine_learning_regressions.html#sparse-regressions-on-the-boston-house-price-dataset",
    "title": "Machine learning: regressions",
    "section": "Sparse regressions on the Boston House Price Dataset",
    "text": "Sparse regressions on the Boston House Price Dataset\nImport the Boston House Price Dataset from sklearn. Explore the data (description, correlations, histograms…)\nSplit the dataset into a training set (70%) and a test set (30%).\nTrain a lasso model to predict house prices. Compute the score on the test set.\nTrain a ridge model to predict house prices. Which one is better?\n(bonus) Use statsmodels to build a model predicting house prices. What is the problem?"
  },
  {
    "objectID": "session_7/machine_learning_regressions_correction.html",
    "href": "session_7/machine_learning_regressions_correction.html",
    "title": "Machine learning: regressions",
    "section": "",
    "text": "Objectives:"
  },
  {
    "objectID": "session_7/machine_learning_regressions_correction.html#diabetes-dataset-basic-regression",
    "href": "session_7/machine_learning_regressions_correction.html#diabetes-dataset-basic-regression",
    "title": "Machine learning: regressions",
    "section": "Diabetes dataset: basic regression",
    "text": "Diabetes dataset: basic regression\nImport the diabetes dataset from sklearn. Describe it.\n\nimport sklearn\nimport sklearn.datasets\n\ndataset = sklearn.datasets.load_diabetes()\n# the result is a dictionary:\n# 'data': features\n# 'target' labels\n# 'feature_names': names of the features\n# `DESCR`: description\n\n\nprint( dataset['DESCR'] )\n\n.. _diabetes_dataset:\n\nDiabetes dataset\n----------------\n\nTen baseline variables, age, sex, body mass index, average blood\npressure, and six blood serum measurements were obtained for each of n =\n442 diabetes patients, as well as the response of interest, a\nquantitative measure of disease progression one year after baseline.\n\n**Data Set Characteristics:**\n\n  :Number of Instances: 442\n\n  :Number of Attributes: First 10 columns are numeric predictive values\n\n  :Target: Column 11 is a quantitative measure of disease progression one year after baseline\n\n  :Attribute Information:\n      - age     age in years\n      - sex\n      - bmi     body mass index\n      - bp      average blood pressure\n      - s1      tc, total serum cholesterol\n      - s2      ldl, low-density lipoproteins\n      - s3      hdl, high-density lipoproteins\n      - s4      tch, total cholesterol / HDL\n      - s5      ltg, possibly log of serum triglycerides level\n      - s6      glu, blood sugar level\n\nNote: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times the square root of `n_samples` (i.e. the sum of squares of each column totals 1).\n\nSource URL:\nhttps://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\n\nFor more information see:\nBradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\n(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\n\n\n\n\n# create a dataframe\nimport pandas\n\ndf = pandas.DataFrame(dataset['data'], columns=dataset['feature_names'])\n\ndf['disease_progression'] = dataset['target']\n\n\ndf.describe()\n# we observe that mean of varaibles  is zero\n# standard deviations are the same for all variables\n# model has been normalized already:\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nbp\ns1\ns2\ns3\ns4\ns5\ns6\ndisease_progression\n\n\n\n\ncount\n4.420000e+02\n4.420000e+02\n4.420000e+02\n4.420000e+02\n4.420000e+02\n4.420000e+02\n4.420000e+02\n4.420000e+02\n4.420000e+02\n4.420000e+02\n442.000000\n\n\nmean\n-2.511817e-19\n1.230790e-17\n-2.245564e-16\n-4.797570e-17\n-1.381499e-17\n3.918434e-17\n-5.777179e-18\n-9.042540e-18\n9.293722e-17\n1.130318e-17\n152.133484\n\n\nstd\n4.761905e-02\n4.761905e-02\n4.761905e-02\n4.761905e-02\n4.761905e-02\n4.761905e-02\n4.761905e-02\n4.761905e-02\n4.761905e-02\n4.761905e-02\n77.093005\n\n\nmin\n-1.072256e-01\n-4.464164e-02\n-9.027530e-02\n-1.123988e-01\n-1.267807e-01\n-1.156131e-01\n-1.023071e-01\n-7.639450e-02\n-1.260971e-01\n-1.377672e-01\n25.000000\n\n\n25%\n-3.729927e-02\n-4.464164e-02\n-3.422907e-02\n-3.665608e-02\n-3.424784e-02\n-3.035840e-02\n-3.511716e-02\n-3.949338e-02\n-3.324559e-02\n-3.317903e-02\n87.000000\n\n\n50%\n5.383060e-03\n-4.464164e-02\n-7.283766e-03\n-5.670422e-03\n-4.320866e-03\n-3.819065e-03\n-6.584468e-03\n-2.592262e-03\n-1.947171e-03\n-1.077698e-03\n140.500000\n\n\n75%\n3.807591e-02\n5.068012e-02\n3.124802e-02\n3.564379e-02\n2.835801e-02\n2.984439e-02\n2.931150e-02\n3.430886e-02\n3.243232e-02\n2.791705e-02\n211.500000\n\n\nmax\n1.107267e-01\n5.068012e-02\n1.705552e-01\n1.320436e-01\n1.539137e-01\n1.987880e-01\n1.811791e-01\n1.852344e-01\n1.335973e-01\n1.356118e-01\n346.000000\n\n\n\n\n\n\n\n\nimport seaborn\n\n\nseaborn.pairplot(df)\n\n\n\n\nSplit the dataset into a training set (70%) and a test set (30%)\n\nfrom sklearn.model_selection import train_test_split\n\n\n# features: dataset['data']\n# dataset['data'].shape # one line per observation, one column per feature (variable)\n\n\n# labels: dataset['target'] what we are trying to predict\ndataset['target'].shape\n\n(442,)\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(dataset['data'], dataset['target'], test_size=0.3, random_state=56)\n# the choice of a random_state initializes a random seed so that every time it is run the notebook\n# returns exactly the same results\n\nTrain a linear model (with intercept) on the training set\n\n# since the model is already normalized, we can create the model directly\nfrom sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression()   # don't forget the round bracket to get a model object\nmodel.fit(X_train, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\n# visualize model predictions:\n\n# from matplotlib import pyplot as plt\n\n# plt.plot(  )\n# plt.plot( model.predict(X_train) )\n\n\nmodel.intercept_ # a\n\n152.82810842206453\n\n\n\nmodel.coef_ # b_1, b_2, .... b_10|\n\narray([   3.04174075, -209.76813682,  501.77871853,  286.88207011,\n       -991.92731799,  603.10838272,  228.80501285,  226.30296964,\n        905.67772303,   92.55739263])\n\n\nCompute the fitting score on the test set. (Bonus: compare with your own computation of \\(R^2\\))\n\nmodel.score(X_test, y_test)\n\n0.43965636272283437\n\n\n\n# compare with the training set:\nmodel.score(X_train, y_train)\n\n0.541861476456197\n\n\nShould we adjust the size of the test set? What would be the problem?\n\n#### WARNING\n####\n#### very bad approach\n\n\n# let's try different sizes\n\nsizes = [0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\nscores = []\nfor s in sizes:\n    X_train, X_test, y_train, y_test = train_test_split(dataset['data'], dataset['target'], test_size=0.3)\n\n    model = LinearRegression()   # don't forget the round bracket to get a model object\n    model.fit(X_train, y_train)\n    score = model.score(X_test, y_test) # score with x% test set\n\n    scores.append(score)\n\n\nfrom matplotlib import pyplot as plt\nplt.plot(sizes, scores)\n\n\n\n\nImplement \\(k\\)-fold model with \\(k=3\\).\n\nX = dataset['data']\ny = dataset['target']\n\n\n# to keep the scores\nscores = []\n\n\nfrom sklearn.model_selection import KFold\nkf = KFold(n_splits=3)\n\nfor train_index, test_index in kf.split(X):\n    \n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n    ## train a model in X_train, y_train\n    ## test it on X_test, y_test\n    model = LinearRegression()   # don't forget the round bracket to get a model object\n    model.fit(X_train, y_train)\n    score = model.score(X_test, y_test) # score with x% test set\n\n    scores.append(score)\n\n\nscores\n\n[0.46930417754348197, 0.4872526062543143, 0.5095496056127979]\n\n\n\n# it gives us a sense of the predictive power of the regression\n\nBonus: use statsmodels (or linearmodels) to estimate the same linear model on the full sample. Is it always a superior method?\n\nimport statsmodels\nfrom statsmodels.formula import api as smf\n\n\ndf.columns\n\nIndex(['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6',\n       'disease_progression'],\n      dtype='object')\n\n\n\nregmodel = smf.ols(formula=\"disease_progression ~ age + sex + bmi + bp + s1 + s2 + s3 + s4 + s5 + s6\", data=df)\nregresults = regmodel.fit()\n\n\nregresults.summary() # econometric estimation of R^2 is 0.51\n\n\nOLS Regression Results\n\n\nDep. Variable:\ndisease_progression\nR-squared:\n0.518\n\n\nModel:\nOLS\nAdj. R-squared:\n0.507\n\n\nMethod:\nLeast Squares\nF-statistic:\n46.27\n\n\nDate:\nMon, 27 Mar 2023\nProb (F-statistic):\n3.83e-62\n\n\nTime:\n21:46:43\nLog-Likelihood:\n-2386.0\n\n\nNo. Observations:\n442\nAIC:\n4794.\n\n\nDf Residuals:\n431\nBIC:\n4839.\n\n\nDf Model:\n10\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n152.1335\n2.576\n59.061\n0.000\n147.071\n157.196\n\n\nage\n-10.0099\n59.749\n-0.168\n0.867\n-127.446\n107.426\n\n\nsex\n-239.8156\n61.222\n-3.917\n0.000\n-360.147\n-119.484\n\n\nbmi\n519.8459\n66.533\n7.813\n0.000\n389.076\n650.616\n\n\nbp\n324.3846\n65.422\n4.958\n0.000\n195.799\n452.970\n\n\ns1\n-792.1756\n416.680\n-1.901\n0.058\n-1611.153\n26.802\n\n\ns2\n476.7390\n339.030\n1.406\n0.160\n-189.620\n1143.098\n\n\ns3\n101.0433\n212.531\n0.475\n0.635\n-316.684\n518.770\n\n\ns4\n177.0632\n161.476\n1.097\n0.273\n-140.315\n494.441\n\n\ns5\n751.2737\n171.900\n4.370\n0.000\n413.407\n1089.140\n\n\ns6\n67.6267\n65.984\n1.025\n0.306\n-62.064\n197.318\n\n\n\n\n\n\nOmnibus:\n1.506\nDurbin-Watson:\n2.029\n\n\nProb(Omnibus):\n0.471\nJarque-Bera (JB):\n1.404\n\n\nSkew:\n0.017\nProb(JB):\n0.496\n\n\nKurtosis:\n2.726\nCond. No.\n227.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nTo use lasso regression:\n\nfrom sklearn.linear_model import Lasso\n\n\nX_train, X_test, y_train, y_test = train_test_split(dataset['data'], dataset['target'], test_size=0.3)\nmodel = Lasso()   # don't forget the round bracket to get a model object\nmodel.fit(X_train, y_train)\nscore = model.score(X_test, y_test) # s\n\n\n# on the test set, the fit of the lasso regression is worse than regular regression\n# the regularization parameter should be changed"
  },
  {
    "objectID": "session_7/machine_learning_regressions_correction.html#sparse-regressions-on-the-boston-house-price-dataset",
    "href": "session_7/machine_learning_regressions_correction.html#sparse-regressions-on-the-boston-house-price-dataset",
    "title": "Machine learning: regressions",
    "section": "Sparse regressions on the Boston House Price Dataset",
    "text": "Sparse regressions on the Boston House Price Dataset\n!!! update: boston price dataset has been deprecated\n!!! use california_housing instead\nImport the Boston House Price Dataset from sklearn. Explore the data (description, correlations, histograms…)\n\n# dataset = sklearn.datasets.load_boston()\nfrom sklearn.datasets import fetch_california_housing \ndataset = fetch_california_housing()\n\n\nprint(dataset[\"DESCR\"])\n\n.. _california_housing_dataset:\n\nCalifornia Housing dataset\n--------------------------\n\n**Data Set Characteristics:**\n\n    :Number of Instances: 20640\n\n    :Number of Attributes: 8 numeric, predictive attributes and the target\n\n    :Attribute Information:\n        - MedInc        median income in block group\n        - HouseAge      median house age in block group\n        - AveRooms      average number of rooms per household\n        - AveBedrms     average number of bedrooms per household\n        - Population    block group population\n        - AveOccup      average number of household members\n        - Latitude      block group latitude\n        - Longitude     block group longitude\n\n    :Missing Attribute Values: None\n\nThis dataset was obtained from the StatLib repository.\nhttps://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\n\nThe target variable is the median house value for California districts,\nexpressed in hundreds of thousands of dollars ($100,000).\n\nThis dataset was derived from the 1990 U.S. census, using one row per census\nblock group. A block group is the smallest geographical unit for which the U.S.\nCensus Bureau publishes sample data (a block group typically has a population\nof 600 to 3,000 people).\n\nAn household is a group of people residing within a home. Since the average\nnumber of rooms and bedrooms in this dataset are provided per household, these\ncolumns may take surpinsingly large values for block groups with few households\nand many empty houses, such as vacation resorts.\n\nIt can be downloaded/loaded using the\n:func:`sklearn.datasets.fetch_california_housing` function.\n\n.. topic:: References\n\n    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n      Statistics and Probability Letters, 33 (1997) 291-297\n\n\n\n\n# dataset = sklearn.datasets.load_boston()\nfrom sklearn.datasets import fetch_california_housing \ndataset = fetch_california_housing()\n\nSplit the dataset into a training set (70%) and a test set (30%).\n\nX = dataset['data']\ny = dataset['target']\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=58)\n\nTrain a lasso model to predict house prices. Compute the score on the test set.\n\n# we should check that the data is normalized, or normalize it ourselves\n\n\nfrom sklearn.linear_model import Lasso\nmodel_lasso = Lasso()\nmodel_lasso.fit(X_train, y_train)\n\nLasso()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LassoLasso()\n\n\n\nmodel_lasso.score(X_test, y_test)\n\n0.28204855993177635\n\n\nTrain a ridge model to predict house prices. Which one is better?\n\nfrom sklearn.linear_model import Ridge\nmodel_ridge = Ridge()\nmodel_ridge.fit(X_train, y_train)\n\nRidge()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RidgeRidge()\n\n\n\nmodel_ridge.score(X_test, y_test)\n\n0.6060031802405054\n\n\nIt looks like the ridge model has a better fit (score). However, we should have left a test set appart and not used it at all during training phase. Here it has influenced the choice of the model (between ridge and lasso)."
  },
  {
    "objectID": "session_7/machine_learning_regressions_elements.html",
    "href": "session_7/machine_learning_regressions_elements.html",
    "title": "Intro to sklearn",
    "section": "",
    "text": "Objectives:"
  },
  {
    "objectID": "session_7/machine_learning_regressions_elements.html#diabetes-dataset",
    "href": "session_7/machine_learning_regressions_elements.html#diabetes-dataset",
    "title": "Intro to sklearn",
    "section": "Diabetes dataset",
    "text": "Diabetes dataset\nImport the diabetes dataset from sklearn. Describe it.\n\nfrom sklearn.datasets import load_diabetes\ndata = load_diabetes()\nX = data['data']\nY = data['target']\n\n\nprint(data['DESCR'])\n\n.. _diabetes_dataset:\n\nDiabetes dataset\n----------------\n\nTen baseline variables, age, sex, body mass index, average blood\npressure, and six blood serum measurements were obtained for each of n =\n442 diabetes patients, as well as the response of interest, a\nquantitative measure of disease progression one year after baseline.\n\n**Data Set Characteristics:**\n\n  :Number of Instances: 442\n\n  :Number of Attributes: First 10 columns are numeric predictive values\n\n  :Target: Column 11 is a quantitative measure of disease progression one year after baseline\n\n  :Attribute Information:\n      - age     age in years\n      - sex\n      - bmi     body mass index\n      - bp      average blood pressure\n      - s1      tc, T-Cells (a type of white blood cells)\n      - s2      ldl, low-density lipoproteins\n      - s3      hdl, high-density lipoproteins\n      - s4      tch, thyroid stimulating hormone\n      - s5      ltg, lamotrigine\n      - s6      glu, blood sugar level\n\nNote: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times `n_samples` (i.e. the sum of squares of each column totals 1).\n\nSource URL:\nhttps://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\n\nFor more information see:\nBradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\n(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\n\n\nSplit the dataset into a training set (70%) and a test set (30%)\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.3)\n\n\nX_train.shape\n\n(309, 10)\n\n\n\nX_test.shape\n\n(133, 10)\n\n\n\n133/(133+309)\n\n0.3009049773755656\n\n\nFeatures are already “centered and scaled”: no need to renormalize them\nTrain a linear model (with intercept) on the training set\n\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\n# by default there is an intercept (check the doc: default value for fit_intercept is True)\n\n\nLinearRegression?\n\n\nInit signature:\nLinearRegression(\n    *,\n    fit_intercept=True,\n    normalize=False,\n    copy_X=True,\n    n_jobs=None,\n    positive=False,\n)\nDocstring:     \nOrdinary least squares Linear Regression.\nLinearRegression fits a linear model with coefficients w = (w1, ..., wp)\nto minimize the residual sum of squares between the observed targets in\nthe dataset, and the targets predicted by the linear approximation.\nParameters\n----------\nfit_intercept : bool, default=True\n    Whether to calculate the intercept for this model. If set\n    to False, no intercept will be used in calculations\n    (i.e. data is expected to be centered).\nnormalize : bool, default=False\n    This parameter is ignored when ``fit_intercept`` is set to False.\n    If True, the regressors X will be normalized before regression by\n    subtracting the mean and dividing by the l2-norm.\n    If you wish to standardize, please use\n    :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n    on an estimator with ``normalize=False``.\ncopy_X : bool, default=True\n    If True, X will be copied; else, it may be overwritten.\nn_jobs : int, default=None\n    The number of jobs to use for the computation. This will only provide\n    speedup for n_targets &gt; 1 and sufficient large problems.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary &lt;n_jobs&gt;`\n    for more details.\npositive : bool, default=False\n    When set to ``True``, forces the coefficients to be positive. This\n    option is only supported for dense arrays.\n    .. versionadded:: 0.24\nAttributes\n----------\ncoef_ : array of shape (n_features, ) or (n_targets, n_features)\n    Estimated coefficients for the linear regression problem.\n    If multiple targets are passed during the fit (y 2D), this\n    is a 2D array of shape (n_targets, n_features), while if only\n    one target is passed, this is a 1D array of length n_features.\nrank_ : int\n    Rank of matrix `X`. Only available when `X` is dense.\nsingular_ : array of shape (min(X, y),)\n    Singular values of `X`. Only available when `X` is dense.\nintercept_ : float or array of shape (n_targets,)\n    Independent term in the linear model. Set to 0.0 if\n    `fit_intercept = False`.\nSee Also\n--------\nRidge : Ridge regression addresses some of the\n    problems of Ordinary Least Squares by imposing a penalty on the\n    size of the coefficients with l2 regularization.\nLasso : The Lasso is a linear model that estimates\n    sparse coefficients with l1 regularization.\nElasticNet : Elastic-Net is a linear regression\n    model trained with both l1 and l2 -norm regularization of the\n    coefficients.\nNotes\n-----\nFrom the implementation point of view, this is just plain Ordinary\nLeast Squares (scipy.linalg.lstsq) or Non Negative Least Squares\n(scipy.optimize.nnls) wrapped as a predictor object.\nExamples\n--------\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n&gt;&gt;&gt; X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n&gt;&gt;&gt; # y = 1 * x_0 + 2 * x_1 + 3\n&gt;&gt;&gt; y = np.dot(X, np.array([1, 2])) + 3\n&gt;&gt;&gt; reg = LinearRegression().fit(X, y)\n&gt;&gt;&gt; reg.score(X, y)\n1.0\n&gt;&gt;&gt; reg.coef_\narray([1., 2.])\n&gt;&gt;&gt; reg.intercept_\n3.0000...\n&gt;&gt;&gt; reg.predict(np.array([[3, 5]]))\narray([16.])\nFile:           ~/.local/opt/miniconda/lib/python3.8/site-packages/sklearn/linear_model/_base.py\nType:           ABCMeta\nSubclasses:     \n\n\n\n\n\nmodel.fit(X_train, Y_train)\n\nLinearRegression()\n\n\n\nmodel.fit?\n\n\nSignature: model.fit(X, y, sample_weight=None)\nDocstring:\nFit linear model.\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training data\ny : array-like of shape (n_samples,) or (n_samples, n_targets)\n    Target values. Will be cast to X's dtype if necessary\nsample_weight : array-like of shape (n_samples,), default=None\n    Individual weights for each sample\n    .. versionadded:: 0.17\n       parameter *sample_weight* support to LinearRegression.\nReturns\n-------\nself : returns an instance of self.\nFile:      ~/.local/opt/miniconda/lib/python3.8/site-packages/sklearn/linear_model/_base.py\nType:      method\n\n\n\n\nCompute the fitting score on the test set. (Bonus: compare with your own computation of \\(R^2\\))\n\npred = model.predict(X_test)\n\n\nY_test\n\narray([190., 225., 141., 281., 168.,  42., 116., 276., 281.,  51., 121.,\n       156., 163., 142., 187., 173.,  39., 229., 155., 332., 257., 261.,\n        42., 201., 103.,  47., 142., 172.,  71.,  96., 173., 127., 150.,\n       230., 185., 209.,  37.,  92., 235., 131.,  97.,  40., 279.,  97.,\n       245., 258., 102., 168.,  51., 248.,  88.,  91.,  58., 110., 308.,\n        88.,  60., 311., 246., 310., 214., 200., 220., 131.,  72.,  72.,\n       181.,  89., 163., 104.,  96.,  70., 217.,  55., 317., 259.,  50.,\n       118., 200.,  25., 124., 129., 179., 109.,  71., 102., 252., 189.,\n       190., 263., 174., 259., 111.,  85., 145.,  85., 252., 258., 274.,\n        83., 140., 196., 219., 200., 197.,  51.,  66.,  79., 275.,  78.,\n       257., 180., 202.,  71., 122., 136., 270.,  70., 146., 281., 114.,\n        59., 191.,  91.,  65., 143., 185., 243.,  53.,  99., 125., 139.,\n       292.])\n\n\n\nfrom matplotlib import pyplot as plt\n\n\nplt.plot(Y_test, pred,'o')\nplt.xlabel(\"True value\")\nplt.ylabel(\"Prediction\")\nplt.title(\"Out of sample test\")\n\nText(0.5, 1.0, 'Out of sample test')\n\n\n\n\n\n\nmodel.score(X_test, Y_test)\n\n0.5249868646449161\n\n\n\nmodel.score?\n\n\nSignature: model.score(X, y, sample_weight=None)\nDocstring:\nReturn the coefficient of determination :math:`R^2` of the\nprediction.\nThe coefficient :math:`R^2` is defined as :math:`(1 - \\frac{u}{v})`,\nwhere :math:`u` is the residual sum of squares ``((y_true - y_pred)\n** 2).sum()`` and :math:`v` is the total sum of squares ``((y_true -\ny_true.mean()) ** 2).sum()``. The best possible score is 1.0 and it\ncan be negative (because the model can be arbitrarily worse). A\nconstant model that always predicts the expected value of `y`,\ndisregarding the input features, would get a :math:`R^2` score of\n0.0.\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Test samples. For some estimators this may be a precomputed\n    kernel matrix or a list of generic objects instead with shape\n    ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n    is the number of samples used in the fitting for the estimator.\ny : array-like of shape (n_samples,) or (n_samples, n_outputs)\n    True values for `X`.\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\nReturns\n-------\nscore : float\n    :math:`R^2` of ``self.predict(X)`` wrt. `y`.\nNotes\n-----\nThe :math:`R^2` score used when calling ``score`` on a regressor uses\n``multioutput='uniform_average'`` from version 0.23 to keep consistent\nwith default value of :func:`~sklearn.metrics.r2_score`.\nThis influences the ``score`` method of all the multioutput\nregressors (except for\n:class:`~sklearn.multioutput.MultiOutputRegressor`).\nFile:      ~/.local/opt/miniconda/lib/python3.8/site-packages/sklearn/base.py\nType:      method\n\n\n\n\nShould we adjust the size of the test set? What would be the problem?\n\nfor values in [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]:\n\n    X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=values)\n    \n    model = LinearRegression()\n    model.fit(X_train, Y_train)\n    \n    score = model.score(X_test, Y_test)\n    \n    print(f\"Test Set {values:.2f}% | Score: {score:.3f}\")\n\nTest Set 0.05% | Score: 0.427\nTest Set 0.10% | Score: 0.537\nTest Set 0.20% | Score: 0.558\nTest Set 0.30% | Score: 0.451\nTest Set 0.40% | Score: 0.513\nTest Set 0.50% | Score: 0.475\nTest Set 0.60% | Score: 0.488\nTest Set 0.70% | Score: 0.439\n\n\nThere is a tradeoff between: - a big test set: score more accurate, but the fitting is less accurate (more bias) - a small test set: score more volatile, but the fitting is more accurate (more variance)\nImplement \\(k\\)-fold model with \\(k=3\\).\n\nscores = []\n\nfrom sklearn.model_selection import KFold\nkf = KFold(n_splits=3)\n\nfor train_index, test_index in kf.split(X):\n    X_train, X_test = X[train_index], X[test_index]\n    Y_train, Y_test = Y[train_index], Y[test_index]\n    print(X_train.shape)\n\n    \n    model = LinearRegression()\n    model.fit(X_train, Y_train)\n    \n    score = model.score(X_test, Y_test)\n    \n    scores.append(score)\n    \n   ## train a model in X_train, y_train\n   ## test it on X_test, y_test\n\n(294, 10)\n(295, 10)\n(295, 10)\n\n\n\nscores\n\n[0.4693057771290108, 0.48724993937707484, 0.5095525852352711]\n\n\n\nsum(scores)/3\n\n0.4887027672471189\n\n\nBonus: use statsmodels (or linearmodels) to estimate the same linear model on the full sample. Is it always a superior method?\n\ndata['feature_names']\n\n['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']\n\n\n\nimport pandas\ndf = pandas.DataFrame(X, columns=data['feature_names'])\ndf['target'] = data['target']\n\n\ndf\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nbp\ns1\ns2\ns3\ns4\ns5\ns6\ntarget\n\n\n\n\n0\n0.038076\n0.050680\n0.061696\n0.021872\n-0.044223\n-0.034821\n-0.043401\n-0.002592\n0.019908\n-0.017646\n151.0\n\n\n1\n-0.001882\n-0.044642\n-0.051474\n-0.026328\n-0.008449\n-0.019163\n0.074412\n-0.039493\n-0.068330\n-0.092204\n75.0\n\n\n2\n0.085299\n0.050680\n0.044451\n-0.005671\n-0.045599\n-0.034194\n-0.032356\n-0.002592\n0.002864\n-0.025930\n141.0\n\n\n3\n-0.089063\n-0.044642\n-0.011595\n-0.036656\n0.012191\n0.024991\n-0.036038\n0.034309\n0.022692\n-0.009362\n206.0\n\n\n4\n0.005383\n-0.044642\n-0.036385\n0.021872\n0.003935\n0.015596\n0.008142\n-0.002592\n-0.031991\n-0.046641\n135.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n437\n0.041708\n0.050680\n0.019662\n0.059744\n-0.005697\n-0.002566\n-0.028674\n-0.002592\n0.031193\n0.007207\n178.0\n\n\n438\n-0.005515\n0.050680\n-0.015906\n-0.067642\n0.049341\n0.079165\n-0.028674\n0.034309\n-0.018118\n0.044485\n104.0\n\n\n439\n0.041708\n0.050680\n-0.015906\n0.017282\n-0.037344\n-0.013840\n-0.024993\n-0.011080\n-0.046879\n0.015491\n132.0\n\n\n440\n-0.045472\n-0.044642\n0.039062\n0.001215\n0.016318\n0.015283\n-0.028674\n0.026560\n0.044528\n-0.025930\n220.0\n\n\n441\n-0.045472\n-0.044642\n-0.073030\n-0.081414\n0.083740\n0.027809\n0.173816\n-0.039493\n-0.004220\n0.003064\n57.0\n\n\n\n\n442 rows × 11 columns\n\n\n\n\nfrom statsmodels.formula import api\n\n\nols_model = api.ols('target ~ age + sex + bmi + bp + s1 + s2 + s3 + s4 + s5 + s6', df)\n\n\nresult = ols_model.fit()\n\n\nresult.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\ntarget\nR-squared:\n0.518\n\n\nModel:\nOLS\nAdj. R-squared:\n0.507\n\n\nMethod:\nLeast Squares\nF-statistic:\n46.27\n\n\nDate:\nWed, 16 Mar 2022\nProb (F-statistic):\n3.83e-62\n\n\nTime:\n11:59:29\nLog-Likelihood:\n-2386.0\n\n\nNo. Observations:\n442\nAIC:\n4794.\n\n\nDf Residuals:\n431\nBIC:\n4839.\n\n\nDf Model:\n10\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n152.1335\n2.576\n59.061\n0.000\n147.071\n157.196\n\n\nage\n-10.0122\n59.749\n-0.168\n0.867\n-127.448\n107.424\n\n\nsex\n-239.8191\n61.222\n-3.917\n0.000\n-360.151\n-119.488\n\n\nbmi\n519.8398\n66.534\n7.813\n0.000\n389.069\n650.610\n\n\nbp\n324.3904\n65.422\n4.958\n0.000\n195.805\n452.976\n\n\ns1\n-792.1842\n416.684\n-1.901\n0.058\n-1611.169\n26.801\n\n\ns2\n476.7458\n339.035\n1.406\n0.160\n-189.621\n1143.113\n\n\ns3\n101.0446\n212.533\n0.475\n0.635\n-316.685\n518.774\n\n\ns4\n177.0642\n161.476\n1.097\n0.273\n-140.313\n494.442\n\n\ns5\n751.2793\n171.902\n4.370\n0.000\n413.409\n1089.150\n\n\ns6\n67.6254\n65.984\n1.025\n0.306\n-62.065\n197.316\n\n\n\n\n\n\nOmnibus:\n1.506\nDurbin-Watson:\n2.029\n\n\nProb(Omnibus):\n0.471\nJarque-Bera (JB):\n1.404\n\n\nSkew:\n0.017\nProb(JB):\n0.496\n\n\nKurtosis:\n2.726\nCond. No.\n227.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "session_7/machine_learning_regressions_elements.html#sparse-regressions-on-the-boston-house-price-dataset",
    "href": "session_7/machine_learning_regressions_elements.html#sparse-regressions-on-the-boston-house-price-dataset",
    "title": "Intro to sklearn",
    "section": "Sparse regressions on the Boston House Price Dataset",
    "text": "Sparse regressions on the Boston House Price Dataset\nImport the Boston House Price Dataset from sklearn. Describe it. Compute correlations.\nSplit the dataset into a training set (70%) and a test set (30%).\nTrain a lasso model to predict house prices. Compute the score on the test set.\nTrain a ridge model to predict house prices. Which one is better?\n(bonus) Use statsmodels to build a model predicting house prices. What is the problem?"
  },
  {
    "objectID": "session_7/machine_learning_regressions_elements.html#predicting-breast-cancer",
    "href": "session_7/machine_learning_regressions_elements.html#predicting-breast-cancer",
    "title": "Intro to sklearn",
    "section": "Predicting Breast Cancer",
    "text": "Predicting Breast Cancer\nSklearn includes the Winsconsin breast cancer database. It associates medical outcomes for tumor observation, with several characteristics. Can a machine learn how to predict whether a cancer is benign or malignant ?\nImport the Breast Cancer Dataset from sklearn. Describe it.\nProperly train a linear logistic regression to predict cancer morbidity. (bonus: use k-fold validation)\nTry with other classifiers. Which one is best?"
  },
  {
    "objectID": "session_7/recap.html#important-points",
    "href": "session_7/recap.html#important-points",
    "title": "Quick Recap",
    "section": "Important points",
    "text": "Important points"
  },
  {
    "objectID": "session_7/transcript.html#what-is-machine-learning-1",
    "href": "session_7/transcript.html#what-is-machine-learning-1",
    "title": "Introduction to Machine Learning",
    "section": "What is Machine learning?",
    "text": "What is Machine learning?\nDefinition Candidates:\nArthur Samuel: Field of study that gives computers the ability to learn without being explicitly programmed\nTom Mitchell: A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E."
  },
  {
    "objectID": "session_7/transcript.html#what-about-artificial-intelligence",
    "href": "session_7/transcript.html#what-about-artificial-intelligence",
    "title": "Introduction to Machine Learning",
    "section": "What about artificial intelligence ?",
    "text": "What about artificial intelligence ?\n\n\n\nAIs\n\nthink and learn\nmimmic human cognition"
  },
  {
    "objectID": "session_7/transcript.html#econometrics-vs-machine-learning",
    "href": "session_7/transcript.html#econometrics-vs-machine-learning",
    "title": "Introduction to Machine Learning",
    "section": "Econometrics vs Machine Learning",
    "text": "Econometrics vs Machine Learning\n\nEconometrics is essentially a subfield of machine learning with a different jargon and a focus on:\n\nstudying properties and validity of results\n\ndata is scarce\ninference\n\nsingling out effects of specific explanatory variables\nestablishing causality\n\nMachine learning:\n\nstructure data\nmake predictions (interpolate data)"
  },
  {
    "objectID": "session_7/transcript.html#data-types",
    "href": "session_7/transcript.html#data-types",
    "title": "Introduction to Machine Learning",
    "section": "Data types",
    "text": "Data types\n\nstructured:\n\ntabular\n\nlong\nwide\n\n\nunstructured:\n\nfiles\nnetworks\ntext, mails\nimages, sound"
  },
  {
    "objectID": "session_7/transcript.html#tabular-data",
    "href": "session_7/transcript.html#tabular-data",
    "title": "Introduction to Machine Learning",
    "section": "Tabular Data",
    "text": "Tabular Data\n\n\n\ntabular data"
  },
  {
    "objectID": "session_7/transcript.html#networks",
    "href": "session_7/transcript.html#networks",
    "title": "Introduction to Machine Learning",
    "section": "Networks",
    "text": "Networks\n\nBanking networks\nProduction network"
  },
  {
    "objectID": "session_7/transcript.html#big-data-1",
    "href": "session_7/transcript.html#big-data-1",
    "title": "Introduction to Machine Learning",
    "section": "Big Data",
    "text": "Big Data\n\nBig data:\n\nwide data (K&gt;&gt;N)\nlong data (N&gt;&gt;K)\nheterogenous, unstructured data\n\nMight not even fit in memory\n\nout of core computations\nlearn from a subset of the data"
  },
  {
    "objectID": "session_7/transcript.html#big-subfields-of-machine-learning",
    "href": "session_7/transcript.html#big-subfields-of-machine-learning",
    "title": "Introduction to Machine Learning",
    "section": "Big Subfields of Machine Learning",
    "text": "Big Subfields of Machine Learning\n\n\n\nTraditional classification\n\nsupervised (labelled data)\n\nregression: predict quantity\nclassification: predict index (categorical variable)\n\nunsupervised (no labels)\n\ndimension reduction\nclustering\n\nsemi-supervised / self-supervised\nreinforcement learning\n\nBazillions of different algorithms: https://scikit-learn.org/stable/user_guide.html\n\n\n\n\n\nregression:\n\nPredict: \\(y = f(x; \\theta)\\)\n\n\n\n\n\nsupervised: regression\n\n\n\n\n\n\nAge\n\n\nActivity\n\n\nSalary\n\n\n\n\n23\n\n\nExplorer\n\n\n1200\n\n\n\n\n40\n\n\nMortician\n\n\n2000\n\n\n\n\n45\n\n\nMortician\n\n\n2500\n\n\n\n\n33\n\n\nMovie Star\n\n\n3000\n\n\n\n\n35\n\n\nExplorer\n\n\n???\n\n\n\n\n\n\n\nsupervised: classification\n\nOutput is discrete\nRegular trick: \\(\\sigma(f(x; \\theta))\\) where \\(\\sigma(x)=\\frac{1}{1-e^{-x}}\\)\n\n\n\n\n\nclassification\n\n\n\n\n\n\nAge\n\n\nSalary\n\n\nActivity\n\n\n\n\n23\n\n\n1200\n\n\nExplorer\n\n\n\n\n40\n\n\n2000\n\n\nMortician\n\n\n\n\n45\n\n\n2500\n\n\nMortician\n\n\n\n\n33\n\n\n3000\n\n\nMovie Star\n\n\n\n\n35\n\n\n3000\n\n\n???\n\n\n\n\n\n\nunsupervised\n\norganize data without labels\n\ndimension reduction: describe data with less parameters\nclustering: sort data into “similar groups” (exemple)\n\n\n\n\n\n\nAge\n\n\nSalary\n\n\nActivity\n\n\n\n\n23\n\n\n1200\n\n\nExplorer\n\n\n\n\n40\n\n\n2000\n\n\nMortician\n\n\n\n\n45\n\n\n2500\n\n\nMortician\n\n\n\n\n33\n\n\n3000\n\n\nMovie Star\n\n\n\n\n35\n\n\n3000\n\n\nExplorer\n\n\n\n\n\n\nunsupervised: clustering\n\n\n\nkmeansclustering\n\n\n\n\nunsupervised: clustering\nWomen buying dresses during the year:"
  },
  {
    "objectID": "session_7/transcript.html#difference-with-traditional-regression",
    "href": "session_7/transcript.html#difference-with-traditional-regression",
    "title": "Introduction to Machine Learning",
    "section": "Difference with traditional regression",
    "text": "Difference with traditional regression\n\\[\\underbrace{y}_{\\text{explained variable}} = a \\underbrace{x}_{\\text{explanatory variable}} + b\\]"
  },
  {
    "objectID": "session_7/transcript.html#difference-with-traditional-regression-1",
    "href": "session_7/transcript.html#difference-with-traditional-regression-1",
    "title": "Introduction to Machine Learning",
    "section": "Difference with traditional regression",
    "text": "Difference with traditional regression\n\\[\\underbrace{y}_{\\text{labels}} = a \\underbrace{x}_{\\text{features}} + b\\]\n. . .\n\n\n\n\n\n\n\nEconometrics\nMachine learning\n\n\n\n\nRegressand / independent variable / explanatory variable\nFeatures\n\n\nRegressor / dependent variable / explained variable\nLabels\n\n\nRegression\nModel Training"
  },
  {
    "objectID": "session_7/transcript.html#difference-with-traditional-regression-2",
    "href": "session_7/transcript.html#difference-with-traditional-regression-2",
    "title": "Introduction to Machine Learning",
    "section": "Difference with traditional regression",
    "text": "Difference with traditional regression\n\nBig data requires other means to process the data:\n\ndata is long: so many observations \\(x\\) doesn’t fit in the memory\n\nneed to use incremental training method to use only a subsample at a time\n\ndata is wide: so many features, the model is crudely overspecified\n\nneed to build dimension reduction into the objective\n\ndata is nonlinear:\n\nuse nonlinear model (and nonlinear training)\n\ndata is not a simple vector…\n\nsame as nonlinear"
  },
  {
    "objectID": "session_7/transcript.html#long-data",
    "href": "session_7/transcript.html#long-data",
    "title": "Introduction to Machine Learning",
    "section": "Long data",
    "text": "Long data"
  },
  {
    "objectID": "session_7/transcript.html#long-data-1",
    "href": "session_7/transcript.html#long-data-1",
    "title": "Introduction to Machine Learning",
    "section": "Long data",
    "text": "Long data\nLong data is characterized by a high number of observations."
  },
  {
    "objectID": "session_7/transcript.html#long-data-2",
    "href": "session_7/transcript.html#long-data-2",
    "title": "Introduction to Machine Learning",
    "section": "Long data",
    "text": "Long data\n\n\n\nTraditional regression:\n\nfull sample \\(X,Y=(x_i,y_i)_{i=1:N}\\)\nOLS: \\(\\min_{a,b} \\sum_{i=1}^N (a x_i + b - y_i)^2\\)\nclosed-form solution: \\(a = X^{\\prime}X Y\\) and \\(b= ...\\)\nhard to compute if \\(X\\) is very big\n\n\n\n\n\nIncremental learning:\n\ngiven initial \\(a_n\\), \\(b_n\\)\npick \\(N\\) random observations (the batch)\n\nregress them to get new estimate \\(a\\), \\(b\\)\nthis minimizes the square of errors\n\nupdate with learning rate \\(\\beta\\):\n\n\\(a_{n+1} \\leftarrow a_n (1-\\beta_n) + \\beta_n a\\)\n\\(b_{n+1} \\leftarrow b_n (1-\\beta_n) + \\beta_n b\\)\n\nprocess is not biased (that is \\(a\\) converges to the true value) as long as one decreases \\(\\beta\\) sufficiently fast over time (ex: \\(\\beta_n=\\frac{1}{n}\\))"
  },
  {
    "objectID": "session_7/transcript.html#formalisation-a-typical-machine-learning-task",
    "href": "session_7/transcript.html#formalisation-a-typical-machine-learning-task",
    "title": "Introduction to Machine Learning",
    "section": "Formalisation: a typical machine learning task",
    "text": "Formalisation: a typical machine learning task\n\nvector of unknowns: \\(\\theta=(a,b)\\)\ndataset \\(X,Y=(x_i,y_i)_{i=1:N}\\)\nfor a random draw \\(\\omega = (a_{\\sigma(i)}, b_{\\sigma(i)})_{i=[1,N]} \\subset (X,Y)\\)\n\n\\(\\omega\\) is just a random batch of size \\(N\\)\n\ndefine the empirical risk (or empirical cost) \\[\\xi(\\theta, \\omega) = \\sum_{(x,y) \\in \\omega} (y - (a x + b))^2\\]\nwe want to minimize theoretical risk: \\[\\Xi(\\theta) = \\mathbb{E} \\left[ \\xi(\\theta, \\omega)\\right]\\]"
  },
  {
    "objectID": "session_7/transcript.html#training-gradient-descent",
    "href": "session_7/transcript.html#training-gradient-descent",
    "title": "Introduction to Machine Learning",
    "section": "Training: Gradient Descent",
    "text": "Training: Gradient Descent\n\n\n\nHow do we minimize a function \\(f(a,b)\\)?\nGradient descent:\n\n\\(a_k, b_k\\) given\ncompute the gradient (slope) \\(\\nabla_{a,b} f = \\begin{bmatrix} \\frac{\\partial f}{\\partial a} \\\\\\\\ \\frac{\\partial f}{\\partial b}\\end{bmatrix}\\)\nfollow the steepest slope: (Newton Algorithm)\n\n\\[ \\begin{bmatrix} a_{k+1} \\\\\\\\ b_{k+1} \\end{bmatrix} \\leftarrow  \\begin{bmatrix} a_k \\\\\\\\ b_k \\end{bmatrix} - \\nabla_{a,b} f\\]\n\nbut not too fast: use learning rate \\(\\lambda\\): \\[ \\begin{bmatrix} a_{k+1} \\\\\\\\ b_{k+1} \\end{bmatrix} \\leftarrow  (1-\\lambda) \\begin{bmatrix} a_k \\\\\\\\ b_k \\end{bmatrix} + \\lambda (- \\nabla_{a,b} f )\\]"
  },
  {
    "objectID": "session_7/transcript.html#not-everything-goes-wrong-all-the-time",
    "href": "session_7/transcript.html#not-everything-goes-wrong-all-the-time",
    "title": "Introduction to Machine Learning",
    "section": "Not everything goes wrong all the time",
    "text": "Not everything goes wrong all the time\n \n\nIn practice, choosing the right learning rate \\(\\lambda\\) is crucial\n\\(\\lambda\\) is a metaparameter of the model training."
  },
  {
    "objectID": "session_7/transcript.html#wide-data",
    "href": "session_7/transcript.html#wide-data",
    "title": "Introduction to Machine Learning",
    "section": "Wide data",
    "text": "Wide data\n\nWide Data is characterized by a high number of features compared to the number of observations.\n\n. . .\nProblem: - with many independent variables \\(x_1, ... x_K\\), \\(K&gt;&gt;N\\) and one dependent variable \\(y\\) the regression \\[y = a_1 x_1 + a_2 x_2 + \\cdots + a_N x_N + b\\] is grossly overidentified."
  },
  {
    "objectID": "session_7/transcript.html#wide-data-regression",
    "href": "session_7/transcript.html#wide-data-regression",
    "title": "Introduction to Machine Learning",
    "section": "Wide data regression",
    "text": "Wide data regression\n\nMain Idea: penalize non-zero coefficients to encourage scarcity\n\nRidge: \\[\\Xi(a,b) = \\min_{a,b} \\sum_{i=1}^N ( \\sum_j a_j x_j + b - y_i)^2 + \\mu \\sum_i |a_i|^2\\]\n\nshrinks parameters towards zero\nclosed form\n\nLasso: \\[\\Xi(a,b) = \\min_{a,b} \\sum_{i=1}^N (\\sum_j a_j x_j + b - y_i)^2 + \\mu \\sum_i |a_i|\\]\n\neliminates zero coefficients\n\nElastic: Ridge + Lasso\n\nRemarks:\n\n\\(\\mu\\) is called a regularization term.\nit is a hyperparameter\n\\(\\mu \\uparrow\\), bias increases, variance decreases"
  },
  {
    "objectID": "session_7/transcript.html#training",
    "href": "session_7/transcript.html#training",
    "title": "Introduction to Machine Learning",
    "section": "Training",
    "text": "Training\nTo perform Lasso and ridge regression:\n\nAI approach:\n\nminimize objective \\(\\Xi(a,b)\\) directly.\napproach is known as (stochastic) Gradient Descent\n\nUse special algorithms"
  },
  {
    "objectID": "session_7/transcript.html#example-imf-challenge",
    "href": "session_7/transcript.html#example-imf-challenge",
    "title": "Introduction to Machine Learning",
    "section": "Example: IMF challenge",
    "text": "Example: IMF challenge\n\nAn internal IMF challenge to predict crises in countries\nLots of different approaches\nLots of data:\n\nwhich one is relevant\nmachine must select relevant informations\n\nExample: Lasso Regressions and Forecasting Models in Applied Stress Testing by Jorge A. Chan-Lau\n\nin a given developing country\ntries to predict probability of default in various sectors"
  },
  {
    "objectID": "session_7/transcript.html#nonlinear-regression-1",
    "href": "session_7/transcript.html#nonlinear-regression-1",
    "title": "Introduction to Machine Learning",
    "section": "Nonlinear Regression",
    "text": "Nonlinear Regression\n\nSo far, we have assumed,\n\n\\(y_i = a + b x_i\\)\n\\(y_i = a + b x_i + μ_1 (a^2 + b^2) + μ_2 (|a| + |b|)\\)\ndefined \\(\\Xi(a,b)\\) and tried to minimize it\n\nSame approach works for fully nonlinear models\n\n\\(y_i = a x_i + a^2 x_i^2 + c\\)\n\\(y_i = \\varphi(x; \\theta)\\) ()\n\nSpecial case: neural network:\n\nprimer tensor playground"
  },
  {
    "objectID": "session_7/transcript.html#how-to-evaluate-the-machine-learning",
    "href": "session_7/transcript.html#how-to-evaluate-the-machine-learning",
    "title": "Introduction to Machine Learning",
    "section": "how to evaluate the machine learning",
    "text": "how to evaluate the machine learning\nIn machine learning we can’t perform statistical inference easily. How do we assess the validity of a model?\n\nBasic idea (independent of how complex the algorithm is)\n\nseparate data in\n\ntraining set (in-sample)\ntest set (out of sample)\n\ntrain using only the training set\nevaluate performance on the test set\n\nPerformance can be:\n\nfitness, number of classification errors (false positive, false negative)"
  },
  {
    "objectID": "session_7/transcript.html#how-to-evaluate-the-machine-learning-1",
    "href": "session_7/transcript.html#how-to-evaluate-the-machine-learning-1",
    "title": "Introduction to Machine Learning",
    "section": "how to evaluate the machine learning",
    "text": "how to evaluate the machine learning\nIn case the training method depends itself on many parameters (the hyperparameters) we make three samples instead:\n\ntraining set (in-sample)\nvalidation set (to update hyperparameters)\ntest set (out of sample)\n\nGolden Rule: the test set should not be used to estimate the model, and should not affect the choice any training parameter (hyperparameter)."
  },
  {
    "objectID": "session_7/transcript.html#section",
    "href": "session_7/transcript.html#section",
    "title": "Introduction to Machine Learning",
    "section": "",
    "text": "Traintest\n\n\n. . .\nThe test set reveals that orange model is overfitting."
  },
  {
    "objectID": "session_7/transcript.html#how-to-choose-the-validation-set",
    "href": "session_7/transcript.html#how-to-choose-the-validation-set",
    "title": "Introduction to Machine Learning",
    "section": "How to choose the validation set?",
    "text": "How to choose the validation set?\n\nHoldout validation approach:\n\nkeeps x% of the data for the training, (100-x)% for the test\n\nHow to choose the sizes of the subsets?\n\nsmall dataset: 90-10\nbig data set: 70-30 (we can afford to waste more training data for the test)\n\n\n. . .\n\nProblem:\n\nare we sure the validation size is correct? Are the results determined by an (un-) lucky draw?\na problem for smaller datasets"
  },
  {
    "objectID": "session_7/transcript.html#how-to-choose-the-validation-set-1",
    "href": "session_7/transcript.html#how-to-choose-the-validation-set-1",
    "title": "Introduction to Machine Learning",
    "section": "How to choose the validation set?",
    "text": "How to choose the validation set?\nA more robust solution: \\(k\\)-fold validation\n\n\n\nsplit dataset randomly in \\(K\\) subsets of equal size \\(S_1, ... S_K\\)\nuse subset \\(S_i\\) as test set, the rest as training set, compute the score\ncompare the scores obtained for all \\(i\\in[1,K]\\)\n\nthey should be similar (compute standard deviation)\n\naverage them"
  },
  {
    "objectID": "session_7/transcript.html#wait",
    "href": "session_7/transcript.html#wait",
    "title": "Introduction to Machine Learning",
    "section": "Wait",
    "text": "Wait\n\nAnother library to do regression ?\nstatsmodels:\n\nexplanatory analysis\nstatistical tests\nformula interface for many estimation algorithms\n\nstateless approach (model.fit() returns another object)\n\n\nlinearmodels\n\nextends statsmodels (very similar interface)\n\n(panel models, IV, systems…)\n\n\nsklearn:\n\nprediction\nfaster for big datasets\ncommon interface for several machine learning tasks\n\nstateful approach (model is modified by .fit operation)\n\ndefacto standard for machine learning"
  },
  {
    "objectID": "session_7/transcript.html#in-practice",
    "href": "session_7/transcript.html#in-practice",
    "title": "Introduction to Machine Learning",
    "section": "In practice",
    "text": "In practice\n\n\nBasic sklearn workflow:\n\n\nimport data\n\nfeatures: a matrix X (2d numpy array)\nlabels: a vector y (1d numpy array)\n\nsplit the data, between training and test datasets\n\nsplit needs to be random to avoid any bias\n\nnormalize the data\n\nmost ML algorithm are sensitive to scale\n\ncreate a model (independent from data)\ntrain the model on training dataset\nevaluate accuracy on test dataset (here \\(R^2\\))\nuse the model to make predictions\n\n\nThe workflow is always the same, no matter what the model is\n\ntry sklearn.linear_model.Lasso instead of LinearRegression\n\n\nfrom sklearn.datasets import load_diabetes\ndataset = load_diabetes()\nX = dataset['data']\ny = dataset['target']\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.1)\n\n#Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\nmodel.score(X_test, y_test)\nmodel.predict(X_new)"
  },
  {
    "objectID": "session_7/transcript.html#k-fold-validation-with-sklearn",
    "href": "session_7/transcript.html#k-fold-validation-with-sklearn",
    "title": "Introduction to Machine Learning",
    "section": "\\(k\\)-fold validation with sklearn",
    "text": "\\(k\\)-fold validation with sklearn\nfrom sklearn.model_selection import KFold\nkf = KFold(n_splits=10)\n\nfor train_index, test_index in kf.split(X):\n   X_train, X_test = X[train_index], X[test_index]\n   y_train, y_test = y[train_index], y[test_index]\n\n   ## train a model in X_train, y_train\n   ## test it on X_test, y_test"
  },
  {
    "objectID": "session_8/Classification_Correction.html",
    "href": "session_8/Classification_Correction.html",
    "title": "Classification and clustering",
    "section": "",
    "text": "The two csv files (origin: kaggle) contain the training set (resp the validation set) about the clients from a “global finance company”.\nYour goal is to use all available information to build a model to accurately predict the probability of default which is coded up as a qualitative variable with three values.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn import metrics\nfrom sklearn .metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder as le                    # label encoder\nfrom sklearn.model_selection import train_test_split                    # train & test split \nfrom sklearn.neighbors import KNeighborsClassifier as knnClassifier     # knn\nfrom sklearn.linear_model import LogisticRegression as lgrClassifier    # logistic regression\n\nImport training set and validation sets\n\ndata=pd.read_csv(\"train.csv\")\ntest=pd.read_csv(\"test.csv\")\n\nDescribe the dataset. How is the credit category encoded?\n\ndata.columns\n\nIndex(['ID', 'Customer_ID', 'Month', 'Name', 'Age', 'SSN', 'Occupation',\n       'Annual_Income', 'Monthly_Inhand_Salary', 'Num_Bank_Accounts',\n       'Num_Credit_Card', 'Interest_Rate', 'Num_of_Loan', 'Type_of_Loan',\n       'Delay_from_due_date', 'Num_of_Delayed_Payment', 'Changed_Credit_Limit',\n       'Num_Credit_Inquiries', 'Credit_Mix', 'Outstanding_Debt',\n       'Credit_Utilization_Ratio', 'Credit_History_Age',\n       'Payment_of_Min_Amount', 'Total_EMI_per_month',\n       'Amount_invested_monthly', 'Payment_Behaviour', 'Monthly_Balance',\n       'Credit_Score'],\n      dtype='object')\n\n\n\ndata.Payment_Behaviour.unique()\n\narray(['High_spent_Small_value_payments',\n       'Low_spent_Large_value_payments',\n       'Low_spent_Medium_value_payments',\n       'Low_spent_Small_value_payments',\n       'High_spent_Medium_value_payments',\n       'High_spent_Large_value_payments'], dtype=object)\n\n\nLet’s re rencode the string variables\n\ndata['Occupation'] = le().fit_transform(data['Occupation'])\ndata['Type_of_Loan'] = le().fit_transform(data['Type_of_Loan'])\ndata['Credit_Mix'] = le().fit_transform(data['Credit_Mix'])\ndata['Payment_of_Min_Amount'] = le().fit_transform(data['Payment_of_Min_Amount'])\ndata['Payment_Behaviour'] = le().fit_transform(data['Payment_Behaviour'])\ndata['Credit_Score'] = le().fit_transform(data['Credit_Score'])\n\n\nCredit_Score_le = le()\n\n\nplt.figure(figsize = (14,10))\nsns.heatmap(data.corr() , annot = True , cmap = \"YlGnBu\")\n\n/tmp/ipykernel_221/4218269893.py:2: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n  sns.heatmap(data.corr() , annot = True , cmap = \"YlGnBu\")\n\n\n&lt;AxesSubplot: &gt;\n\n\n\n\n\n\ndata\n\n\n\n\n\n\n\n\nID\nCustomer_ID\nMonth\nName\nAge\nSSN\nOccupation\nAnnual_Income\nMonthly_Inhand_Salary\nNum_Bank_Accounts\n...\nCredit_Mix\nOutstanding_Debt\nCredit_Utilization_Ratio\nCredit_History_Age\nPayment_of_Min_Amount\nTotal_EMI_per_month\nAmount_invested_monthly\nPayment_Behaviour\nMonthly_Balance\nCredit_Score\n\n\n\n\n0\n5634\n3392\n1\nAaron Maashoh\n23.0\n821000265.0\n12\n19114.12\n1824.843333\n3.0\n...\n1\n809.98\n26.822620\n265.0\n1\n49.574949\n21.465380\n2\n312.494089\n0\n\n\n1\n5635\n3392\n2\nAaron Maashoh\n23.0\n821000265.0\n12\n19114.12\n1824.843333\n3.0\n...\n1\n809.98\n31.944960\n266.0\n1\n49.574949\n21.465380\n3\n284.629162\n0\n\n\n2\n5636\n3392\n3\nAaron Maashoh\n23.0\n821000265.0\n12\n19114.12\n1824.843333\n3.0\n...\n1\n809.98\n28.609352\n267.0\n1\n49.574949\n21.465380\n4\n331.209863\n0\n\n\n3\n5637\n3392\n4\nAaron Maashoh\n23.0\n821000265.0\n12\n19114.12\n1824.843333\n3.0\n...\n1\n809.98\n31.377862\n268.0\n1\n49.574949\n21.465380\n5\n223.451310\n0\n\n\n4\n5638\n3392\n5\nAaron Maashoh\n23.0\n821000265.0\n12\n19114.12\n1824.843333\n3.0\n...\n1\n809.98\n24.797347\n269.0\n1\n49.574949\n21.465380\n1\n341.489231\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n99995\n155625\n37932\n4\nNicks\n25.0\n78735990.0\n9\n39628.99\n3359.415833\n4.0\n...\n1\n502.38\n34.663572\n378.0\n1\n35.104023\n24.028477\n0\n479.866228\n1\n\n\n99996\n155626\n37932\n5\nNicks\n25.0\n78735990.0\n9\n39628.99\n3359.415833\n4.0\n...\n1\n502.38\n40.565631\n379.0\n1\n35.104023\n24.028477\n1\n496.651610\n1\n\n\n99997\n155627\n37932\n6\nNicks\n25.0\n78735990.0\n9\n39628.99\n3359.415833\n4.0\n...\n1\n502.38\n41.255522\n380.0\n1\n35.104023\n24.028477\n0\n516.809083\n1\n\n\n99998\n155628\n37932\n7\nNicks\n25.0\n78735990.0\n9\n39628.99\n3359.415833\n4.0\n...\n1\n502.38\n33.638208\n381.0\n1\n35.104023\n24.028477\n3\n319.164979\n2\n\n\n99999\n155629\n37932\n8\nNicks\n25.0\n78735990.0\n9\n39628.99\n3359.415833\n4.0\n...\n1\n502.38\n34.192463\n382.0\n1\n35.104023\n24.028477\n1\n393.673696\n1\n\n\n\n\n100000 rows × 28 columns\n\n\n\n\ndataset_model = data[['Credit_Score','Changed_Credit_Limit', 'Payment_of_Min_Amount', 'Credit_Mix', 'Delay_from_due_date', 'Annual_Income', 'Monthly_Inhand_Salary', 'Age', 'Monthly_Balance', 'Num_of_Delayed_Payment', 'Outstanding_Debt', 'Payment_Behaviour', 'Credit_History_Age', 'Num_Bank_Accounts', 'Credit_Utilization_Ratio']]\n\nMake several plots about the dataset (histograms, correlation plots, …)_\n\nx = dataset_model.drop(['Credit_Score'] , axis = 1).values\ny = dataset_model['Credit_Score' ].values\n\nSplit the train dataset into a df_train and a df_test dataset.\n\nx_train , x_test , y_train , y_test = train_test_split(x,y , test_size= 0.25 , random_state=42)\n[x_train.shape, y_train.shape]\n\n[(75000, 14), (75000,)]\n\n\nImplement a logistic regression.\n\nlr = lgrClassifier(C = 100)\n\n\nlr.fit(x_train , y_train)\n\n/opt/conda/envs/escpython/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\nLogisticRegression(C=100)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(C=100)\n\n\n\nlr_score_test=lr.score(x_test , y_test)\nlr_score_test\n\n0.54208\n\n\nCompute the confusion matrix using the test set. Comment\n\nactual = y_test\npredicted = lr.predict(x_test)\nconfusion_matrix = metrics.confusion_matrix(actual, predicted)\ncm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = ['Poor', 'Standard','Good'])\n\n\nfig, ax = plt.subplots(figsize=(10,10))\nax.grid(False)\ncm_display.plot(ax=ax)\n\n&lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f32e3692a10&gt;\n\n\n\n\n\nPerform the same analysis with other classification methods and compare their performance using the test set.\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\nx_train,x_test,y_train,y_test = train_test_split(x,y, test_size=0.3,random_state = 1234)\n\n\nknn = KNeighborsClassifier(n_neighbors=7)\n\n\nknn.fit(x_train , y_train)\n\nKNeighborsClassifier(n_neighbors=7)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsClassifierKNeighborsClassifier(n_neighbors=7)\n\n\n\nknn_score_test=knn.score(x_test , y_test)\nknn_score_test\n\n0.7286666666666667\n\n\n\nactual = y_test\npredicted = knn.predict(x_test)\nconfusion_matrix = metrics.confusion_matrix(actual, predicted)\ncm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = ['Poor', 'Standard','Good'])\n\n\nfig, ax = plt.subplots(figsize=(10,10))\nax.grid(False)\ncm_display.plot(ax=ax)\n\n&lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f7ddeb3eb30&gt;\n\n\n\n\n\nWhich one would you choose? Test its performance on the validation set"
  },
  {
    "objectID": "session_8/Classification_Correction.html#predicting-the-credit-score",
    "href": "session_8/Classification_Correction.html#predicting-the-credit-score",
    "title": "Classification and clustering",
    "section": "",
    "text": "The two csv files (origin: kaggle) contain the training set (resp the validation set) about the clients from a “global finance company”.\nYour goal is to use all available information to build a model to accurately predict the probability of default which is coded up as a qualitative variable with three values.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn import metrics\nfrom sklearn .metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder as le                    # label encoder\nfrom sklearn.model_selection import train_test_split                    # train & test split \nfrom sklearn.neighbors import KNeighborsClassifier as knnClassifier     # knn\nfrom sklearn.linear_model import LogisticRegression as lgrClassifier    # logistic regression\n\nImport training set and validation sets\n\ndata=pd.read_csv(\"train.csv\")\ntest=pd.read_csv(\"test.csv\")\n\nDescribe the dataset. How is the credit category encoded?\n\ndata.columns\n\nIndex(['ID', 'Customer_ID', 'Month', 'Name', 'Age', 'SSN', 'Occupation',\n       'Annual_Income', 'Monthly_Inhand_Salary', 'Num_Bank_Accounts',\n       'Num_Credit_Card', 'Interest_Rate', 'Num_of_Loan', 'Type_of_Loan',\n       'Delay_from_due_date', 'Num_of_Delayed_Payment', 'Changed_Credit_Limit',\n       'Num_Credit_Inquiries', 'Credit_Mix', 'Outstanding_Debt',\n       'Credit_Utilization_Ratio', 'Credit_History_Age',\n       'Payment_of_Min_Amount', 'Total_EMI_per_month',\n       'Amount_invested_monthly', 'Payment_Behaviour', 'Monthly_Balance',\n       'Credit_Score'],\n      dtype='object')\n\n\n\ndata.Payment_Behaviour.unique()\n\narray(['High_spent_Small_value_payments',\n       'Low_spent_Large_value_payments',\n       'Low_spent_Medium_value_payments',\n       'Low_spent_Small_value_payments',\n       'High_spent_Medium_value_payments',\n       'High_spent_Large_value_payments'], dtype=object)\n\n\nLet’s re rencode the string variables\n\ndata['Occupation'] = le().fit_transform(data['Occupation'])\ndata['Type_of_Loan'] = le().fit_transform(data['Type_of_Loan'])\ndata['Credit_Mix'] = le().fit_transform(data['Credit_Mix'])\ndata['Payment_of_Min_Amount'] = le().fit_transform(data['Payment_of_Min_Amount'])\ndata['Payment_Behaviour'] = le().fit_transform(data['Payment_Behaviour'])\ndata['Credit_Score'] = le().fit_transform(data['Credit_Score'])\n\n\nCredit_Score_le = le()\n\n\nplt.figure(figsize = (14,10))\nsns.heatmap(data.corr() , annot = True , cmap = \"YlGnBu\")\n\n/tmp/ipykernel_221/4218269893.py:2: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n  sns.heatmap(data.corr() , annot = True , cmap = \"YlGnBu\")\n\n\n&lt;AxesSubplot: &gt;\n\n\n\n\n\n\ndata\n\n\n\n\n\n\n\n\nID\nCustomer_ID\nMonth\nName\nAge\nSSN\nOccupation\nAnnual_Income\nMonthly_Inhand_Salary\nNum_Bank_Accounts\n...\nCredit_Mix\nOutstanding_Debt\nCredit_Utilization_Ratio\nCredit_History_Age\nPayment_of_Min_Amount\nTotal_EMI_per_month\nAmount_invested_monthly\nPayment_Behaviour\nMonthly_Balance\nCredit_Score\n\n\n\n\n0\n5634\n3392\n1\nAaron Maashoh\n23.0\n821000265.0\n12\n19114.12\n1824.843333\n3.0\n...\n1\n809.98\n26.822620\n265.0\n1\n49.574949\n21.465380\n2\n312.494089\n0\n\n\n1\n5635\n3392\n2\nAaron Maashoh\n23.0\n821000265.0\n12\n19114.12\n1824.843333\n3.0\n...\n1\n809.98\n31.944960\n266.0\n1\n49.574949\n21.465380\n3\n284.629162\n0\n\n\n2\n5636\n3392\n3\nAaron Maashoh\n23.0\n821000265.0\n12\n19114.12\n1824.843333\n3.0\n...\n1\n809.98\n28.609352\n267.0\n1\n49.574949\n21.465380\n4\n331.209863\n0\n\n\n3\n5637\n3392\n4\nAaron Maashoh\n23.0\n821000265.0\n12\n19114.12\n1824.843333\n3.0\n...\n1\n809.98\n31.377862\n268.0\n1\n49.574949\n21.465380\n5\n223.451310\n0\n\n\n4\n5638\n3392\n5\nAaron Maashoh\n23.0\n821000265.0\n12\n19114.12\n1824.843333\n3.0\n...\n1\n809.98\n24.797347\n269.0\n1\n49.574949\n21.465380\n1\n341.489231\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n99995\n155625\n37932\n4\nNicks\n25.0\n78735990.0\n9\n39628.99\n3359.415833\n4.0\n...\n1\n502.38\n34.663572\n378.0\n1\n35.104023\n24.028477\n0\n479.866228\n1\n\n\n99996\n155626\n37932\n5\nNicks\n25.0\n78735990.0\n9\n39628.99\n3359.415833\n4.0\n...\n1\n502.38\n40.565631\n379.0\n1\n35.104023\n24.028477\n1\n496.651610\n1\n\n\n99997\n155627\n37932\n6\nNicks\n25.0\n78735990.0\n9\n39628.99\n3359.415833\n4.0\n...\n1\n502.38\n41.255522\n380.0\n1\n35.104023\n24.028477\n0\n516.809083\n1\n\n\n99998\n155628\n37932\n7\nNicks\n25.0\n78735990.0\n9\n39628.99\n3359.415833\n4.0\n...\n1\n502.38\n33.638208\n381.0\n1\n35.104023\n24.028477\n3\n319.164979\n2\n\n\n99999\n155629\n37932\n8\nNicks\n25.0\n78735990.0\n9\n39628.99\n3359.415833\n4.0\n...\n1\n502.38\n34.192463\n382.0\n1\n35.104023\n24.028477\n1\n393.673696\n1\n\n\n\n\n100000 rows × 28 columns\n\n\n\n\ndataset_model = data[['Credit_Score','Changed_Credit_Limit', 'Payment_of_Min_Amount', 'Credit_Mix', 'Delay_from_due_date', 'Annual_Income', 'Monthly_Inhand_Salary', 'Age', 'Monthly_Balance', 'Num_of_Delayed_Payment', 'Outstanding_Debt', 'Payment_Behaviour', 'Credit_History_Age', 'Num_Bank_Accounts', 'Credit_Utilization_Ratio']]\n\nMake several plots about the dataset (histograms, correlation plots, …)_\n\nx = dataset_model.drop(['Credit_Score'] , axis = 1).values\ny = dataset_model['Credit_Score' ].values\n\nSplit the train dataset into a df_train and a df_test dataset.\n\nx_train , x_test , y_train , y_test = train_test_split(x,y , test_size= 0.25 , random_state=42)\n[x_train.shape, y_train.shape]\n\n[(75000, 14), (75000,)]\n\n\nImplement a logistic regression.\n\nlr = lgrClassifier(C = 100)\n\n\nlr.fit(x_train , y_train)\n\n/opt/conda/envs/escpython/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\nLogisticRegression(C=100)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(C=100)\n\n\n\nlr_score_test=lr.score(x_test , y_test)\nlr_score_test\n\n0.54208\n\n\nCompute the confusion matrix using the test set. Comment\n\nactual = y_test\npredicted = lr.predict(x_test)\nconfusion_matrix = metrics.confusion_matrix(actual, predicted)\ncm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = ['Poor', 'Standard','Good'])\n\n\nfig, ax = plt.subplots(figsize=(10,10))\nax.grid(False)\ncm_display.plot(ax=ax)\n\n&lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f32e3692a10&gt;\n\n\n\n\n\nPerform the same analysis with other classification methods and compare their performance using the test set.\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\nx_train,x_test,y_train,y_test = train_test_split(x,y, test_size=0.3,random_state = 1234)\n\n\nknn = KNeighborsClassifier(n_neighbors=7)\n\n\nknn.fit(x_train , y_train)\n\nKNeighborsClassifier(n_neighbors=7)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsClassifierKNeighborsClassifier(n_neighbors=7)\n\n\n\nknn_score_test=knn.score(x_test , y_test)\nknn_score_test\n\n0.7286666666666667\n\n\n\nactual = y_test\npredicted = knn.predict(x_test)\nconfusion_matrix = metrics.confusion_matrix(actual, predicted)\ncm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = ['Poor', 'Standard','Good'])\n\n\nfig, ax = plt.subplots(figsize=(10,10))\nax.grid(False)\ncm_display.plot(ax=ax)\n\n&lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f7ddeb3eb30&gt;\n\n\n\n\n\nWhich one would you choose? Test its performance on the validation set"
  },
  {
    "objectID": "session_8/Classification_Correction.html#segmenting-the-bank-clients",
    "href": "session_8/Classification_Correction.html#segmenting-the-bank-clients",
    "title": "Classification and clustering",
    "section": "Segmenting the bank clients",
    "text": "Segmenting the bank clients\nWith the same database, without using the credit score, implement a k-means clustering algorithm.\nAre the clusters related to the credit score?"
  },
  {
    "objectID": "session_8/graphs/inference.html",
    "href": "session_8/graphs/inference.html",
    "title": "Data-Based Economics",
    "section": "",
    "text": "from matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n\ndef generate_dataset(μ1, μ2, α, β, σ, N=10):\n    xvec = np.random.uniform(μ1, μ2, N)\n    yvec = α + β*xvec + np.random.normal(size=N)*σ\n    return pd.DataFrame({'x': xvec, 'y': yvec})\n\n\ndf = generate_dataset(0.0, 1.0, 0.1, 0.8, 0.1)\n\n\nplt.plot(df['x'], df['y'], 'o')\nplt.grid()\n\n\n\n\n\ndef plot_distribution(α, β, σ, N=100000, μ1=0.0, μ2=1.0):\n    xvec = np.random.uniform(μ1, μ2, N)\n    yvec = α + β*xvec + np.random.normal(size=N)*σ\n    plt.plot(xvec, yvec, '.r', alpha=0.005)\n    plt.plot(xvec, α + β*xvec, color='black')\n\n# missing ridge line\n\n\nimport statsmodels\n\n\nμ1 = 0\nμ2 = 1.0\nα = 0.1\nβ = 0.8\nσ = 0.2\nN = 20\nK = 1000\n\n\nimport statsmodels.formula.api as smf\n\n\ndf = generate_dataset(μ1, μ2, α, β, σ, N=N)\n\n\nres = smf.ols(formula='y ~ x + 1', data=df).fit()\nparams = res.params\nαhat = params['Intercept']\nβhat = params['x']\nσhat = res.resid.std()\n\n\nres.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\ny\nR-squared:\n0.692\n\n\nModel:\nOLS\nAdj. R-squared:\n0.675\n\n\nMethod:\nLeast Squares\nF-statistic:\n40.48\n\n\nDate:\nTue, 26 Jan 2021\nProb (F-statistic):\n5.41e-06\n\n\nTime:\n04:02:36\nLog-Likelihood:\n7.6662\n\n\nNo. Observations:\n20\nAIC:\n-11.33\n\n\nDf Residuals:\n18\nBIC:\n-9.341\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n0.1210\n0.077\n1.565\n0.135\n-0.041\n0.283\n\n\nx\n0.7941\n0.125\n6.362\n0.000\n0.532\n1.056\n\n\n\n\n\n\nOmnibus:\n1.410\nDurbin-Watson:\n1.507\n\n\nProb(Omnibus):\n0.494\nJarque-Bera (JB):\n0.890\n\n\nSkew:\n-0.081\nProb(JB):\n0.641\n\n\nKurtosis:\n1.979\nCond. No.\n4.20\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nres.predict(df['x'])\n\n0     0.326200\n1     0.211704\n2     0.798819\n3     0.603306\n4     0.573319\n5     0.823919\n6     0.740622\n7     0.503227\n8     0.292622\n9     0.489566\n10    0.138720\n11    0.355157\n12    0.594171\n13    0.883917\n14    0.266229\n15    0.827021\n16    0.912376\n17    0.163088\n18    0.684858\n19    0.732782\ndtype: float64\n\n\n\nfor i in [1,2,3]:\n    \n    fig = plt.figure(figsize=(10,14))\n    plt.subplot(311)\n    plot_distribution(0.1, 0.8, 0.2)\n    plt.grid()\n    plt.title(f\"True Distribution: $y = {α:.2f} + {β:.2f} x + {σ:.2f} u$\")\n    plt.xlim(0,1)\n    plt.ylim(-0.5, 1.5)\n\n    plt.subplot(312)\n    plt.xlim(0,1)\n    plt.ylim(-0.5, 1.5)\n    if i&gt;=2:\n        plt.plot(df['x'], df['y'], 'o')\n    if i&gt;=3:\n        plt.plot(df['x'], res.predict(), label=f'$\\hat{{α}}={αhat:.2f}; \\hat{{β}}={βhat:.2f}$')\n        plt.legend(loc='lower right')\n    plt.title(\"Random Draw\")\n    plt.grid()\n    \n    plt.savefig(f\"regression_uncertainty_{i}.png\")\n\n\n\n\n\n\n\n\n\n\n\nimport scipy.stats\n\n\ndatasets = [generate_dataset(μ1, μ2, αhat, βhat, σhat, N=N) for i in range(K)]\nall_params = [smf.ols(formula='x ~ y + 1', data=df).fit() for df in datasets]\nαvec = np.array( [e.params['Intercept'] for e in all_params] )\nβvec = np.array( [e.params['y'] for e in all_params] )\n\n\ngkd = scipy.stats.kde.gaussian_kde(βvec)\n\n\nfor i in [1,2,3,4,5,6,7,8,9,10,100]:\n\n    fig = plt.figure(figsize=(10,14))\n    plt.subplot(311)\n    plot_distribution(0.1, 0.8, 0.2)\n    plt.grid()\n    plt.title(f\"True Distribution: $y = {αhat:.2f} + {βhat:.2f} x + {σhat:.2f} u$\")\n    plt.xlim(0,1)\n    plt.ylim(-0.5, 1.5)\n    \n    plt.subplot(312)\n    plt.xlim(0,1)\n    plt.ylim(-0.5, 1.5)\n    df = datasets[i]\n    if i&gt;=2:\n        plt.plot(df['x'], df['y'], 'o')\n    plt.title(\"Random Draw\")\n    plt.grid()\n\n    plt.subplot(313)\n    if i==3:\n        plt.plot(βvec[i], βvec[i]*0, 'o')\n    if i&gt;4:\n        plt.plot(βvec[3:i], βvec[3:i]*0, 'o')\n    if i&gt;10:\n        xx = np.linspace(0.2, 1.4, 10000)\n        plt.plot( βvec, gkd.pdf(βvec), '.')\n    plt.title(\"Distribution of β\")\n    plt.xlim(0.2, 1.4)\n    plt.ylim(-0.1, 4)\n    plt.grid()\n\n    plt.tight_layout()\n\n    plt.savefig(f\"random_estimates_{i}.png\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplt.plot( βvec, βvec*0, 'o')"
  },
  {
    "objectID": "session_8/graphs/Untitled1.html",
    "href": "session_8/graphs/Untitled1.html",
    "title": "Data-Based Economics",
    "section": "",
    "text": "from matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n\nimport statsmodels.api as sm\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nincome\neducation\nprestige\n\n\n\n\ncount\n45.000000\n45.000000\n45.000000\n\n\nmean\n41.866667\n52.555556\n47.688889\n\n\nstd\n24.435072\n29.760831\n31.510332\n\n\nmin\n7.000000\n7.000000\n3.000000\n\n\n25%\n21.000000\n26.000000\n16.000000\n\n\n50%\n42.000000\n45.000000\n41.000000\n\n\n75%\n64.000000\n84.000000\n81.000000\n\n\nmax\n81.000000\n100.000000\n97.000000\n\n\n\n\n\n\n\n\ndf.cov()\n\n\n\n\n\n\n\n\nincome\neducation\nprestige\n\n\n\n\nincome\n597.072727\n526.871212\n645.071212\n\n\neducation\n526.871212\n885.707071\n798.904040\n\n\nprestige\n645.071212\n798.904040\n992.901010\n\n\n\n\n\n\n\n\nfrom matplotlib import pyplot as plt\n\n\nplt.figure(figsize=(8,6))\nplt.plot(df['education'],df['income'],'o')\nplt.grid()\nplt.xlabel(\"x (Education)\")\nplt.ylabel(\"y (Income)\")\nplt.savefig(\"data_description.png\")\n\n\n\n\n\nfor i in [1,2,3]:\n    xvec = np.linspace(10,100)\n\n    plt.figure(figsize=(12,8))\n    plt.plot(df['education'],df['income'],'o')\n\n    plt.plot(xvec, xvec * 0 + 50)\n    if i&gt;=2:\n        plt.plot(xvec, xvec )\n    if i&gt;=3:\n        plt.plot(xvec,  90- 0.6*xvec )\n\n    plt.grid()\n    plt.xlabel(\"x (Education)\")\n    plt.ylabel(\"y (Income)\")\n    plt.savefig(f\"which_line_{i}.png\")\n\n\n\n\n\n\n\n\n\n\n\nfrom ipywidgets import interact\n\n\nimport matplotlib.patches as patches\n\n\na = 0.1\nb = 1.0\nind = 23\n\n\napprox =  a + b*xvec\n\n# Create figure and axes\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\nplt.plot(df['education'],df['income'],'o')\nplt.plot(xvec, approx, color='red')\n\nx, y = df['education'][ind], df['income'][ind]\nplt.plot(x, y, 'o', color='red' )\np = a+b*x\nplt.grid(True)\nh = abs(p-y)\nplt.vlines(x, y+h, y, color='red')\n\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.savefig(f\"error_0.png\")\n\n\n\n\n\nplt.vlines?\n\n\nSignature:\nplt.vlines(\n    x,\n    ymin,\n    ymax,\n    colors=None,\n    linestyles='solid',\n    label='',\n    *,\n    data=None,\n    **kwargs,\n)\nDocstring:\nPlot vertical lines.\nPlot vertical lines at each *x* from *ymin* to *ymax*.\nParameters\n----------\nx : float or array-like\n    x-indexes where to plot the lines.\nymin, ymax : float or array-like\n    Respective beginning and end of each line. If scalars are\n    provided, all lines will have same length.\ncolors : list of colors, default: :rc:`lines.color`\nlinestyles : {'solid', 'dashed', 'dashdot', 'dotted'}, optional\nlabel : str, default: ''\nReturns\n-------\n`~matplotlib.collections.LineCollection`\nOther Parameters\n----------------\n**kwargs : `~matplotlib.collections.LineCollection` properties.\nSee Also\n--------\nhlines : horizontal lines\naxvline: vertical line across the axes\nNotes\n-----\n.. note::\n    In addition to the above described arguments, this function can take\n    a *data* keyword argument. If such a *data* argument is given,\n    the following arguments can also be string ``s``, which is\n    interpreted as ``data[s]`` (unless this raises an exception):\n    *x*, *ymin*, *ymax*, *colors*.\n    Objects passed as **data** must support item access (``data[s]``) and\n    membership test (``s in data``).\nFile:      ~/.local/opt/miniconda/lib/python3.8/site-packages/matplotlib/pyplot.py\nType:      function\n\n\n\n\n\na = 0.1\nb = 1.0\nind = 23\n\n\napprox =  a + b*xvec\n\n# Create figure and axes\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\nplt.plot(df['education'],df['income'],'o')\nplt.plot(xvec, approx, color='red')\n\nx, y = df['education'][ind], df['income'][ind]\nplt.plot(x, y, 'o', color='red' )\np = a+b*x\nplt.grid(True)\nh = abs(p-y)\nif p-y&gt;0:\n    # Create a Rectangle patch\n    rect = patches.Rectangle((x,y),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n    ax.add_patch(rect)\n    \nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.savefig(f\"errors_{1}.png\")\n\n\n\n\n\ndef L(a,b):\n    Δ = a + b*df['education'] - df['income']\n    return (Δ**2).sum()\n\n\na = 0.1\nb = 0.8\n\napprox =  a + b*xvec\n\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\n\n\nplt.plot(df['education'],df['income'],'o', label=f\"L({a,b})={L(a,b)}\")\nplt.plot(xvec, approx, color='red')\n\nplt.grid(True)\nfor ind in range(df.shape[0]):\n    \n    x, y = df['education'][ind], df['income'][ind]\n    p = a+b*x\n\n    h = abs(p-y)\n    if p-y&gt;0:\n        # Create a Rectangle patch\n        rect = patches.Rectangle((x,y),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\n    else:\n        rect = patches.Rectangle((x,y-h),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.legend(loc='upper right')\nplt.savefig(f\"errors_2.png\")\n\n\n\n\n\na = 90\nb = -0.6\n\napprox =  a + b*xvec\n\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\n\n\nplt.plot(df['education'],df['income'],'o', label=f\"L({a,b})={L(a,b)}\")\nplt.plot(xvec, approx, color='red')\n\nplt.grid(True)\nfor ind in range(df.shape[0]):\n    \n    x, y = df['education'][ind], df['income'][ind]\n    p = a+b*x\n\n    h = abs(p-y)\n    if p-y&gt;0:\n        # Create a Rectangle patch\n        rect = patches.Rectangle((x,y),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\n    else:\n        rect = patches.Rectangle((x,y-h),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.legend(loc='upper right')\nplt.savefig(f\"errors_3.png\")\n\n\n\n\n\nimport scipy.optimize\n\n\nscipy.optimize.minimize(lambda x: L(x[0], x[1]),np.array([0.5, 0.5]))\n\n      fun: 12480.970174488397\n hess_inv: array([[ 7.14169839e-09, -3.91281920e-09],\n       [-3.91281920e-09,  2.46663613e-09]])\n      jac: array([0.00024414, 0.00012207])\n  message: 'Desired error not necessarily achieved due to precision loss.'\n     nfev: 57\n      nit: 7\n     njev: 19\n   status: 2\n  success: False\n        x: array([10.60350224,  0.59485938])\n\n\n\na = 10\nb = 0.59\n\napprox =  a + b*xvec\n\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\n\n\nplt.plot(df['education'],df['income'],'o', label=f\"L({a,b})={L(a,b)}\")\nplt.plot(xvec, approx, color='red')\n\nplt.grid(True)\nfor ind in range(df.shape[0]):\n    \n    x, y = df['education'][ind], df['income'][ind]\n    p = a+b*x\n\n    h = abs(p-y)\n    if p-y&gt;0:\n        # Create a Rectangle patch\n        rect = patches.Rectangle((x,y),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\n    else:\n        rect = patches.Rectangle((x,y-h),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.legend(loc='upper right')\nplt.savefig(f\"errors_4.png\")\n\n\n\n\n\na = 10\nb = 0.59\n\napprox =  a + b*xvec\n\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\n\n\nplt.plot(df['education'],df['income'],'o', label=f\"L({a,b})={L(a,b)}\")\nplt.plot(xvec, approx, color='red', alpha=0.5)\n\nplt.plot(60, a + b*60, 'o', color='red',)\n\nprint(a+b*60)\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.legend(loc='upper right')\nplt.savefig(f\"prediction.png\")\n\n45.4\n\n\n\n\n\n\na = 10\nb = 0.59\n\napprox =  (a + b*df['education'] - df['income'])\n\nplt.figure(figsize=(12,6))\n\nplt.subplot(121)\nplt.plot(approx)\nplt.grid(False)\nplt.title(\"Residuals\")\n\n\nplt.subplot(122)\ndistplot(approx)\nplt.title(\"Distribution of residuals\")\nplt.grid()\n\nplt.savefig(\"residuals.png\")\n\n/home/pablo/.local/opt/miniconda/lib/python3.8/site-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n  warnings.warn(msg, FutureWarning)\n\n\n\n\n\n\n(a + b*df['education'] - df['income']).std()\n\n16.842782676352154\n\n\n\n\n\n/home/pablo/.local/opt/miniconda/lib/python3.8/site-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n  warnings.warn(msg, FutureWarning)\n\n\n&lt;AxesSubplot:ylabel='Density'&gt;\n\n\n\n\n\n\nfrom scipy.stats import f\n\n\nf(0.3)\n\nTypeError: _parse_args() missing 1 required positional argument: 'dfd'\n\n\n\nnp.rand\n\n\nK = 100\nxvec = np.linspace(0,1,K)\ne1 = np.random.randn(K)*0.1\nyvec = 0.1 + xvec*0.4 + e1\ne2 = np.random.randn(K)*0.05\nyvec2 = 0.1 + xvec*(xvec-1)/2 + e2\ne3 = np.random.randn(K)*xvec/2\nyvec3 = 0.1 + xvec + e3\n\nyvec4 = 0.1 + np.sin(xvec*6) + np.random.randn(K)*xvec/2\n\n\nfrom dolo.numeric.processes import VAR1\n\n\nsim = VAR1( ρ=0.8, Σ=0.001).simulate(N=1,T=100)\nyvec4 = 0.1 + xvec*0.4 + sim.ravel()\n\n\nplt.figure(figsize=(18,6))\nplt.subplot(241)\nplt.plot(xvec, yvec,'o')\nplt.plot(xvec, 0.1 + xvec*0.4 )\nplt.ylabel(\"Series\")\nplt.title(\"white noise\")\nplt.subplot(242)\nplt.plot(xvec, yvec2, 'o')\nplt.plot(xvec, yvec2*0)\nplt.title('nonlinear')\nplt.subplot(243)\nplt.plot(xvec, yvec3,'o')\nplt.plot(xvec, 0.1 + xvec)\nplt.title('heteroskedastic')\nplt.subplot(244)\nplt.plot(xvec, yvec4,'o')\nplt.plot(xvec, xvec*0.6)\n\nplt.title('correlated')\n\n\nplt.subplot(245)\nplt.plot(xvec, e1,'o')\nplt.ylabel(\"Residuals\")\nplt.subplot(246)\nplt.plot(xvec, yvec2-0.075, 'o')\n\nplt.subplot(247)\nplt.plot(xvec, e3,'o')\nplt.subplot(248)\nplt.plot(xvec, sim.ravel(),'o')\n\nplt.tight_layout()\n\nplt.savefig(\"residuals_circus.png\")"
  },
  {
    "objectID": "session_8/slides.html#classification-problem",
    "href": "session_8/slides.html#classification-problem",
    "title": "Introduction to Machine Learning (2)",
    "section": "Classification problem",
    "text": "Classification problem\n\nBinary Classification\n\nGoal is to make a prediction \\(c_n = f(x_{1,1}, ... x_{k,n})\\) …\n…where \\(c_i\\) is a binary variable (\\(\\in\\{0,1\\}\\))\n… and \\((x_{i,n})_k\\), \\(k\\) different features to predict \\(c_n\\)\n\nMulticategory Classification\n\nThe variable to predict takes values in a non ordered set with \\(p\\) different values"
  },
  {
    "objectID": "session_8/slides.html#logistic-regression",
    "href": "session_8/slides.html#logistic-regression",
    "title": "Introduction to Machine Learning (2)",
    "section": "Logistic regression",
    "text": "Logistic regression\n\n\n\nGiven a regression model (a linear predictor) \\[ a_0 + a_1 x_1 + a_2 x_2 + \\cdots a_n x_n \\]\none can build a classification model: \\[ f(x_1, ..., x_n) = \\sigma( a_0 + a_1 x_1 + a_2 x_2 + \\cdots a_n x_n )\\] where \\(\\sigma(x)=\\frac{1}{1+\\exp(-x)}\\) is the logistic function a.k.a. sigmoid\nThe loss function to minimize is: \\[L() = \\sum_n (c_n - \\sigma( a_{0} + a_1 x_{1,n} + a_2 x_{2,n} + \\cdots a_k x_{k,n} ) )^2\\]\nThis works for any regression model (LASSO, RIDGE, nonlinear…)"
  },
  {
    "objectID": "session_8/slides.html#logistic-regression-1",
    "href": "session_8/slides.html#logistic-regression-1",
    "title": "Introduction to Machine Learning (2)",
    "section": "Logistic regression",
    "text": "Logistic regression\n\nThe linear model predicts an intensity/score (not a category) \\[ f(x_1, ..., x_n) = \\sigma( \\underbrace{a_0 + a_1 x_1 + a_2 x_2 + \\cdots a_n x_n }_{\\text{score}})\\]\nTo make a prediction: round to 0 or 1."
  },
  {
    "objectID": "session_8/slides.html#multinomial-regression",
    "href": "session_8/slides.html#multinomial-regression",
    "title": "Introduction to Machine Learning (2)",
    "section": "Multinomial regression",
    "text": "Multinomial regression\n\n\nIf there are \\(P\\) categories to predict:\n\nbuild a linear predictor \\(f_p\\) for each category \\(p\\)\nlinear predictor is also called score\n\nTo predict:\n\nevaluate the score of all categories\nchoose the one with highest score\n\nTo train the model:\n\ntrain separately all scores (works for any predictor, not just linear)\n… there are more subtle approaches (not here)"
  },
  {
    "objectID": "session_8/slides.html#common-classification-algorithms",
    "href": "session_8/slides.html#common-classification-algorithms",
    "title": "Introduction to Machine Learning (2)",
    "section": "Common classification algorithms",
    "text": "Common classification algorithms\nThere are many:\n\nLogistic Regression\nNaive Bayes Classifier\nNearest Distance\nneural networks (replace score in sigmoid by n.n.)\nDecision Trees\nSupport Vector Machines"
  },
  {
    "objectID": "session_8/slides.html#nearest-distance",
    "href": "session_8/slides.html#nearest-distance",
    "title": "Introduction to Machine Learning (2)",
    "section": "Nearest distance",
    "text": "Nearest distance\n\n\n\nIdea:\n\nin order to predict category \\(c\\) corresponding to \\(x\\) find the closest point \\(x_0\\) in the training set\nAssign to \\(x\\) the same category as \\(x_0\\)\n\nBut this would be very susceptible to noise\nAmended idea: \\(k-nearest\\) neighbours\n\nlook for the \\(k\\) points closest to \\(x\\)\nlabel \\(x\\) with the same category as the majority of them\n\nRemark: this algorithm uses Euclidean distance. This is why it is important to normalize the dataset."
  },
  {
    "objectID": "session_8/slides.html#decision-tree-random-forests",
    "href": "session_8/slides.html#decision-tree-random-forests",
    "title": "Introduction to Machine Learning (2)",
    "section": "Decision Tree / Random Forests",
    "text": "Decision Tree / Random Forests\n\n\n\nDecision Tree\n\nrecursively find simple criteria to subdivide dataset\n\nProblems:\n\nGreedy: algorithm does not simplify branches\neasily overfits\n\nExtension : random tree forest\n\nuses several (randomly generated) trees to generate a prediction\nsolves the overfitting problem"
  },
  {
    "objectID": "session_8/slides.html#support-vector-classification",
    "href": "session_8/slides.html#support-vector-classification",
    "title": "Introduction to Machine Learning (2)",
    "section": "Support Vector Classification",
    "text": "Support Vector Classification\n\n\n\n\nSeparates data by one line (hyperplane).\n\nChooses the largest margin according to support vectors\n\nCan use a nonlinear kernel."
  },
  {
    "objectID": "session_8/slides.html#all-these-algorithms-are-super-easy-to-use",
    "href": "session_8/slides.html#all-these-algorithms-are-super-easy-to-use",
    "title": "Introduction to Machine Learning (2)",
    "section": "All these algorithms are super easy to use!",
    "text": "All these algorithms are super easy to use!\nExamples:\n\nDecision Tree\n\nfrom sklearn.tree import DecisionTreeClassifier\nclf = DecisionTreeClassifier(random_state=0)\n\n\nSupport Vector\n\nfrom sklearn.svm import SVC\nclf = SVC(random_state=0)\n\n\n\nRidge Regression\n\nfrom sklearn.linear_model import Ridge\nclf = Ridge(random_state=0)"
  },
  {
    "objectID": "session_8/slides.html#validity-of-a-classification-algorithm",
    "href": "session_8/slides.html#validity-of-a-classification-algorithm",
    "title": "Introduction to Machine Learning (2)",
    "section": "Validity of a classification algorithm",
    "text": "Validity of a classification algorithm\n\nIndependently of how the classification is made, its validity can be assessed with a similar procedure as in the regression.\nSeparate training set and test set\n\ndo not touch test set at all during the training\n\nCompute score: number of correctly identified categories\n\nnote that this is not the same as the loss function minimized by the training"
  },
  {
    "objectID": "session_8/slides.html#classification-matrix",
    "href": "session_8/slides.html#classification-matrix",
    "title": "Introduction to Machine Learning (2)",
    "section": "Classification matrix",
    "text": "Classification matrix\n\nFor binary classification, we focus on the classification matrix or confusion matrix.\n\n\n\n\nPredicted\n(0) Actual\n(1) Actual\n\n\n\n\n0\ntrue negatives (TN)\nfalse negatives (FN)\n\n\n1\nfalse positives (FP)\ntrue positives (TP)\n\n\n\n\nWe can then define different measures:\n\nSensitivity aka True Positive Rate (TPR): \\(\\frac{TP}{FP+TP}\\)\nFalse Positive Rate (FPR): \\(\\frac{FP}{TN+FP}\\)\nOverall accuracy: \\(\\frac{\\text{TN}+\\text{TP}}{\\text{total}}\\)\n\n\n\nWhich one to favour depends on the use case"
  },
  {
    "objectID": "session_8/slides.html#example-london-police",
    "href": "session_8/slides.html#example-london-police",
    "title": "Introduction to Machine Learning (2)",
    "section": "Example: London Police",
    "text": "Example: London Police\n\nPolice cameras in LondonAccording to London Police the cameras in London have\n\nTrue Positive Identification rate of over 80% at a fixed number of False Positive Alerts.29 nov. 2022\n\n\nInterpretation? Is failure rate too high?"
  },
  {
    "objectID": "session_8/slides.html#example",
    "href": "session_8/slides.html#example",
    "title": "Introduction to Machine Learning (2)",
    "section": "Example",
    "text": "Example\n\nIn-sample confusion matrixBased on consumer data, an algorithm tries to predict the credit score from.\nCan you calculate: FPR, TPR and overall accuracy?"
  },
  {
    "objectID": "session_8/slides.html#confusion-matrix-with-sklearn",
    "href": "session_8/slides.html#confusion-matrix-with-sklearn",
    "title": "Introduction to Machine Learning (2)",
    "section": "Confusion matrix with sklearn",
    "text": "Confusion matrix with sklearn\n\nPredict on the test set:\n\ny_pred = model.predict(x_test)\n\nCompute confusion matrix:\n\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)"
  },
  {
    "objectID": "session_8/index.html",
    "href": "session_8/index.html",
    "title": "Introduction to Machine Learning (2)",
    "section": "",
    "text": "Binary Classification\n\nGoal is to make a prediction \\(c_n = f(x_{1,1}, ... x_{k,n})\\) …\n…where \\(c_i\\) is a binary variable (\\(\\in\\{0,1\\}\\))\n… and \\((x_{i,n})_k\\), \\(k\\) different features to predict \\(c_n\\)\n\nMulticategory Classification\n\nThe variable to predict takes values in a non ordered set with \\(p\\) different values\n\n\n\n\n\n\n\n\nGiven a regression model (a linear predictor) \\[ a_0 + a_1 x_1 + a_2 x_2 + \\cdots a_n x_n \\]\none can build a classification model: \\[ f(x_1, ..., x_n) = \\sigma( a_0 + a_1 x_1 + a_2 x_2 + \\cdots a_n x_n )\\] where \\(\\sigma(x)=\\frac{1}{1+\\exp(-x)}\\) is the logistic function a.k.a. sigmoid\nThe loss function to minimize is: \\[L() = \\sum_n (c_n - \\sigma( a_{0} + a_1 x_{1,n} + a_2 x_{2,n} + \\cdots a_k x_{k,n} ) )^2\\]\nThis works for any regression model (LASSO, RIDGE, nonlinear…)\n\n\n\n\n\n\n\n\n\nThe linear model predicts an intensity/score (not a category) \\[ f(x_1, ..., x_n) = \\sigma( \\underbrace{a_0 + a_1 x_1 + a_2 x_2 + \\cdots a_n x_n }_{\\text{score}})\\]\nTo make a prediction: round to 0 or 1.\n\n\n\n\n\n\n\nIf there are \\(P\\) categories to predict:\n\nbuild a linear predictor \\(f_p\\) for each category \\(p\\)\nlinear predictor is also called score\n\nTo predict:\n\nevaluate the score of all categories\nchoose the one with highest score\n\nTo train the model:\n\ntrain separately all scores (works for any predictor, not just linear)\n… there are more subtle approaches (not here)"
  },
  {
    "objectID": "session_8/index.html#classification-problem",
    "href": "session_8/index.html#classification-problem",
    "title": "Introduction to Machine Learning (2)",
    "section": "",
    "text": "Binary Classification\n\nGoal is to make a prediction \\(c_n = f(x_{1,1}, ... x_{k,n})\\) …\n…where \\(c_i\\) is a binary variable (\\(\\in\\{0,1\\}\\))\n… and \\((x_{i,n})_k\\), \\(k\\) different features to predict \\(c_n\\)\n\nMulticategory Classification\n\nThe variable to predict takes values in a non ordered set with \\(p\\) different values"
  },
  {
    "objectID": "session_8/index.html#logistic-regression",
    "href": "session_8/index.html#logistic-regression",
    "title": "Introduction to Machine Learning (2)",
    "section": "",
    "text": "Given a regression model (a linear predictor) \\[ a_0 + a_1 x_1 + a_2 x_2 + \\cdots a_n x_n \\]\none can build a classification model: \\[ f(x_1, ..., x_n) = \\sigma( a_0 + a_1 x_1 + a_2 x_2 + \\cdots a_n x_n )\\] where \\(\\sigma(x)=\\frac{1}{1+\\exp(-x)}\\) is the logistic function a.k.a. sigmoid\nThe loss function to minimize is: \\[L() = \\sum_n (c_n - \\sigma( a_{0} + a_1 x_{1,n} + a_2 x_{2,n} + \\cdots a_k x_{k,n} ) )^2\\]\nThis works for any regression model (LASSO, RIDGE, nonlinear…)"
  },
  {
    "objectID": "session_8/index.html#logistic-regression-1",
    "href": "session_8/index.html#logistic-regression-1",
    "title": "Introduction to Machine Learning (2)",
    "section": "",
    "text": "The linear model predicts an intensity/score (not a category) \\[ f(x_1, ..., x_n) = \\sigma( \\underbrace{a_0 + a_1 x_1 + a_2 x_2 + \\cdots a_n x_n }_{\\text{score}})\\]\nTo make a prediction: round to 0 or 1."
  },
  {
    "objectID": "session_8/index.html#multinomial-regression",
    "href": "session_8/index.html#multinomial-regression",
    "title": "Introduction to Machine Learning (2)",
    "section": "",
    "text": "If there are \\(P\\) categories to predict:\n\nbuild a linear predictor \\(f_p\\) for each category \\(p\\)\nlinear predictor is also called score\n\nTo predict:\n\nevaluate the score of all categories\nchoose the one with highest score\n\nTo train the model:\n\ntrain separately all scores (works for any predictor, not just linear)\n… there are more subtle approaches (not here)"
  },
  {
    "objectID": "session_8/index.html#common-classification-algorithms",
    "href": "session_8/index.html#common-classification-algorithms",
    "title": "Introduction to Machine Learning (2)",
    "section": "Common classification algorithms",
    "text": "Common classification algorithms\nThere are many:\n\nLogistic Regression\nNaive Bayes Classifier\nNearest Distance\nneural networks (replace score in sigmoid by n.n.)\nDecision Trees\nSupport Vector Machines"
  },
  {
    "objectID": "session_8/index.html#nearest-distance",
    "href": "session_8/index.html#nearest-distance",
    "title": "Introduction to Machine Learning (2)",
    "section": "Nearest distance",
    "text": "Nearest distance\n\n\n\nIdea:\n\nin order to predict category \\(c\\) corresponding to \\(x\\) find the closest point \\(x_0\\) in the training set\nAssign to \\(x\\) the same category as \\(x_0\\)\n\nBut this would be very susceptible to noise\nAmended idea: \\(k-nearest\\) neighbours\n\nlook for the \\(k\\) points closest to \\(x\\)\nlabel \\(x\\) with the same category as the majority of them\n\nRemark: this algorithm uses Euclidean distance. This is why it is important to normalize the dataset."
  },
  {
    "objectID": "session_8/index.html#decision-tree-random-forests",
    "href": "session_8/index.html#decision-tree-random-forests",
    "title": "Introduction to Machine Learning (2)",
    "section": "Decision Tree / Random Forests",
    "text": "Decision Tree / Random Forests\n\n\n\nDecision Tree\n\nrecursively find simple criteria to subdivide dataset\n\nProblems:\n\nGreedy: algorithm does not simplify branches\neasily overfits\n\nExtension : random tree forest\n\nuses several (randomly generated) trees to generate a prediction\nsolves the overfitting problem"
  },
  {
    "objectID": "session_8/index.html#support-vector-classification",
    "href": "session_8/index.html#support-vector-classification",
    "title": "Introduction to Machine Learning (2)",
    "section": "Support Vector Classification",
    "text": "Support Vector Classification\n\n\n\n\nSeparates data by one line (hyperplane).\n\nChooses the largest margin according to support vectors\n\nCan use a nonlinear kernel."
  },
  {
    "objectID": "session_8/index.html#all-these-algorithms-are-super-easy-to-use",
    "href": "session_8/index.html#all-these-algorithms-are-super-easy-to-use",
    "title": "Introduction to Machine Learning (2)",
    "section": "All these algorithms are super easy to use!",
    "text": "All these algorithms are super easy to use!\nExamples:\n\nDecision Tree\n\nfrom sklearn.tree import DecisionTreeClassifier\nclf = DecisionTreeClassifier(random_state=0)\n. . .\n\nSupport Vector\n\nfrom sklearn.svm import SVC\nclf = SVC(random_state=0)\n. . .\n\nRidge Regression\n\nfrom sklearn.linear_model import Ridge\nclf = Ridge(random_state=0)"
  },
  {
    "objectID": "session_8/index.html#validity-of-a-classification-algorithm",
    "href": "session_8/index.html#validity-of-a-classification-algorithm",
    "title": "Introduction to Machine Learning (2)",
    "section": "Validity of a classification algorithm",
    "text": "Validity of a classification algorithm\n\nIndependently of how the classification is made, its validity can be assessed with a similar procedure as in the regression.\nSeparate training set and test set\n\ndo not touch test set at all during the training\n\nCompute score: number of correctly identified categories\n\nnote that this is not the same as the loss function minimized by the training"
  },
  {
    "objectID": "session_8/index.html#classification-matrix",
    "href": "session_8/index.html#classification-matrix",
    "title": "Introduction to Machine Learning (2)",
    "section": "Classification matrix",
    "text": "Classification matrix\n\nFor binary classification, we focus on the classification matrix or confusion matrix.\n\n\n\n\nPredicted\n(0) Actual\n(1) Actual\n\n\n\n\n0\ntrue negatives (TN)\nfalse negatives (FN)\n\n\n1\nfalse positives (FP)\ntrue positives (TP)\n\n\n\n. . .\nWe can then define different measures:\n\nSensitivity aka True Positive Rate (TPR): \\(\\frac{TP}{FP+TP}\\)\nFalse Positive Rate (FPR): \\(\\frac{FP}{TN+FP}\\)\nOverall accuracy: \\(\\frac{\\text{TN}+\\text{TP}}{\\text{total}}\\)\n\n. . .\nWhich one to favour depends on the use case"
  },
  {
    "objectID": "session_8/index.html#example-london-police",
    "href": "session_8/index.html#example-london-police",
    "title": "Introduction to Machine Learning (2)",
    "section": "Example: London Police",
    "text": "Example: London Police\n\n\n\nPolice cameras in London\n\n\nAccording to London Police the cameras in London have\n\nTrue Positive Identification rate of over 80% at a fixed number of False Positive Alerts.29 nov. 2022\n\n. . .\nInterpretation? Is failure rate too high?"
  },
  {
    "objectID": "session_8/index.html#example",
    "href": "session_8/index.html#example",
    "title": "Introduction to Machine Learning (2)",
    "section": "Example",
    "text": "Example\n\n\n\nIn-sample confusion matrix\n\n\nBased on consumer data, an algorithm tries to predict the credit score from.\nCan you calculate: FPR, TPR and overall accuracy?"
  },
  {
    "objectID": "session_8/index.html#confusion-matrix-with-sklearn",
    "href": "session_8/index.html#confusion-matrix-with-sklearn",
    "title": "Introduction to Machine Learning (2)",
    "section": "Confusion matrix with sklearn",
    "text": "Confusion matrix with sklearn\n\nPredict on the test set:\n\ny_pred = model.predict(x_test)\n\nCompute confusion matrix:\n\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)"
  },
  {
    "objectID": "session_8/machine_learning_regressions_correction.html",
    "href": "session_8/machine_learning_regressions_correction.html",
    "title": "Machine learning: regressions",
    "section": "",
    "text": "Objectives:"
  },
  {
    "objectID": "session_8/machine_learning_regressions_correction.html#diabetes-dataset-basic-regression",
    "href": "session_8/machine_learning_regressions_correction.html#diabetes-dataset-basic-regression",
    "title": "Machine learning: regressions",
    "section": "Diabetes dataset: basic regression",
    "text": "Diabetes dataset: basic regression\nImport the diabetes dataset from sklearn. Describe it.\n\nimport sklearn\nimport sklearn.datasets\n\ndataset = sklearn.datasets.load_diabetes()\n# the result is a dictionary:\n# 'data': features\n# 'target' labels\n# 'feature_names': names of the features\n# `DESCR`: description\n\n\nprint( dataset['DESCR'] )\n\n.. _diabetes_dataset:\n\nDiabetes dataset\n----------------\n\nTen baseline variables, age, sex, body mass index, average blood\npressure, and six blood serum measurements were obtained for each of n =\n442 diabetes patients, as well as the response of interest, a\nquantitative measure of disease progression one year after baseline.\n\n**Data Set Characteristics:**\n\n  :Number of Instances: 442\n\n  :Number of Attributes: First 10 columns are numeric predictive values\n\n  :Target: Column 11 is a quantitative measure of disease progression one year after baseline\n\n  :Attribute Information:\n      - age     age in years\n      - sex\n      - bmi     body mass index\n      - bp      average blood pressure\n      - s1      tc, total serum cholesterol\n      - s2      ldl, low-density lipoproteins\n      - s3      hdl, high-density lipoproteins\n      - s4      tch, total cholesterol / HDL\n      - s5      ltg, possibly log of serum triglycerides level\n      - s6      glu, blood sugar level\n\nNote: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times the square root of `n_samples` (i.e. the sum of squares of each column totals 1).\n\nSource URL:\nhttps://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\n\nFor more information see:\nBradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\n(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\n\n\n\n\n# create a dataframe\nimport pandas\n\ndf = pandas.DataFrame(dataset['data'], columns=dataset['feature_names'])\n\ndf['disease_progression'] = dataset['target']\n\n\ndf.describe()\n# we observe that mean of varaibles  is zero\n# standard deviations are the same for all variables\n# model has been normalized already:\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nbp\ns1\ns2\ns3\ns4\ns5\ns6\ndisease_progression\n\n\n\n\ncount\n4.420000e+02\n4.420000e+02\n4.420000e+02\n4.420000e+02\n4.420000e+02\n4.420000e+02\n4.420000e+02\n4.420000e+02\n4.420000e+02\n4.420000e+02\n442.000000\n\n\nmean\n-2.511817e-19\n1.230790e-17\n-2.245564e-16\n-4.797570e-17\n-1.381499e-17\n3.918434e-17\n-5.777179e-18\n-9.042540e-18\n9.293722e-17\n1.130318e-17\n152.133484\n\n\nstd\n4.761905e-02\n4.761905e-02\n4.761905e-02\n4.761905e-02\n4.761905e-02\n4.761905e-02\n4.761905e-02\n4.761905e-02\n4.761905e-02\n4.761905e-02\n77.093005\n\n\nmin\n-1.072256e-01\n-4.464164e-02\n-9.027530e-02\n-1.123988e-01\n-1.267807e-01\n-1.156131e-01\n-1.023071e-01\n-7.639450e-02\n-1.260971e-01\n-1.377672e-01\n25.000000\n\n\n25%\n-3.729927e-02\n-4.464164e-02\n-3.422907e-02\n-3.665608e-02\n-3.424784e-02\n-3.035840e-02\n-3.511716e-02\n-3.949338e-02\n-3.324559e-02\n-3.317903e-02\n87.000000\n\n\n50%\n5.383060e-03\n-4.464164e-02\n-7.283766e-03\n-5.670422e-03\n-4.320866e-03\n-3.819065e-03\n-6.584468e-03\n-2.592262e-03\n-1.947171e-03\n-1.077698e-03\n140.500000\n\n\n75%\n3.807591e-02\n5.068012e-02\n3.124802e-02\n3.564379e-02\n2.835801e-02\n2.984439e-02\n2.931150e-02\n3.430886e-02\n3.243232e-02\n2.791705e-02\n211.500000\n\n\nmax\n1.107267e-01\n5.068012e-02\n1.705552e-01\n1.320436e-01\n1.539137e-01\n1.987880e-01\n1.811791e-01\n1.852344e-01\n1.335973e-01\n1.356118e-01\n346.000000\n\n\n\n\n\n\n\n\nimport seaborn\n\n\nseaborn.pairplot(df)\n\n\n\n\nSplit the dataset into a training set (70%) and a test set (30%)\n\nfrom sklearn.model_selection import train_test_split\n\n\n# features: dataset['data']\n# dataset['data'].shape # one line per observation, one column per feature (variable)\n\n\n# labels: dataset['target'] what we are trying to predict\ndataset['target'].shape\n\n(442,)\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(dataset['data'], dataset['target'], test_size=0.3, random_state=56)\n# the choice of a random_state initializes a random seed so that every time it is run the notebook\n# returns exactly the same results\n\nTrain a linear model (with intercept) on the training set\n\n# since the model is already normalized, we can create the model directly\nfrom sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression()   # don't forget the round bracket to get a model object\nmodel.fit(X_train, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\n# visualize model predictions:\n\n# from matplotlib import pyplot as plt\n\n# plt.plot(  )\n# plt.plot( model.predict(X_train) )\n\n\nmodel.intercept_ # a\n\n152.82810842206453\n\n\n\nmodel.coef_ # b_1, b_2, .... b_10|\n\narray([   3.04174075, -209.76813682,  501.77871853,  286.88207011,\n       -991.92731799,  603.10838272,  228.80501285,  226.30296964,\n        905.67772303,   92.55739263])\n\n\nCompute the fitting score on the test set. (Bonus: compare with your own computation of \\(R^2\\))\n\nmodel.score(X_test, y_test)\n\n0.43965636272283437\n\n\n\n# compare with the training set:\nmodel.score(X_train, y_train)\n\n0.541861476456197\n\n\nShould we adjust the size of the test set? What would be the problem?\n\n#### WARNING\n####\n#### very bad approach\n\n\n# let's try different sizes\n\nsizes = [0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\nscores = []\nfor s in sizes:\n    X_train, X_test, y_train, y_test = train_test_split(dataset['data'], dataset['target'], test_size=0.3)\n\n    model = LinearRegression()   # don't forget the round bracket to get a model object\n    model.fit(X_train, y_train)\n    score = model.score(X_test, y_test) # score with x% test set\n\n    scores.append(score)\n\n\nfrom matplotlib import pyplot as plt\nplt.plot(sizes, scores)\n\n\n\n\nImplement \\(k\\)-fold model with \\(k=3\\).\n\nX = dataset['data']\ny = dataset['target']\n\n\n# to keep the scores\nscores = []\n\n\nfrom sklearn.model_selection import KFold\nkf = KFold(n_splits=3)\n\nfor train_index, test_index in kf.split(X):\n    \n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n    ## train a model in X_train, y_train\n    ## test it on X_test, y_test\n    model = LinearRegression()   # don't forget the round bracket to get a model object\n    model.fit(X_train, y_train)\n    score = model.score(X_test, y_test) # score with x% test set\n\n    scores.append(score)\n\n\nscores\n\n[0.46930417754348197, 0.4872526062543143, 0.5095496056127979]\n\n\n\n# it gives us a sense of the predictive power of the regression\n\nBonus: use statsmodels (or linearmodels) to estimate the same linear model on the full sample. Is it always a superior method?\n\nimport statsmodels\nfrom statsmodels.formula import api as smf\n\n\ndf.columns\n\nIndex(['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6',\n       'disease_progression'],\n      dtype='object')\n\n\n\nregmodel = smf.ols(formula=\"disease_progression ~ age + sex + bmi + bp + s1 + s2 + s3 + s4 + s5 + s6\", data=df)\nregresults = regmodel.fit()\n\n\nregresults.summary() # econometric estimation of R^2 is 0.51\n\n\nOLS Regression Results\n\n\nDep. Variable:\ndisease_progression\nR-squared:\n0.518\n\n\nModel:\nOLS\nAdj. R-squared:\n0.507\n\n\nMethod:\nLeast Squares\nF-statistic:\n46.27\n\n\nDate:\nMon, 27 Mar 2023\nProb (F-statistic):\n3.83e-62\n\n\nTime:\n21:46:43\nLog-Likelihood:\n-2386.0\n\n\nNo. Observations:\n442\nAIC:\n4794.\n\n\nDf Residuals:\n431\nBIC:\n4839.\n\n\nDf Model:\n10\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n152.1335\n2.576\n59.061\n0.000\n147.071\n157.196\n\n\nage\n-10.0099\n59.749\n-0.168\n0.867\n-127.446\n107.426\n\n\nsex\n-239.8156\n61.222\n-3.917\n0.000\n-360.147\n-119.484\n\n\nbmi\n519.8459\n66.533\n7.813\n0.000\n389.076\n650.616\n\n\nbp\n324.3846\n65.422\n4.958\n0.000\n195.799\n452.970\n\n\ns1\n-792.1756\n416.680\n-1.901\n0.058\n-1611.153\n26.802\n\n\ns2\n476.7390\n339.030\n1.406\n0.160\n-189.620\n1143.098\n\n\ns3\n101.0433\n212.531\n0.475\n0.635\n-316.684\n518.770\n\n\ns4\n177.0632\n161.476\n1.097\n0.273\n-140.315\n494.441\n\n\ns5\n751.2737\n171.900\n4.370\n0.000\n413.407\n1089.140\n\n\ns6\n67.6267\n65.984\n1.025\n0.306\n-62.064\n197.318\n\n\n\n\n\n\nOmnibus:\n1.506\nDurbin-Watson:\n2.029\n\n\nProb(Omnibus):\n0.471\nJarque-Bera (JB):\n1.404\n\n\nSkew:\n0.017\nProb(JB):\n0.496\n\n\nKurtosis:\n2.726\nCond. No.\n227.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nTo use lasso regression:\n\nfrom sklearn.linear_model import Lasso\n\n\nX_train, X_test, y_train, y_test = train_test_split(dataset['data'], dataset['target'], test_size=0.3)\nmodel = Lasso()   # don't forget the round bracket to get a model object\nmodel.fit(X_train, y_train)\nscore = model.score(X_test, y_test) # s\n\n\n# on the test set, the fit of the lasso regression is worse than regular regression\n# the regularization parameter should be changed"
  },
  {
    "objectID": "session_8/machine_learning_regressions_correction.html#sparse-regressions-on-the-boston-house-price-dataset",
    "href": "session_8/machine_learning_regressions_correction.html#sparse-regressions-on-the-boston-house-price-dataset",
    "title": "Machine learning: regressions",
    "section": "Sparse regressions on the Boston House Price Dataset",
    "text": "Sparse regressions on the Boston House Price Dataset\n!!! update: boston price dataset has been deprecated\n!!! use california_housing instead\nImport the Boston House Price Dataset from sklearn. Explore the data (description, correlations, histograms…)\n\n# dataset = sklearn.datasets.load_boston()\nfrom sklearn.datasets import fetch_california_housing \ndataset = fetch_california_housing()\n\n\nprint(dataset[\"DESCR\"])\n\n.. _california_housing_dataset:\n\nCalifornia Housing dataset\n--------------------------\n\n**Data Set Characteristics:**\n\n    :Number of Instances: 20640\n\n    :Number of Attributes: 8 numeric, predictive attributes and the target\n\n    :Attribute Information:\n        - MedInc        median income in block group\n        - HouseAge      median house age in block group\n        - AveRooms      average number of rooms per household\n        - AveBedrms     average number of bedrooms per household\n        - Population    block group population\n        - AveOccup      average number of household members\n        - Latitude      block group latitude\n        - Longitude     block group longitude\n\n    :Missing Attribute Values: None\n\nThis dataset was obtained from the StatLib repository.\nhttps://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\n\nThe target variable is the median house value for California districts,\nexpressed in hundreds of thousands of dollars ($100,000).\n\nThis dataset was derived from the 1990 U.S. census, using one row per census\nblock group. A block group is the smallest geographical unit for which the U.S.\nCensus Bureau publishes sample data (a block group typically has a population\nof 600 to 3,000 people).\n\nAn household is a group of people residing within a home. Since the average\nnumber of rooms and bedrooms in this dataset are provided per household, these\ncolumns may take surpinsingly large values for block groups with few households\nand many empty houses, such as vacation resorts.\n\nIt can be downloaded/loaded using the\n:func:`sklearn.datasets.fetch_california_housing` function.\n\n.. topic:: References\n\n    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n      Statistics and Probability Letters, 33 (1997) 291-297\n\n\n\n\n# dataset = sklearn.datasets.load_boston()\nfrom sklearn.datasets import fetch_california_housing \ndataset = fetch_california_housing()\n\nSplit the dataset into a training set (70%) and a test set (30%).\n\nX = dataset['data']\ny = dataset['target']\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=58)\n\nTrain a lasso model to predict house prices. Compute the score on the test set.\n\n# we should check that the data is normalized, or normalize it ourselves\n\n\nfrom sklearn.linear_model import Lasso\nmodel_lasso = Lasso()\nmodel_lasso.fit(X_train, y_train)\n\nLasso()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LassoLasso()\n\n\n\nmodel_lasso.score(X_test, y_test)\n\n0.28204855993177635\n\n\nTrain a ridge model to predict house prices. Which one is better?\n\nfrom sklearn.linear_model import Ridge\nmodel_ridge = Ridge()\nmodel_ridge.fit(X_train, y_train)\n\nRidge()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RidgeRidge()\n\n\n\nmodel_ridge.score(X_test, y_test)\n\n0.6060031802405054\n\n\nIt looks like the ridge model has a better fit (score). However, we should have left a test set appart and not used it at all during training phase. Here it has influenced the choice of the model (between ridge and lasso)."
  },
  {
    "objectID": "session_9/graphs/inference.html",
    "href": "session_9/graphs/inference.html",
    "title": "Data-Based Economics",
    "section": "",
    "text": "from matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n\ndef generate_dataset(μ1, μ2, α, β, σ, N=10):\n    xvec = np.random.uniform(μ1, μ2, N)\n    yvec = α + β*xvec + np.random.normal(size=N)*σ\n    return pd.DataFrame({'x': xvec, 'y': yvec})\n\n\ndf = generate_dataset(0.0, 1.0, 0.1, 0.8, 0.1)\n\n\nplt.plot(df['x'], df['y'], 'o')\nplt.grid()\n\n\n\n\n\ndef plot_distribution(α, β, σ, N=100000, μ1=0.0, μ2=1.0):\n    xvec = np.random.uniform(μ1, μ2, N)\n    yvec = α + β*xvec + np.random.normal(size=N)*σ\n    plt.plot(xvec, yvec, '.r', alpha=0.005)\n    plt.plot(xvec, α + β*xvec, color='black')\n\n# missing ridge line\n\n\nimport statsmodels\n\n\nμ1 = 0\nμ2 = 1.0\nα = 0.1\nβ = 0.8\nσ = 0.2\nN = 20\nK = 1000\n\n\nimport statsmodels.formula.api as smf\n\n\ndf = generate_dataset(μ1, μ2, α, β, σ, N=N)\n\n\nres = smf.ols(formula='y ~ x + 1', data=df).fit()\nparams = res.params\nαhat = params['Intercept']\nβhat = params['x']\nσhat = res.resid.std()\n\n\nres.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\ny\nR-squared:\n0.692\n\n\nModel:\nOLS\nAdj. R-squared:\n0.675\n\n\nMethod:\nLeast Squares\nF-statistic:\n40.48\n\n\nDate:\nTue, 26 Jan 2021\nProb (F-statistic):\n5.41e-06\n\n\nTime:\n04:02:36\nLog-Likelihood:\n7.6662\n\n\nNo. Observations:\n20\nAIC:\n-11.33\n\n\nDf Residuals:\n18\nBIC:\n-9.341\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n0.1210\n0.077\n1.565\n0.135\n-0.041\n0.283\n\n\nx\n0.7941\n0.125\n6.362\n0.000\n0.532\n1.056\n\n\n\n\n\n\nOmnibus:\n1.410\nDurbin-Watson:\n1.507\n\n\nProb(Omnibus):\n0.494\nJarque-Bera (JB):\n0.890\n\n\nSkew:\n-0.081\nProb(JB):\n0.641\n\n\nKurtosis:\n1.979\nCond. No.\n4.20\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nres.predict(df['x'])\n\n0     0.326200\n1     0.211704\n2     0.798819\n3     0.603306\n4     0.573319\n5     0.823919\n6     0.740622\n7     0.503227\n8     0.292622\n9     0.489566\n10    0.138720\n11    0.355157\n12    0.594171\n13    0.883917\n14    0.266229\n15    0.827021\n16    0.912376\n17    0.163088\n18    0.684858\n19    0.732782\ndtype: float64\n\n\n\nfor i in [1,2,3]:\n    \n    fig = plt.figure(figsize=(10,14))\n    plt.subplot(311)\n    plot_distribution(0.1, 0.8, 0.2)\n    plt.grid()\n    plt.title(f\"True Distribution: $y = {α:.2f} + {β:.2f} x + {σ:.2f} u$\")\n    plt.xlim(0,1)\n    plt.ylim(-0.5, 1.5)\n\n    plt.subplot(312)\n    plt.xlim(0,1)\n    plt.ylim(-0.5, 1.5)\n    if i&gt;=2:\n        plt.plot(df['x'], df['y'], 'o')\n    if i&gt;=3:\n        plt.plot(df['x'], res.predict(), label=f'$\\hat{{α}}={αhat:.2f}; \\hat{{β}}={βhat:.2f}$')\n        plt.legend(loc='lower right')\n    plt.title(\"Random Draw\")\n    plt.grid()\n    \n    plt.savefig(f\"regression_uncertainty_{i}.png\")\n\n\n\n\n\n\n\n\n\n\n\nimport scipy.stats\n\n\ndatasets = [generate_dataset(μ1, μ2, αhat, βhat, σhat, N=N) for i in range(K)]\nall_params = [smf.ols(formula='x ~ y + 1', data=df).fit() for df in datasets]\nαvec = np.array( [e.params['Intercept'] for e in all_params] )\nβvec = np.array( [e.params['y'] for e in all_params] )\n\n\ngkd = scipy.stats.kde.gaussian_kde(βvec)\n\n\nfor i in [1,2,3,4,5,6,7,8,9,10,100]:\n\n    fig = plt.figure(figsize=(10,14))\n    plt.subplot(311)\n    plot_distribution(0.1, 0.8, 0.2)\n    plt.grid()\n    plt.title(f\"True Distribution: $y = {αhat:.2f} + {βhat:.2f} x + {σhat:.2f} u$\")\n    plt.xlim(0,1)\n    plt.ylim(-0.5, 1.5)\n    \n    plt.subplot(312)\n    plt.xlim(0,1)\n    plt.ylim(-0.5, 1.5)\n    df = datasets[i]\n    if i&gt;=2:\n        plt.plot(df['x'], df['y'], 'o')\n    plt.title(\"Random Draw\")\n    plt.grid()\n\n    plt.subplot(313)\n    if i==3:\n        plt.plot(βvec[i], βvec[i]*0, 'o')\n    if i&gt;4:\n        plt.plot(βvec[3:i], βvec[3:i]*0, 'o')\n    if i&gt;10:\n        xx = np.linspace(0.2, 1.4, 10000)\n        plt.plot( βvec, gkd.pdf(βvec), '.')\n    plt.title(\"Distribution of β\")\n    plt.xlim(0.2, 1.4)\n    plt.ylim(-0.1, 4)\n    plt.grid()\n\n    plt.tight_layout()\n\n    plt.savefig(f\"random_estimates_{i}.png\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplt.plot( βvec, βvec*0, 'o')"
  },
  {
    "objectID": "session_9/graphs/Untitled1.html",
    "href": "session_9/graphs/Untitled1.html",
    "title": "Data-Based Economics",
    "section": "",
    "text": "from matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n\nimport statsmodels.api as sm\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nincome\neducation\nprestige\n\n\n\n\ncount\n45.000000\n45.000000\n45.000000\n\n\nmean\n41.866667\n52.555556\n47.688889\n\n\nstd\n24.435072\n29.760831\n31.510332\n\n\nmin\n7.000000\n7.000000\n3.000000\n\n\n25%\n21.000000\n26.000000\n16.000000\n\n\n50%\n42.000000\n45.000000\n41.000000\n\n\n75%\n64.000000\n84.000000\n81.000000\n\n\nmax\n81.000000\n100.000000\n97.000000\n\n\n\n\n\n\n\n\ndf.cov()\n\n\n\n\n\n\n\n\nincome\neducation\nprestige\n\n\n\n\nincome\n597.072727\n526.871212\n645.071212\n\n\neducation\n526.871212\n885.707071\n798.904040\n\n\nprestige\n645.071212\n798.904040\n992.901010\n\n\n\n\n\n\n\n\nfrom matplotlib import pyplot as plt\n\n\nplt.figure(figsize=(8,6))\nplt.plot(df['education'],df['income'],'o')\nplt.grid()\nplt.xlabel(\"x (Education)\")\nplt.ylabel(\"y (Income)\")\nplt.savefig(\"data_description.png\")\n\n\n\n\n\nfor i in [1,2,3]:\n    xvec = np.linspace(10,100)\n\n    plt.figure(figsize=(12,8))\n    plt.plot(df['education'],df['income'],'o')\n\n    plt.plot(xvec, xvec * 0 + 50)\n    if i&gt;=2:\n        plt.plot(xvec, xvec )\n    if i&gt;=3:\n        plt.plot(xvec,  90- 0.6*xvec )\n\n    plt.grid()\n    plt.xlabel(\"x (Education)\")\n    plt.ylabel(\"y (Income)\")\n    plt.savefig(f\"which_line_{i}.png\")\n\n\n\n\n\n\n\n\n\n\n\nfrom ipywidgets import interact\n\n\nimport matplotlib.patches as patches\n\n\na = 0.1\nb = 1.0\nind = 23\n\n\napprox =  a + b*xvec\n\n# Create figure and axes\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\nplt.plot(df['education'],df['income'],'o')\nplt.plot(xvec, approx, color='red')\n\nx, y = df['education'][ind], df['income'][ind]\nplt.plot(x, y, 'o', color='red' )\np = a+b*x\nplt.grid(True)\nh = abs(p-y)\nplt.vlines(x, y+h, y, color='red')\n\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.savefig(f\"error_0.png\")\n\n\n\n\n\nplt.vlines?\n\n\nSignature:\nplt.vlines(\n    x,\n    ymin,\n    ymax,\n    colors=None,\n    linestyles='solid',\n    label='',\n    *,\n    data=None,\n    **kwargs,\n)\nDocstring:\nPlot vertical lines.\nPlot vertical lines at each *x* from *ymin* to *ymax*.\nParameters\n----------\nx : float or array-like\n    x-indexes where to plot the lines.\nymin, ymax : float or array-like\n    Respective beginning and end of each line. If scalars are\n    provided, all lines will have same length.\ncolors : list of colors, default: :rc:`lines.color`\nlinestyles : {'solid', 'dashed', 'dashdot', 'dotted'}, optional\nlabel : str, default: ''\nReturns\n-------\n`~matplotlib.collections.LineCollection`\nOther Parameters\n----------------\n**kwargs : `~matplotlib.collections.LineCollection` properties.\nSee Also\n--------\nhlines : horizontal lines\naxvline: vertical line across the axes\nNotes\n-----\n.. note::\n    In addition to the above described arguments, this function can take\n    a *data* keyword argument. If such a *data* argument is given,\n    the following arguments can also be string ``s``, which is\n    interpreted as ``data[s]`` (unless this raises an exception):\n    *x*, *ymin*, *ymax*, *colors*.\n    Objects passed as **data** must support item access (``data[s]``) and\n    membership test (``s in data``).\nFile:      ~/.local/opt/miniconda/lib/python3.8/site-packages/matplotlib/pyplot.py\nType:      function\n\n\n\n\n\na = 0.1\nb = 1.0\nind = 23\n\n\napprox =  a + b*xvec\n\n# Create figure and axes\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\nplt.plot(df['education'],df['income'],'o')\nplt.plot(xvec, approx, color='red')\n\nx, y = df['education'][ind], df['income'][ind]\nplt.plot(x, y, 'o', color='red' )\np = a+b*x\nplt.grid(True)\nh = abs(p-y)\nif p-y&gt;0:\n    # Create a Rectangle patch\n    rect = patches.Rectangle((x,y),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n    ax.add_patch(rect)\n    \nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.savefig(f\"errors_{1}.png\")\n\n\n\n\n\ndef L(a,b):\n    Δ = a + b*df['education'] - df['income']\n    return (Δ**2).sum()\n\n\na = 0.1\nb = 0.8\n\napprox =  a + b*xvec\n\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\n\n\nplt.plot(df['education'],df['income'],'o', label=f\"L({a,b})={L(a,b)}\")\nplt.plot(xvec, approx, color='red')\n\nplt.grid(True)\nfor ind in range(df.shape[0]):\n    \n    x, y = df['education'][ind], df['income'][ind]\n    p = a+b*x\n\n    h = abs(p-y)\n    if p-y&gt;0:\n        # Create a Rectangle patch\n        rect = patches.Rectangle((x,y),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\n    else:\n        rect = patches.Rectangle((x,y-h),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.legend(loc='upper right')\nplt.savefig(f\"errors_2.png\")\n\n\n\n\n\na = 90\nb = -0.6\n\napprox =  a + b*xvec\n\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\n\n\nplt.plot(df['education'],df['income'],'o', label=f\"L({a,b})={L(a,b)}\")\nplt.plot(xvec, approx, color='red')\n\nplt.grid(True)\nfor ind in range(df.shape[0]):\n    \n    x, y = df['education'][ind], df['income'][ind]\n    p = a+b*x\n\n    h = abs(p-y)\n    if p-y&gt;0:\n        # Create a Rectangle patch\n        rect = patches.Rectangle((x,y),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\n    else:\n        rect = patches.Rectangle((x,y-h),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.legend(loc='upper right')\nplt.savefig(f\"errors_3.png\")\n\n\n\n\n\nimport scipy.optimize\n\n\nscipy.optimize.minimize(lambda x: L(x[0], x[1]),np.array([0.5, 0.5]))\n\n      fun: 12480.970174488397\n hess_inv: array([[ 7.14169839e-09, -3.91281920e-09],\n       [-3.91281920e-09,  2.46663613e-09]])\n      jac: array([0.00024414, 0.00012207])\n  message: 'Desired error not necessarily achieved due to precision loss.'\n     nfev: 57\n      nit: 7\n     njev: 19\n   status: 2\n  success: False\n        x: array([10.60350224,  0.59485938])\n\n\n\na = 10\nb = 0.59\n\napprox =  a + b*xvec\n\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\n\n\nplt.plot(df['education'],df['income'],'o', label=f\"L({a,b})={L(a,b)}\")\nplt.plot(xvec, approx, color='red')\n\nplt.grid(True)\nfor ind in range(df.shape[0]):\n    \n    x, y = df['education'][ind], df['income'][ind]\n    p = a+b*x\n\n    h = abs(p-y)\n    if p-y&gt;0:\n        # Create a Rectangle patch\n        rect = patches.Rectangle((x,y),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\n    else:\n        rect = patches.Rectangle((x,y-h),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.legend(loc='upper right')\nplt.savefig(f\"errors_4.png\")\n\n\n\n\n\na = 10\nb = 0.59\n\napprox =  a + b*xvec\n\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\n\n\nplt.plot(df['education'],df['income'],'o', label=f\"L({a,b})={L(a,b)}\")\nplt.plot(xvec, approx, color='red', alpha=0.5)\n\nplt.plot(60, a + b*60, 'o', color='red',)\n\nprint(a+b*60)\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.legend(loc='upper right')\nplt.savefig(f\"prediction.png\")\n\n45.4\n\n\n\n\n\n\na = 10\nb = 0.59\n\napprox =  (a + b*df['education'] - df['income'])\n\nplt.figure(figsize=(12,6))\n\nplt.subplot(121)\nplt.plot(approx)\nplt.grid(False)\nplt.title(\"Residuals\")\n\n\nplt.subplot(122)\ndistplot(approx)\nplt.title(\"Distribution of residuals\")\nplt.grid()\n\nplt.savefig(\"residuals.png\")\n\n/home/pablo/.local/opt/miniconda/lib/python3.8/site-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n  warnings.warn(msg, FutureWarning)\n\n\n\n\n\n\n(a + b*df['education'] - df['income']).std()\n\n16.842782676352154\n\n\n\n\n\n/home/pablo/.local/opt/miniconda/lib/python3.8/site-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n  warnings.warn(msg, FutureWarning)\n\n\n&lt;AxesSubplot:ylabel='Density'&gt;\n\n\n\n\n\n\nfrom scipy.stats import f\n\n\nf(0.3)\n\nTypeError: _parse_args() missing 1 required positional argument: 'dfd'\n\n\n\nnp.rand\n\n\nK = 100\nxvec = np.linspace(0,1,K)\ne1 = np.random.randn(K)*0.1\nyvec = 0.1 + xvec*0.4 + e1\ne2 = np.random.randn(K)*0.05\nyvec2 = 0.1 + xvec*(xvec-1)/2 + e2\ne3 = np.random.randn(K)*xvec/2\nyvec3 = 0.1 + xvec + e3\n\nyvec4 = 0.1 + np.sin(xvec*6) + np.random.randn(K)*xvec/2\n\n\nfrom dolo.numeric.processes import VAR1\n\n\nsim = VAR1( ρ=0.8, Σ=0.001).simulate(N=1,T=100)\nyvec4 = 0.1 + xvec*0.4 + sim.ravel()\n\n\nplt.figure(figsize=(18,6))\nplt.subplot(241)\nplt.plot(xvec, yvec,'o')\nplt.plot(xvec, 0.1 + xvec*0.4 )\nplt.ylabel(\"Series\")\nplt.title(\"white noise\")\nplt.subplot(242)\nplt.plot(xvec, yvec2, 'o')\nplt.plot(xvec, yvec2*0)\nplt.title('nonlinear')\nplt.subplot(243)\nplt.plot(xvec, yvec3,'o')\nplt.plot(xvec, 0.1 + xvec)\nplt.title('heteroskedastic')\nplt.subplot(244)\nplt.plot(xvec, yvec4,'o')\nplt.plot(xvec, xvec*0.6)\n\nplt.title('correlated')\n\n\nplt.subplot(245)\nplt.plot(xvec, e1,'o')\nplt.ylabel(\"Residuals\")\nplt.subplot(246)\nplt.plot(xvec, yvec2-0.075, 'o')\n\nplt.subplot(247)\nplt.plot(xvec, e3,'o')\nplt.subplot(248)\nplt.plot(xvec, sim.ravel(),'o')\n\nplt.tight_layout()\n\nplt.savefig(\"residuals_circus.png\")"
  },
  {
    "objectID": "session_9/slides.html#how-to-deal-with-text",
    "href": "session_9/slides.html#how-to-deal-with-text",
    "title": "Text Analysis",
    "section": "How to deal with text?",
    "text": "How to deal with text?\n\nRecall: big data contains heterogenous data\n\ntext / images / sound"
  },
  {
    "objectID": "session_9/slides.html#example-1-fomc-meetings",
    "href": "session_9/slides.html#example-1-fomc-meetings",
    "title": "Text Analysis",
    "section": "Example 1: FOMC meetings",
    "text": "Example 1: FOMC meetings\nTaking the Fed at its Word: A New Approach to Estimating Central Bank Objectives using Text Analysis by Adam H. Shapiro and Daniel J. Wilson link\n\nRemember the Taylor rule? We tried to estimate it from the data.\nGeneralized version: \\(i_t = \\alpha_\\pi (\\pi_t-\\pi^{\\star}) + \\alpha_y (y_t-y)\\)\nIs there a way to measure the preferences of the central bank? (coefficients and inflation target?)\nShapiro and Wilson: let’s look at the FOMC meeting transcripts\nExcerpts (there are tons of them: 704,499)\n\n\n\n\nI had several conversations at Jackson Hole with Wall Street economists and journalists, and they said, quite frankly, that they really do not believe that our effective inflation target is 1 to 2 percent. They believe we have morphed into 1+1/2 to 2+1/2 percent, and no one thought that we were really going to do anything over time to bring it down to 1 to 2.\n\nSep 2006 St. Louis Federal Reserve President William Poole\n\n\n\nLike most of you, I am not at all alarmist about inflation. I think the worst that is likely to happen would be 20 or 30 basis points over the next year. But even that amount is a little disconcerting for me. I think it is very important for us to maintain our credibility on inflation and it would be somewhat expensive to bring that additional inflation back down.\n\nMarch 2006 Chairman Ben Bernanke\n\n\n\nWith inflation remaining at such rates, we could begin to lose credibility if markets mistakenly inferred that our comfort zone had drifted higher. When we stop raising rates, we ought to be reasonably confident that policy is restrictive enough to bring inflation back toward the center of our comfort zone, which I believe is 1+1/2 percent…So for today, we should move forward with an increase of 25 basis points…\n\nJan 2006 Chicago Federal Reserve President Michael Moskow"
  },
  {
    "objectID": "session_9/slides.html#example-2",
    "href": "session_9/slides.html#example-2",
    "title": "Text Analysis",
    "section": "Example 2",
    "text": "Example 2\n\n\nSuppose you work in the trading floor of a financial instutition\nThese kind of tweets have disturbing impact on the markets. You need to react quickly.\nYou need a machine to assess the risk in real time.\nMore generally, tweeter is a quite unique source of real-time data\nHow do you analyse the content of the tweets?\nComment: actually it’s not only the content of the tweets, but who reads, who retweets: graph analysis"
  },
  {
    "objectID": "session_9/slides.html#text-mining-what-can-we-extract-from-texts",
    "href": "session_9/slides.html#text-mining-what-can-we-extract-from-texts",
    "title": "Text Analysis",
    "section": "Text-mining: what can we extract from texts",
    "text": "Text-mining: what can we extract from texts\n\nThe main branches of text analysis are:\n\nsentiment analysis\n\nassociate positivity/negativity to a text\nprecise meaning of “sentiment” is context dependent\n\n\ntopic modeling\n\nclassify texts as belonging to known categories (supervised)\nfinding likely texts (unsupervised)\n\nnamed-entity recognition\n\nfind who gets mentioned in the text\nexample: A Cross-verified Database of Notable People, 3500BC-2018AD\n\nevent-extraction\n\nrecognize mention of events\n\nplus everything that can be done with a language model like GPT-3…"
  },
  {
    "objectID": "session_9/slides.html#clarification",
    "href": "session_9/slides.html#clarification",
    "title": "Text Analysis",
    "section": "Clarification",
    "text": "Clarification\n\n\nText analysis / text mining are somewhat used interchangeably\nIn general they consist in quantifying information used in a text…\n… so that it can be incorporated in machine learning analysis\nRecently, deep learning (and GPT-3) has changed this state of facts:\n\nsome models get trained direcly on text (intermediary phases are not explicited)"
  },
  {
    "objectID": "session_9/slides.html#the-even-less-glamorous-part",
    "href": "session_9/slides.html#the-even-less-glamorous-part",
    "title": "Text Analysis",
    "section": "The even-less glamorous part",
    "text": "The even-less glamorous part\n\n\nbefore getting started with text analysis, one needs to get hold of the text in the first place\n\nhow to extract\n\nwebscraping: automate a bot to visit website and download text\ndocument extraction: for instance extract the text from pdf docs, get rid of everything irrelevant\n\n\nhow to store it\n\nwhat kind of database?\nimportant problem when database is big"
  },
  {
    "objectID": "session_9/slides.html#processing-steps",
    "href": "session_9/slides.html#processing-steps",
    "title": "Text Analysis",
    "section": "Processing steps",
    "text": "Processing steps\n\nLet’s briefly see how text gets processed.\nGoal is to transform the text into a numerical vector of features\n\nStupid approach: “abc”-&gt;[1,2,3]\nwe need to capture some form of language structure\n\nAll the steps can be done fairly easily with nltk\n\nnltk is comparable to sklearn in terms of widespread adoption"
  },
  {
    "objectID": "session_9/slides.html#processing-steps-2",
    "href": "session_9/slides.html#processing-steps-2",
    "title": "Text Analysis",
    "section": "Processing steps (2)",
    "text": "Processing steps (2)\n\nSteps:\n\ntokenization\nstopwords\nlexicon normalization\n\nstemming\nlemmatization\n\nPOS tagging"
  },
  {
    "objectID": "session_9/slides.html#tokenization",
    "href": "session_9/slides.html#tokenization",
    "title": "Text Analysis",
    "section": "Tokenization",
    "text": "Tokenization\n\n\n\nTokenization: split input into atomic elements.\n\nWe can recognize sentences.\n\nOr words.\n\nIt is enough for some basic analysis:\n\n\nfrom nltk.probability import FreqDist\nfdist = FreqDist(words)\nprint(fdist.most_common(2))\n[('It', 1), (\"'s\", 1)]\n\n\n\n\nfrom nltk.tokenize import sent_tokenize\ntxt = \"\"\"Animal Farm is a short novel by George Orwell. It was\nwritten during World War II and published in 1945. It is about \na group of farm animals who rebel against their farmer. They \nhope to create a place where the animals can be equal, free,\n and happy.\"\"\"\nsentences  = sent_tokenize(txt)\nprint(sentences)\n\n\n['Animal Farm is a short novel by George Orwell.',\n 'It was\\nwritten during World War II and published in 1945.', \n 'It is about \\na group of farm animals who rebel against their farmer.', \n 'They \\nhope to create a place where the animals can be equal, free,\\n and happy.']\n\n\nfrom nltk.tokenize import word_tokenize\ntxt = \"It's a beautiful thing, the destruction of words.\"\nwords  = word_tokenize(txt)\nprint(words)\n['It', \"'s\", 'a', 'beautiful', 'thing', ',', 'the', 'destruction', 'of', 'words', '.']"
  },
  {
    "objectID": "session_9/slides.html#part-of-speech-tagging",
    "href": "session_9/slides.html#part-of-speech-tagging",
    "title": "Text Analysis",
    "section": "Part-of speech tagging",
    "text": "Part-of speech tagging\n\n\n\nSometimes we need information about the kind of tokens that we have\n\nWe can perform part-of-speech tagging (aka grammatical tagging)\n\nThis is useful to refine interpretation of some words\n\n“it’s not a beautiful thing”\nvs “it’s a beautiful thing”\nconnotation of beautiful changes\n\n\n\n\nfrom nltk.tokenize import word_tokenize\ntagged = nltk.pos_tag(words)\ntagged\n[('It', 'PRP'),\n (\"'s\", 'VBZ'),\n ('a', 'DT'),\n ('beautiful', 'JJ'),\n ('thing', 'NN'),\n (',', ','),\n ('the', 'DT'),\n ('destruction', 'NN'),\n ('of', 'IN'),\n ('words', 'NNS'),\n ('.', '.')]"
  },
  {
    "objectID": "session_9/slides.html#simplifying-the-text-1-stopwords",
    "href": "session_9/slides.html#simplifying-the-text-1-stopwords",
    "title": "Text Analysis",
    "section": "Simplifying the text (1): stopwords",
    "text": "Simplifying the text (1): stopwords\n\n\n\nSome words are very frequent and carry no useful meaning\n\n\nThey are called stopwords\n\n\nWe typically remove them from our word list\n\n\n\n\nfrom nltk.corpus import stopwords\nstop_words=set(stopwords.words(\"english\"))\nprint(stop_words)\n{'their', 'then', 'not', 'ma', 'here', ...}\n\n\n\nfiltered_words = [w for w in words if w not in stop_words]\nfiltered_words\n['beautiful', 'thing' 'destruction', 'words']"
  },
  {
    "objectID": "session_9/slides.html#simplifying-the-text-2-lexicon-normalization",
    "href": "session_9/slides.html#simplifying-the-text-2-lexicon-normalization",
    "title": "Text Analysis",
    "section": "Simplifying the text (2): lexicon normalization",
    "text": "Simplifying the text (2): lexicon normalization\n\n\n\nSometimes, there are several variants of a given word\n\ntight, tightening, tighten\n\n\nStemming: keeping the word root\n\nLemmatization: keeps the word base\n\nlinguistically correct contrary to stemming\n\n\n\n\nfrom nltk.stem import PorterStemmer\nps = PorterStemmer()\n\nwords =  [\"tight\", \"tightening\", \"tighten\"]\nstemmed_words=[ps.stem(w) for w in words]\n['tight', 'tighten', 'tighten']\n\n\nfrom nltk.stem.wordnet import WordNetLemmatizer\nlem = WordNetLemmatizer()\n\nwords =  [\"flying\", \"flyers\", \"fly\"]\nstemmed_words=[ps.stem(w) for w in words]\nlemmatized_words=[lem.lemmatize(w) for w in words]\n# lemmatized\n['flying', 'flyer', 'fly']\n# stemmed\n['fli', 'flyer', 'fli']"
  },
  {
    "objectID": "session_9/slides.html#sentiment-analysis-1",
    "href": "session_9/slides.html#sentiment-analysis-1",
    "title": "Text Analysis",
    "section": "Sentiment analysis",
    "text": "Sentiment analysis\n\nWhat do we do now that we have reduced a text to a series of word occurrences?\nTwo main approaches:\n\nlexical analysis\nmachine learning"
  },
  {
    "objectID": "session_9/slides.html#lexical-analysis",
    "href": "session_9/slides.html#lexical-analysis",
    "title": "Text Analysis",
    "section": "Lexical analysis",
    "text": "Lexical analysis\n\nUse a “sentiment dictionary” to provide a value (positive or negative) for each word\n\nsum the weights to get positive or negative sentiment\n\n\nExample: \\[\\underbrace{\\text{Sadly}}_{-}\\text{, there wasn't a glimpse of }\\underbrace{\\text{light}}_{+} \\text{ in his } \\text{world } \\text{ of intense }\\underbrace{\\text{suffering.}}_{-}\\]\n\nTotal:\n\n-1+1-1. Sentiment is negative.\n\n\nProblems:\n\nhere, taking grammar into account would change everything\ndoesn’t capture irony\nour dictionary doesn’t have weights for what matters to us \\[ \\text{the central bank forecasts increased }\\underbrace{\\text{inflation}}_{?}\\]"
  },
  {
    "objectID": "session_9/slides.html#machine-learning",
    "href": "session_9/slides.html#machine-learning",
    "title": "Text Analysis",
    "section": "Machine learning",
    "text": "Machine learning\n\nIdea: we would like the weights to be endogenously determined \\[ \\underbrace{\\text{the}}_{x_1} \\underbrace{\\text{ central}}_{x_2} \\underbrace{\\text{ bank}}_{x_3} \\underbrace{\\text{ forecasts}}_{x_4} \\underbrace{\\text{ increased} }_{x_5} \\underbrace{\\text{ inflation}}_{x_6}\\]\nSuppose we had several texts: we can generate features by counting words in each of them\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nthe\ncentral\nbank\nforecasts\nincreased\ninflation\neconomy\nexchange rate\ncrisis\nsentiment\n\n\n\n\ntext1\n1\n1\n2\n1\n1\n2\n\n\n\n-1\n\n\ntext2\n3\n\n\n\n\n1\n1\n2\n\n+1\n\n\ntext3\n4\n\n1\n\n\n1\n\n1\n1\n-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can the train the model: \\(y = x_1 f(w_1) + \\cdots x_K f(w_K)\\) where \\(y\\) is the sentiment and \\(w_i\\) is wordcount of word \\(w_i\\)\n\nof course, we need a similar procedure as before (split the training set and evaluation set, …)\nwe can use any model (like naive bayesian updating)\n\nThis approach is called Bag of Words (BOW)"
  },
  {
    "objectID": "session_9/slides.html#some-issues",
    "href": "session_9/slides.html#some-issues",
    "title": "Text Analysis",
    "section": "Some issues",
    "text": "Some issues\n\nBag of words has a few pitfalls:\n\nit requires a big training set with labels\nit overweights long documents\nthere is noise due to the very frequent words that don’t affect sentiment\n\nImprovement: TF-IDF\n\nstands for Term-Frequency*Inverse-Distribution-Frequency\nreplace word frequency \\(f(w)\\) by \\[\\text{tf-idf} = f(w)\\frac{\\text{number of documents}}{\\text{number of documents containing $w$}}\\]"
  },
  {
    "objectID": "session_9/index.html",
    "href": "session_9/index.html",
    "title": "Text Analysis",
    "section": "",
    "text": "Recall: big data contains heterogenous data\n\ntext / images / sound\n\n\n\n\n\n\nTaking the Fed at its Word: A New Approach to Estimating Central Bank Objectives using Text Analysis by Adam H. Shapiro and Daniel J. Wilson link\n\nRemember the Taylor rule? We tried to estimate it from the data.\nGeneralized version: \\(i_t = \\alpha_\\pi (\\pi_t-\\pi^{\\star}) + \\alpha_y (y_t-y)\\)\nIs there a way to measure the preferences of the central bank? (coefficients and inflation target?)\nShapiro and Wilson: let’s look at the FOMC meeting transcripts\nExcerpts (there are tons of them: 704,499)\n\n\n\n\nI had several conversations at Jackson Hole with Wall Street economists and journalists, and they said, quite frankly, that they really do not believe that our effective inflation target is 1 to 2 percent. They believe we have morphed into 1+1/2 to 2+1/2 percent, and no one thought that we were really going to do anything over time to bring it down to 1 to 2.\n\nSep 2006 St. Louis Federal Reserve President William Poole\n\n\n\nLike most of you, I am not at all alarmist about inflation. I think the worst that is likely to happen would be 20 or 30 basis points over the next year. But even that amount is a little disconcerting for me. I think it is very important for us to maintain our credibility on inflation and it would be somewhat expensive to bring that additional inflation back down.\n\nMarch 2006 Chairman Ben Bernanke\n\n\n\nWith inflation remaining at such rates, we could begin to lose credibility if markets mistakenly inferred that our comfort zone had drifted higher. When we stop raising rates, we ought to be reasonably confident that policy is restrictive enough to bring inflation back toward the center of our comfort zone, which I believe is 1+1/2 percent…So for today, we should move forward with an increase of 25 basis points…\n\nJan 2006 Chicago Federal Reserve President Michael Moskow\n\n\n\n\n\n\n\nSuppose you work in the trading floor of a financial instutition\nThese kind of tweets have disturbing impact on the markets. You need to react quickly.\nYou need a machine to assess the risk in real time.\nMore generally, tweeter is a quite unique source of real-time data\nHow do you analyse the content of the tweets?\nComment: actually it’s not only the content of the tweets, but who reads, who retweets: graph analysis\n\n\n\n\n\nThe main branches of text analysis are:\n\nsentiment analysis\n\nassociate positivity/negativity to a text\nprecise meaning of “sentiment” is context dependent\n\n\ntopic modeling\n\nclassify texts as belonging to known categories (supervised)\nfinding likely texts (unsupervised)\n\nnamed-entity recognition\n\nfind who gets mentioned in the text\nexample: A Cross-verified Database of Notable People, 3500BC-2018AD\n\nevent-extraction\n\nrecognize mention of events\n\nplus everything that can be done with a language model like GPT-3…\n\n\n\n\n\n\n\nText analysis / text mining are somewhat used interchangeably\nIn general they consist in quantifying information used in a text…\n… so that it can be incorporated in machine learning analysis\nRecently, deep learning (and GPT-3) has changed this state of facts:\n\nsome models get trained direcly on text (intermediary phases are not explicited)\n\n\n\n\n\n\n\nbefore getting started with text analysis, one needs to get hold of the text in the first place\n\nhow to extract\n\nwebscraping: automate a bot to visit website and download text\ndocument extraction: for instance extract the text from pdf docs, get rid of everything irrelevant\n\n\nhow to store it\n\nwhat kind of database?\nimportant problem when database is big"
  },
  {
    "objectID": "session_9/index.html#how-to-deal-with-text",
    "href": "session_9/index.html#how-to-deal-with-text",
    "title": "Text Analysis",
    "section": "",
    "text": "Recall: big data contains heterogenous data\n\ntext / images / sound"
  },
  {
    "objectID": "session_9/index.html#example-1-fomc-meetings",
    "href": "session_9/index.html#example-1-fomc-meetings",
    "title": "Text Analysis",
    "section": "",
    "text": "Taking the Fed at its Word: A New Approach to Estimating Central Bank Objectives using Text Analysis by Adam H. Shapiro and Daniel J. Wilson link\n\nRemember the Taylor rule? We tried to estimate it from the data.\nGeneralized version: \\(i_t = \\alpha_\\pi (\\pi_t-\\pi^{\\star}) + \\alpha_y (y_t-y)\\)\nIs there a way to measure the preferences of the central bank? (coefficients and inflation target?)\nShapiro and Wilson: let’s look at the FOMC meeting transcripts\nExcerpts (there are tons of them: 704,499)\n\n\n\n\nI had several conversations at Jackson Hole with Wall Street economists and journalists, and they said, quite frankly, that they really do not believe that our effective inflation target is 1 to 2 percent. They believe we have morphed into 1+1/2 to 2+1/2 percent, and no one thought that we were really going to do anything over time to bring it down to 1 to 2.\n\nSep 2006 St. Louis Federal Reserve President William Poole\n\n\n\nLike most of you, I am not at all alarmist about inflation. I think the worst that is likely to happen would be 20 or 30 basis points over the next year. But even that amount is a little disconcerting for me. I think it is very important for us to maintain our credibility on inflation and it would be somewhat expensive to bring that additional inflation back down.\n\nMarch 2006 Chairman Ben Bernanke\n\n\n\nWith inflation remaining at such rates, we could begin to lose credibility if markets mistakenly inferred that our comfort zone had drifted higher. When we stop raising rates, we ought to be reasonably confident that policy is restrictive enough to bring inflation back toward the center of our comfort zone, which I believe is 1+1/2 percent…So for today, we should move forward with an increase of 25 basis points…\n\nJan 2006 Chicago Federal Reserve President Michael Moskow"
  },
  {
    "objectID": "session_9/index.html#example-2",
    "href": "session_9/index.html#example-2",
    "title": "Text Analysis",
    "section": "",
    "text": "Suppose you work in the trading floor of a financial instutition\nThese kind of tweets have disturbing impact on the markets. You need to react quickly.\nYou need a machine to assess the risk in real time.\nMore generally, tweeter is a quite unique source of real-time data\nHow do you analyse the content of the tweets?\nComment: actually it’s not only the content of the tweets, but who reads, who retweets: graph analysis"
  },
  {
    "objectID": "session_9/index.html#text-mining-what-can-we-extract-from-texts",
    "href": "session_9/index.html#text-mining-what-can-we-extract-from-texts",
    "title": "Text Analysis",
    "section": "",
    "text": "The main branches of text analysis are:\n\nsentiment analysis\n\nassociate positivity/negativity to a text\nprecise meaning of “sentiment” is context dependent\n\n\ntopic modeling\n\nclassify texts as belonging to known categories (supervised)\nfinding likely texts (unsupervised)\n\nnamed-entity recognition\n\nfind who gets mentioned in the text\nexample: A Cross-verified Database of Notable People, 3500BC-2018AD\n\nevent-extraction\n\nrecognize mention of events\n\nplus everything that can be done with a language model like GPT-3…"
  },
  {
    "objectID": "session_9/index.html#clarification",
    "href": "session_9/index.html#clarification",
    "title": "Text Analysis",
    "section": "",
    "text": "Text analysis / text mining are somewhat used interchangeably\nIn general they consist in quantifying information used in a text…\n… so that it can be incorporated in machine learning analysis\nRecently, deep learning (and GPT-3) has changed this state of facts:\n\nsome models get trained direcly on text (intermediary phases are not explicited)"
  },
  {
    "objectID": "session_9/index.html#the-even-less-glamorous-part",
    "href": "session_9/index.html#the-even-less-glamorous-part",
    "title": "Text Analysis",
    "section": "",
    "text": "before getting started with text analysis, one needs to get hold of the text in the first place\n\nhow to extract\n\nwebscraping: automate a bot to visit website and download text\ndocument extraction: for instance extract the text from pdf docs, get rid of everything irrelevant\n\n\nhow to store it\n\nwhat kind of database?\nimportant problem when database is big"
  },
  {
    "objectID": "session_9/index.html#processing-steps",
    "href": "session_9/index.html#processing-steps",
    "title": "Text Analysis",
    "section": "Processing steps",
    "text": "Processing steps\n\nLet’s briefly see how text gets processed.\nGoal is to transform the text into a numerical vector of features\n\nStupid approach: “abc”-&gt;[1,2,3]\nwe need to capture some form of language structure\n\nAll the steps can be done fairly easily with nltk\n\nnltk is comparable to sklearn in terms of widespread adoption"
  },
  {
    "objectID": "session_9/index.html#processing-steps-2",
    "href": "session_9/index.html#processing-steps-2",
    "title": "Text Analysis",
    "section": "Processing steps (2)",
    "text": "Processing steps (2)\n\nSteps:\n\ntokenization\nstopwords\nlexicon normalization\n\nstemming\nlemmatization\n\nPOS tagging"
  },
  {
    "objectID": "session_9/index.html#tokenization",
    "href": "session_9/index.html#tokenization",
    "title": "Text Analysis",
    "section": "Tokenization",
    "text": "Tokenization\n\n\n\nTokenization: split input into atomic elements.\n\nWe can recognize sentences.\n\nOr words.\n\nIt is enough for some basic analysis:\n\n\nfrom nltk.probability import FreqDist\nfdist = FreqDist(words)\nprint(fdist.most_common(2))\n[('It', 1), (\"'s\", 1)]\n\n\n\n\nfrom nltk.tokenize import sent_tokenize\ntxt = \"\"\"Animal Farm is a short novel by George Orwell. It was\nwritten during World War II and published in 1945. It is about \na group of farm animals who rebel against their farmer. They \nhope to create a place where the animals can be equal, free,\n and happy.\"\"\"\nsentences  = sent_tokenize(txt)\nprint(sentences)\n\n\n['Animal Farm is a short novel by George Orwell.',\n 'It was\\nwritten during World War II and published in 1945.', \n 'It is about \\na group of farm animals who rebel against their farmer.', \n 'They \\nhope to create a place where the animals can be equal, free,\\n and happy.']\n\n\nfrom nltk.tokenize import word_tokenize\ntxt = \"It's a beautiful thing, the destruction of words.\"\nwords  = word_tokenize(txt)\nprint(words)\n['It', \"'s\", 'a', 'beautiful', 'thing', ',', 'the', 'destruction', 'of', 'words', '.']"
  },
  {
    "objectID": "session_9/index.html#part-of-speech-tagging",
    "href": "session_9/index.html#part-of-speech-tagging",
    "title": "Text Analysis",
    "section": "Part-of speech tagging",
    "text": "Part-of speech tagging\n\n\n\nSometimes we need information about the kind of tokens that we have\n\nWe can perform part-of-speech tagging (aka grammatical tagging)\n\nThis is useful to refine interpretation of some words\n\n“it’s not a beautiful thing”\nvs “it’s a beautiful thing”\nconnotation of beautiful changes\n\n\n\n\nfrom nltk.tokenize import word_tokenize\ntagged = nltk.pos_tag(words)\ntagged\n[('It', 'PRP'),\n (\"'s\", 'VBZ'),\n ('a', 'DT'),\n ('beautiful', 'JJ'),\n ('thing', 'NN'),\n (',', ','),\n ('the', 'DT'),\n ('destruction', 'NN'),\n ('of', 'IN'),\n ('words', 'NNS'),\n ('.', '.')]"
  },
  {
    "objectID": "session_9/index.html#simplifying-the-text-1-stopwords",
    "href": "session_9/index.html#simplifying-the-text-1-stopwords",
    "title": "Text Analysis",
    "section": "Simplifying the text (1): stopwords",
    "text": "Simplifying the text (1): stopwords\n\n\n\nSome words are very frequent and carry no useful meaning\n\n\nThey are called stopwords\n\n\nWe typically remove them from our word list\n\n\n\n\nfrom nltk.corpus import stopwords\nstop_words=set(stopwords.words(\"english\"))\nprint(stop_words)\n{'their', 'then', 'not', 'ma', 'here', ...}\n\n\n\nfiltered_words = [w for w in words if w not in stop_words]\nfiltered_words\n['beautiful', 'thing' 'destruction', 'words']"
  },
  {
    "objectID": "session_9/index.html#simplifying-the-text-2-lexicon-normalization",
    "href": "session_9/index.html#simplifying-the-text-2-lexicon-normalization",
    "title": "Text Analysis",
    "section": "Simplifying the text (2): lexicon normalization",
    "text": "Simplifying the text (2): lexicon normalization\n\n\n\nSometimes, there are several variants of a given word\n\ntight, tightening, tighten\n\n\nStemming: keeping the word root\n\nLemmatization: keeps the word base\n\nlinguistically correct contrary to stemming\n\n\n\n\nfrom nltk.stem import PorterStemmer\nps = PorterStemmer()\n\nwords =  [\"tight\", \"tightening\", \"tighten\"]\nstemmed_words=[ps.stem(w) for w in words]\n['tight', 'tighten', 'tighten']\n\n\nfrom nltk.stem.wordnet import WordNetLemmatizer\nlem = WordNetLemmatizer()\n\nwords =  [\"flying\", \"flyers\", \"fly\"]\nstemmed_words=[ps.stem(w) for w in words]\nlemmatized_words=[lem.lemmatize(w) for w in words]\n# lemmatized\n['flying', 'flyer', 'fly']\n# stemmed\n['fli', 'flyer', 'fli']"
  },
  {
    "objectID": "session_9/index.html#sentiment-analysis-1",
    "href": "session_9/index.html#sentiment-analysis-1",
    "title": "Text Analysis",
    "section": "Sentiment analysis",
    "text": "Sentiment analysis\n\nWhat do we do now that we have reduced a text to a series of word occurrences?\nTwo main approaches:\n\nlexical analysis\nmachine learning"
  },
  {
    "objectID": "session_9/index.html#lexical-analysis",
    "href": "session_9/index.html#lexical-analysis",
    "title": "Text Analysis",
    "section": "Lexical analysis",
    "text": "Lexical analysis\n\nUse a “sentiment dictionary” to provide a value (positive or negative) for each word\n\nsum the weights to get positive or negative sentiment\n\n\nExample: \\[\\underbrace{\\text{Sadly}}_{-}\\text{, there wasn't a glimpse of }\\underbrace{\\text{light}}_{+} \\text{ in his } \\text{world } \\text{ of intense }\\underbrace{\\text{suffering.}}_{-}\\]\n\nTotal:\n\n-1+1-1. Sentiment is negative.\n\n\nProblems:\n\nhere, taking grammar into account would change everything\ndoesn’t capture irony\nour dictionary doesn’t have weights for what matters to us \\[ \\text{the central bank forecasts increased }\\underbrace{\\text{inflation}}_{?}\\]"
  },
  {
    "objectID": "session_9/index.html#machine-learning",
    "href": "session_9/index.html#machine-learning",
    "title": "Text Analysis",
    "section": "Machine learning",
    "text": "Machine learning\n\nIdea: we would like the weights to be endogenously determined \\[ \\underbrace{\\text{the}}_{x_1} \\underbrace{\\text{ central}}_{x_2} \\underbrace{\\text{ bank}}_{x_3} \\underbrace{\\text{ forecasts}}_{x_4} \\underbrace{\\text{ increased} }_{x_5} \\underbrace{\\text{ inflation}}_{x_6}\\]\nSuppose we had several texts: we can generate features by counting words in each of them\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nthe\ncentral\nbank\nforecasts\nincreased\ninflation\neconomy\nexchange rate\ncrisis\nsentiment\n\n\n\n\ntext1\n1\n1\n2\n1\n1\n2\n\n\n\n-1\n\n\ntext2\n3\n\n\n\n\n1\n1\n2\n\n+1\n\n\ntext3\n4\n\n1\n\n\n1\n\n1\n1\n-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can the train the model: \\(y = x_1 f(w_1) + \\cdots x_K f(w_K)\\) where \\(y\\) is the sentiment and \\(w_i\\) is wordcount of word \\(w_i\\)\n\nof course, we need a similar procedure as before (split the training set and evaluation set, …)\nwe can use any model (like naive bayesian updating)\n\nThis approach is called Bag of Words (BOW)"
  },
  {
    "objectID": "session_9/index.html#some-issues",
    "href": "session_9/index.html#some-issues",
    "title": "Text Analysis",
    "section": "Some issues",
    "text": "Some issues\n\nBag of words has a few pitfalls:\n\nit requires a big training set with labels\nit overweights long documents\nthere is noise due to the very frequent words that don’t affect sentiment\n\nImprovement: TF-IDF\n\nstands for Term-Frequency*Inverse-Distribution-Frequency\nreplace word frequency \\(f(w)\\) by \\[\\text{tf-idf} = f(w)\\frac{\\text{number of documents}}{\\text{number of documents containing $w$}}\\]"
  },
  {
    "objectID": "session_9/index_archive.html#classification",
    "href": "session_9/index_archive.html#classification",
    "title": "Introduction to Machine Learning (2)",
    "section": "Classification",
    "text": "Classification"
  },
  {
    "objectID": "session_9/index_archive.html#classification-problems",
    "href": "session_9/index_archive.html#classification-problems",
    "title": "Introduction to Machine Learning (2)",
    "section": "Classification Problems",
    "text": "Classification Problems"
  },
  {
    "objectID": "session_9/index_archive.html#other-classifiers",
    "href": "session_9/index_archive.html#other-classifiers",
    "title": "Introduction to Machine Learning (2)",
    "section": "Other Classifiers",
    "text": "Other Classifiers"
  },
  {
    "objectID": "session_9/index_archive.html#validation",
    "href": "session_9/index_archive.html#validation",
    "title": "Introduction to Machine Learning (2)",
    "section": "Validation",
    "text": "Validation"
  },
  {
    "objectID": "session_never/index.html",
    "href": "session_never/index.html",
    "title": "Panels",
    "section": "",
    "text": "Until now, we have been rather loose about where the data comes from:\nTrying to explain \\(N\\) observations: \\(y_n = a + b x_n, n\\in [1,N]\\)\nAll these lonely observations, where do they all come from?\n\n\n\nIndividuals: cross section\n\n\n\nDates: time-series\n\n\n\n\n\n\n\n\n\nvery simple study: structural break\n\ndoes regression on \\([T_1, \\overline{T}]\\) yield (significantly) different results on \\([\\overline{T}, T_2]\\)\n\ngoing further: time series analysis\n\ndata is typically autocorrelated\nexample (AR1) \\(x_t = a + b x_{t-1} + \\epsilon_t\\)\n\n\n\n\n\n\n\n\n\n\n\nRepeated cross sectional studies\nIndex individual by \\(i\\in [1,J]\\) and time by \\(t\\in[1,T]\\) [graph repeated cross-sectional]\nwe can study evolution of the regression over time \\[\\text{date 1}: y_{i,1} = a_1 + b_1 x_{1,1}\\] … \\[\\text{date t}: y_{i,t} = a_i + b_i x_{i,1}\\] … \\[\\text{date T}: y_{i,T} = a_T + b_T x_{i,T}\\]\n\\(y_{i,t}\\) and \\(y_{i,t+1}\\) are unrelated\nOne big regression: pooled regression.\n\n\n\n\nEven better: longitudinal data\n\n[graph longitudinal data]\n\nThe same individuals are followed over time.\nWe can study the evolution of individuals while taking into account their heterogeneity\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\nBalanced: all individuals in the sample are followed from 1 to T\nUnbalanced: some indivuduals start later, stop earlier\n\n\n\n\n\n\nCrude solutions:\n\ntruncate the dates between \\([T_1, T_2]\\) so that dataset is balanced\neliminate individuals who are not present in the full sample\n\n\nNot very good:\n\ncan limit a lot the size of the sample\ncan induce a “selection bias”\n\n\nReal gurus know how to deal with missing values\n\nmany algorithms can be adapted\n\n\n\n\n\n\n\n\n\nwe tend to prefer here the long format (w.r.t id and date)\nthere can be many columns though (for each variable)\n\n\n\n\n\n\nmicro-panel: \\(T&lt;&lt;N\\)\n\nPanel Study of Income Dynamics (PSID): 5000 consumers since 1967 (US)\n\nreinterview same individuals from year to year\nbut some go in/out of the panel\n\nSurvey of Consumer and Finance (SCF)\n…\n\nmacro-panel: \\(T\\approx N\\)\n\nWIIW: 23 countries since the 60s (central, east and southern Europe)\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nSuppose we want to explain the growth rate of several countries\n\nLet’s do a big regression (pooled regression) \\[y_{i,t} = a + b x_{i,t}\\] (pooled regression)\n\nWhat was missing?\n\nThere must be some effects, not captured by the regression, specific to each individual\n-&gt; Unobserved heterogeneity\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncapture idiosyncratic variability by adding an individual specific constant \\[ y_{i,t} = a + a_i +  b x_{i,t} + \\epsilon_{i,t}\\] \\(a_i\\) is called a fixed effect\n\nWe assume some structure on the data to discipline the regression.\n\nHow can we estimate the model? i.e. find plausible values for \\(a\\), \\(a_i\\), \\(b\\), \\(\\sigma(\\epsilon)\\), etc.\n\nWe focus on OLS estimator: \\[\\min_{a,a_i, b} \\sum_{i,t} ( y_{i,t} - \\underbrace{a + a_i +  b x_{i,t}}\\_{\\text{predicted value}} )^2\\]\n\n\n\n\n\n\n\n\nFixed effect regression: \\[ y_{i,t} = a + a_i +  b x_{i,t} + \\epsilon_{i,t}\\] is equivalent to \\[ y_{i,t} = a.1 + a_1 d\\_{i=1} + \\cdots + a_I d_{i=I} +  b x_{i,t} + \\epsilon_{i,t}\\] where \\(d\\) is a dummy variable such that \\(d_{i=j} = \\begin{cases}1, & \\text{if}\\ i=j \\\\\\\\ 0, & \\text{otherwise}\\end{cases}\\)\nMinor problem: \\(1, d_{i=1}, ... d_{i=I}\\) are not independent: \\(\\sum_{j=1}^I \\delta_{i=j}=1\\)\n\nSolution: ignore one of them, exactly like the dummies for categorical variables\n\nNow the regression can be estimated with OLS…\n\n\n\n\n\n\n… Now the regression can be estimated with OLS (or other)\n\nnaive approach fails for big panels (lots of dummy regressors makes \\(X^{\\prime}X\\) hard to invert)\nsmart approach decomposes computation in several steps\n\n“between” and “within” estimator (for advanced panel course)\n\nsoftware does it for us…\n\nLike always, we get estimates and significance numbers / confidence intervals\n\n\n\n\n\n\nSometimes, we know the whole dataset is affected by common time-varying shocks\n\nassume there isn’t a variable we can use to capture them (unobservable shocks)\n\nWe can use time-fixed effects to capture them: \\[ y_{i,t} = a + a_t +  b x_{i,t}\\]\nAnalysis is very similar to individual fixed effects\n\n\n\n\n\n\nWe can capture time heterogeneity and individual heterogeneity at the same time. \\[ y_{i,t} = a + a_i + a_t +  b x_{i,t}\\]\nMore of it soon.\n\n\n\n\n\n\nProblem with fixed effect model:\n\neach individual has a unique fixed effect\nit is impossible to predict it from other characteristics\n\n… and to compare an individual’s fixed effect to the predicted value\n\n\nSolution:\n\ninstead of assuming that specific effect is completely free, constrain it to follow a distribution: \\[y_{i,t} = \\alpha + \\beta x_{i,t} + \\epsilon_{i,t}\\] \\[\\epsilon_{i,t} = \\epsilon_i + \\epsilon_t + \\epsilon\\]\nwhere \\(\\epsilon_{i}\\), \\(\\epsilon_t\\) and \\(\\epsilon\\) are random variables with usual normality assumptions\n\n\n\n\n\n\n\nComposed coefficients: (coefficients can also be heterogenous in both dimension)\n\\[y_{i,t} = \\alpha_i + \\alpha_t + (\\beta_i + \\beta_t) x_{i,t} + \\epsilon_{i,t}\\]\nRandom coefficients …\n\n\n\n\n\n\n\n\n\n\n\n\n\nSetup:\n\nPopulation \\(i\\in[1,I]\\)\nWe follow the whole population at two successive dates \\(t_1\\) and \\(t_2\\).\nWe measure outcome \\(y\\)\nIndividuals in group \\(\\mathcal{G}_T\\) receive treatment \\(T\\)\nHow do we measure the effect of the treatment?\n\nExamples\n\ntutoring at school (afternoon lessons)\n\n\\(y\\): final grade\ndates: beginning (\\(t_1\\)), end of year (\\(t_2\\))\n\\(T_{i}\\) whether subject received afternoon sessions.\n\n\n\n\n\nGaphical summary:\n\n\n\n\n\n\n\nWhat we want:\n\nAverage change over time in the outcome variable for the treatment group,\nCompare with average change over time for the control group\n\nIdea: regress simultaneously \\[\\forall i \\in \\mathcal{G}\\_T, \\quad y_{i,2} - y_{i,1} = \\alpha + \\beta x_{i,t} +  \\epsilon_{i,t}\\] \\[\\forall i \\notin \\mathcal{G}\\_t, \\quad y_{i,2} - y_{i,1} = \\alpha + \\delta + \\beta x_{i,t} + \\epsilon_{i,t}\\]\nHypotheses:\n\nequal trend (\\(\\alpha\\) is the same in both groups)\nsame effect of all other variables (\\(\\beta\\) is the same)\n\\(\\epsilon_{i,t}\\) normally distributed, independent, etc….\n\\(x_{i,t}\\): (controls) other variables that might explain differences in learning speed\n\nmust be independent from \\(\\epsilon\\) and \\(T\\)\n\n\n\n\n\n\n\n\nUsing Panel data: \\[y_{i,t} = \\beta_{t} + \\delta T_{i} + \\beta x_{i,t} + \\epsilon_{i,t}\\] or using time dummies: \\[y_{i,t} = \\underbrace{\\beta_1 1_{t=1} + \\beta_2 1_{t=2}}\\_{\\text{time fixed effects}} + \\delta T_{i} + \\epsilon_{i,t}\\]\nBy construction the diff in diff is directly given by \\(\\delta\\)\n\nconvince yourself: the effect of time, is completely captured by the fixed effect\n\nThis specification allows for an average change over time that is the same for the two groups\n\n\n\n\n\n\nWe can do better:\n\nimagine we follow the population at many dates\nsome receive the treatment at \\(t=1\\), some at \\(t=2\\), etc…\nsuppose there is unobserved heterogeneity\n\nCode up the treatment: \\(T_{i,t}\\) if treatment was received before \\(t-1\\)\nIf we don’t take unobserved heterogeneity, we might get completely wrong results.\nSolution:\n\nuse time-fixed effect: to capture common trend\nuse individuals indicators: to capture individual heterogeneity\n\nYou might implement a specification like: \\[y_{i,t} = a_{i} + a_t + \\delta T_{i,t} + \\beta x_{i,t} + \\epsilon_{i,t}\\]\n\n\n\n\n\n\nWith linearmodels (example):\n\nindividual fixed effects: invest ~ 1 + value + capital + EntityEffects\ntime fixed effects: invest ~ 1 + value + capital + TimeEffects\nboth invest ~ 1 + value + capital + EntityEffects + TimeEffects\n\nRegress with PanelOLS\n\nmathematically equivalent to OLS but more efficient"
  },
  {
    "objectID": "session_never/index.html#time-and-space",
    "href": "session_never/index.html#time-and-space",
    "title": "Panels",
    "section": "",
    "text": "Until now, we have been rather loose about where the data comes from:\nTrying to explain \\(N\\) observations: \\(y_n = a + b x_n, n\\in [1,N]\\)\nAll these lonely observations, where do they all come from?\n\n\n\nIndividuals: cross section\n\n\n\nDates: time-series\n\n\n\n\n\n\n\n\n\nvery simple study: structural break\n\ndoes regression on \\([T_1, \\overline{T}]\\) yield (significantly) different results on \\([\\overline{T}, T_2]\\)\n\ngoing further: time series analysis\n\ndata is typically autocorrelated\nexample (AR1) \\(x_t = a + b x_{t-1} + \\epsilon_t\\)\n\n\n\n\n\n\n\n\n\n\n\nRepeated cross sectional studies\nIndex individual by \\(i\\in [1,J]\\) and time by \\(t\\in[1,T]\\) [graph repeated cross-sectional]\nwe can study evolution of the regression over time \\[\\text{date 1}: y_{i,1} = a_1 + b_1 x_{1,1}\\] … \\[\\text{date t}: y_{i,t} = a_i + b_i x_{i,1}\\] … \\[\\text{date T}: y_{i,T} = a_T + b_T x_{i,T}\\]\n\\(y_{i,t}\\) and \\(y_{i,t+1}\\) are unrelated\nOne big regression: pooled regression.\n\n\n\n\nEven better: longitudinal data\n\n[graph longitudinal data]\n\nThe same individuals are followed over time.\nWe can study the evolution of individuals while taking into account their heterogeneity\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\nBalanced: all individuals in the sample are followed from 1 to T\nUnbalanced: some indivuduals start later, stop earlier\n\n\n\n\n\n\nCrude solutions:\n\ntruncate the dates between \\([T_1, T_2]\\) so that dataset is balanced\neliminate individuals who are not present in the full sample\n\n\nNot very good:\n\ncan limit a lot the size of the sample\ncan induce a “selection bias”\n\n\nReal gurus know how to deal with missing values\n\nmany algorithms can be adapted\n\n\n\n\n\n\n\n\n\nwe tend to prefer here the long format (w.r.t id and date)\nthere can be many columns though (for each variable)\n\n\n\n\n\n\nmicro-panel: \\(T&lt;&lt;N\\)\n\nPanel Study of Income Dynamics (PSID): 5000 consumers since 1967 (US)\n\nreinterview same individuals from year to year\nbut some go in/out of the panel\n\nSurvey of Consumer and Finance (SCF)\n…\n\nmacro-panel: \\(T\\approx N\\)\n\nWIIW: 23 countries since the 60s (central, east and southern Europe)"
  },
  {
    "objectID": "session_never/index.html#taking-heterogeneity-into-account",
    "href": "session_never/index.html#taking-heterogeneity-into-account",
    "title": "Panels",
    "section": "",
    "text": "Suppose we want to explain the growth rate of several countries\n\nLet’s do a big regression (pooled regression) \\[y_{i,t} = a + b x_{i,t}\\] (pooled regression)\n\nWhat was missing?\n\nThere must be some effects, not captured by the regression, specific to each individual\n-&gt; Unobserved heterogeneity\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncapture idiosyncratic variability by adding an individual specific constant \\[ y_{i,t} = a + a_i +  b x_{i,t} + \\epsilon_{i,t}\\] \\(a_i\\) is called a fixed effect\n\nWe assume some structure on the data to discipline the regression.\n\nHow can we estimate the model? i.e. find plausible values for \\(a\\), \\(a_i\\), \\(b\\), \\(\\sigma(\\epsilon)\\), etc.\n\nWe focus on OLS estimator: \\[\\min_{a,a_i, b} \\sum_{i,t} ( y_{i,t} - \\underbrace{a + a_i +  b x_{i,t}}\\_{\\text{predicted value}} )^2\\]\n\n\n\n\n\n\n\n\nFixed effect regression: \\[ y_{i,t} = a + a_i +  b x_{i,t} + \\epsilon_{i,t}\\] is equivalent to \\[ y_{i,t} = a.1 + a_1 d\\_{i=1} + \\cdots + a_I d_{i=I} +  b x_{i,t} + \\epsilon_{i,t}\\] where \\(d\\) is a dummy variable such that \\(d_{i=j} = \\begin{cases}1, & \\text{if}\\ i=j \\\\\\\\ 0, & \\text{otherwise}\\end{cases}\\)\nMinor problem: \\(1, d_{i=1}, ... d_{i=I}\\) are not independent: \\(\\sum_{j=1}^I \\delta_{i=j}=1\\)\n\nSolution: ignore one of them, exactly like the dummies for categorical variables\n\nNow the regression can be estimated with OLS…\n\n\n\n\n\n\n… Now the regression can be estimated with OLS (or other)\n\nnaive approach fails for big panels (lots of dummy regressors makes \\(X^{\\prime}X\\) hard to invert)\nsmart approach decomposes computation in several steps\n\n“between” and “within” estimator (for advanced panel course)\n\nsoftware does it for us…\n\nLike always, we get estimates and significance numbers / confidence intervals\n\n\n\n\n\n\nSometimes, we know the whole dataset is affected by common time-varying shocks\n\nassume there isn’t a variable we can use to capture them (unobservable shocks)\n\nWe can use time-fixed effects to capture them: \\[ y_{i,t} = a + a_t +  b x_{i,t}\\]\nAnalysis is very similar to individual fixed effects\n\n\n\n\n\n\nWe can capture time heterogeneity and individual heterogeneity at the same time. \\[ y_{i,t} = a + a_i + a_t +  b x_{i,t}\\]\nMore of it soon.\n\n\n\n\n\n\nProblem with fixed effect model:\n\neach individual has a unique fixed effect\nit is impossible to predict it from other characteristics\n\n… and to compare an individual’s fixed effect to the predicted value\n\n\nSolution:\n\ninstead of assuming that specific effect is completely free, constrain it to follow a distribution: \\[y_{i,t} = \\alpha + \\beta x_{i,t} + \\epsilon_{i,t}\\] \\[\\epsilon_{i,t} = \\epsilon_i + \\epsilon_t + \\epsilon\\]\nwhere \\(\\epsilon_{i}\\), \\(\\epsilon_t\\) and \\(\\epsilon\\) are random variables with usual normality assumptions\n\n\n\n\n\n\n\nComposed coefficients: (coefficients can also be heterogenous in both dimension)\n\\[y_{i,t} = \\alpha_i + \\alpha_t + (\\beta_i + \\beta_t) x_{i,t} + \\epsilon_{i,t}\\]\nRandom coefficients …"
  },
  {
    "objectID": "session_never/index.html#diff-in-diff",
    "href": "session_never/index.html#diff-in-diff",
    "title": "Panels",
    "section": "",
    "text": "Setup:\n\nPopulation \\(i\\in[1,I]\\)\nWe follow the whole population at two successive dates \\(t_1\\) and \\(t_2\\).\nWe measure outcome \\(y\\)\nIndividuals in group \\(\\mathcal{G}_T\\) receive treatment \\(T\\)\nHow do we measure the effect of the treatment?\n\nExamples\n\ntutoring at school (afternoon lessons)\n\n\\(y\\): final grade\ndates: beginning (\\(t_1\\)), end of year (\\(t_2\\))\n\\(T_{i}\\) whether subject received afternoon sessions.\n\n\n\n\n\nGaphical summary:\n\n\n\n\n\n\n\nWhat we want:\n\nAverage change over time in the outcome variable for the treatment group,\nCompare with average change over time for the control group\n\nIdea: regress simultaneously \\[\\forall i \\in \\mathcal{G}\\_T, \\quad y_{i,2} - y_{i,1} = \\alpha + \\beta x_{i,t} +  \\epsilon_{i,t}\\] \\[\\forall i \\notin \\mathcal{G}\\_t, \\quad y_{i,2} - y_{i,1} = \\alpha + \\delta + \\beta x_{i,t} + \\epsilon_{i,t}\\]\nHypotheses:\n\nequal trend (\\(\\alpha\\) is the same in both groups)\nsame effect of all other variables (\\(\\beta\\) is the same)\n\\(\\epsilon_{i,t}\\) normally distributed, independent, etc….\n\\(x_{i,t}\\): (controls) other variables that might explain differences in learning speed\n\nmust be independent from \\(\\epsilon\\) and \\(T\\)\n\n\n\n\n\n\n\n\nUsing Panel data: \\[y_{i,t} = \\beta_{t} + \\delta T_{i} + \\beta x_{i,t} + \\epsilon_{i,t}\\] or using time dummies: \\[y_{i,t} = \\underbrace{\\beta_1 1_{t=1} + \\beta_2 1_{t=2}}\\_{\\text{time fixed effects}} + \\delta T_{i} + \\epsilon_{i,t}\\]\nBy construction the diff in diff is directly given by \\(\\delta\\)\n\nconvince yourself: the effect of time, is completely captured by the fixed effect\n\nThis specification allows for an average change over time that is the same for the two groups\n\n\n\n\n\n\nWe can do better:\n\nimagine we follow the population at many dates\nsome receive the treatment at \\(t=1\\), some at \\(t=2\\), etc…\nsuppose there is unobserved heterogeneity\n\nCode up the treatment: \\(T_{i,t}\\) if treatment was received before \\(t-1\\)\nIf we don’t take unobserved heterogeneity, we might get completely wrong results.\nSolution:\n\nuse time-fixed effect: to capture common trend\nuse individuals indicators: to capture individual heterogeneity\n\nYou might implement a specification like: \\[y_{i,t} = a_{i} + a_t + \\delta T_{i,t} + \\beta x_{i,t} + \\epsilon_{i,t}\\]\n\n\n\n\n\n\nWith linearmodels (example):\n\nindividual fixed effects: invest ~ 1 + value + capital + EntityEffects\ntime fixed effects: invest ~ 1 + value + capital + TimeEffects\nboth invest ~ 1 + value + capital + EntityEffects + TimeEffects\n\nRegress with PanelOLS\n\nmathematically equivalent to OLS but more efficient"
  },
  {
    "objectID": "session_never/Panel_data.html",
    "href": "session_never/Panel_data.html",
    "title": "Exercise on panel data",
    "section": "",
    "text": "Our goal here, is to redo some of the analysis from:\nF. Vella and M. Verbeek (1998), “Whose Wages Do Unions Raise? A Dynamic Model of Unionism and Wage Rate Determination for Young Men,” Journal of Applied Econometrics 13, 163-183.\nImport the dataset wage_panel from linearmodels.datasets. Describe it and make sure it is in a format suitable for panel data analysis. Show graphically the decrease in unionization over the period.\n\nfrom linearmodels.datasets import wage_panel\n\n\ndf = wage_panel.load()\n\n\ndf\n\n\n\n\n\n\n\n\nnr\nyear\nblack\nexper\nhisp\nhours\nmarried\neduc\nunion\nlwage\nexpersq\noccupation\n\n\n\n\n0\n13\n1980\n0\n1\n0\n2672\n0\n14\n0\n1.197540\n1\n9\n\n\n1\n13\n1981\n0\n2\n0\n2320\n0\n14\n1\n1.853060\n4\n9\n\n\n2\n13\n1982\n0\n3\n0\n2940\n0\n14\n0\n1.344462\n9\n9\n\n\n3\n13\n1983\n0\n4\n0\n2960\n0\n14\n0\n1.433213\n16\n9\n\n\n4\n13\n1984\n0\n5\n0\n3071\n0\n14\n0\n1.568125\n25\n5\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4355\n12548\n1983\n0\n8\n0\n2080\n1\n9\n0\n1.591879\n64\n5\n\n\n4356\n12548\n1984\n0\n9\n0\n2080\n1\n9\n1\n1.212543\n81\n5\n\n\n4357\n12548\n1985\n0\n10\n0\n2080\n1\n9\n0\n1.765962\n100\n5\n\n\n4358\n12548\n1986\n0\n11\n0\n2080\n1\n9\n1\n1.745894\n121\n5\n\n\n4359\n12548\n1987\n0\n12\n0\n3380\n1\n9\n1\n1.466543\n144\n5\n\n\n\n\n4360 rows × 12 columns\n\n\n\nColumn nr corresponds to the individual number. Column year corresponds to observation date. We need two set these two columns as a hierarchical index (so that first index corresponds to nr and the second one corresponds to year)\n\ndf.set_index([\"nr\", \"year\"], inplace=True)\ndf\n\n\n\n\n\n\n\n\n\nblack\nexper\nhisp\nhours\nmarried\neduc\nunion\nlwage\nexpersq\noccupation\n\n\nnr\nyear\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n13\n1980\n0\n1\n0\n2672\n0\n14\n0\n1.197540\n1\n9\n\n\n1981\n0\n2\n0\n2320\n0\n14\n1\n1.853060\n4\n9\n\n\n1982\n0\n3\n0\n2940\n0\n14\n0\n1.344462\n9\n9\n\n\n1983\n0\n4\n0\n2960\n0\n14\n0\n1.433213\n16\n9\n\n\n1984\n0\n5\n0\n3071\n0\n14\n0\n1.568125\n25\n5\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n12548\n1983\n0\n8\n0\n2080\n1\n9\n0\n1.591879\n64\n5\n\n\n1984\n0\n9\n0\n2080\n1\n9\n1\n1.212543\n81\n5\n\n\n1985\n0\n10\n0\n2080\n1\n9\n0\n1.765962\n100\n5\n\n\n1986\n0\n11\n0\n2080\n1\n9\n1\n1.745894\n121\n5\n\n\n1987\n0\n12\n0\n3380\n1\n9\n1\n1.466543\n144\n5\n\n\n\n\n4360 rows × 10 columns\n\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nblack\nexper\nhisp\nhours\nmarried\neduc\nunion\nlwage\nexpersq\noccupation\n\n\n\n\ncount\n4360.000000\n4360.000000\n4360.000000\n4360.000000\n4360.000000\n4360.000000\n4360.000000\n4360.000000\n4360.000000\n4360.000000\n\n\nmean\n0.115596\n6.514679\n0.155963\n2191.257339\n0.438991\n11.766972\n0.244037\n1.649147\n50.424771\n4.988532\n\n\nstd\n0.319777\n2.825873\n0.362862\n566.352301\n0.496321\n1.746181\n0.429564\n0.532609\n40.781991\n2.319978\n\n\nmin\n0.000000\n0.000000\n0.000000\n120.000000\n0.000000\n3.000000\n0.000000\n-3.579079\n0.000000\n1.000000\n\n\n25%\n0.000000\n4.000000\n0.000000\n2040.000000\n0.000000\n11.000000\n0.000000\n1.350717\n16.000000\n4.000000\n\n\n50%\n0.000000\n6.000000\n0.000000\n2080.000000\n0.000000\n12.000000\n0.000000\n1.671143\n36.000000\n5.000000\n\n\n75%\n0.000000\n9.000000\n0.000000\n2414.250000\n1.000000\n12.000000\n0.000000\n1.991086\n81.000000\n6.000000\n\n\nmax\n1.000000\n18.000000\n1.000000\n4992.000000\n1.000000\n16.000000\n1.000000\n4.051860\n324.000000\n9.000000\n\n\n\n\n\n\n\n\n# groupby syntax is very convenient here\ndf.groupby(\"year\")['union'].sum() # number of unionized employees\n\nyear\n1980    137\n1981    136\n1982    140\n1983    134\n1984    137\n1985    122\n1986    115\n1987    143\nName: union, dtype: int64\n\n\n\nN = df.groupby(\"year\")['union'].count()\n\n\nunion_rate =  df.groupby(\"year\")['union'].sum() / N\nunion_rate\n\nyear\n1980    0.251376\n1981    0.249541\n1982    0.256881\n1983    0.245872\n1984    0.251376\n1985    0.223853\n1986    0.211009\n1987    0.262385\nName: union, dtype: float64\n\n\nLet’s plot decline over time of unionization rate\n\nfrom matplotlib import pyplot as plt\nplt.plot(union_rate.index, union_rate)\n\n\n\n\nOur goal is to estimate the wage premium (if any) associated to belonging to a union.\nWe first consider a model \\(y_{i,t} = \\alpha + X_{i,t} \\beta + \\epsilon_{i,t}\\) where \\(y_{i,t}\\) is the wage and \\(X_{i,t}\\) contains a vector of regressors which can explain it.\nEstimate this model using OLS. What is the effect of union. What is the problem of this regression? Should we use individual or time fixed effects?\n\nimport linearmodels\n\n\nfrom linearmodels import PanelOLS\n\nmod = PanelOLS.from_formula(\n    \"lwage ~ black + hisp + exper + hours + married + educ + union + expersq + occupation\",\n    data=df\n)\nmod.fit()\n\n\nPanelOLS Estimation Summary\n\n\nDep. Variable:\nlwage\nR-squared:\n0.9250\n\n\nEstimator:\nPanelOLS\nR-squared (Between):\n0.9592\n\n\nNo. Observations:\n4360\nR-squared (Within):\n0.1763\n\n\nDate:\nTue, Mar 15 2022\nR-squared (Overall):\n0.9250\n\n\nTime:\n11:45:22\nLog-likelihood\n-2938.0\n\n\nCov. Estimator:\nUnadjusted\n\n\n\n\n\n\nF-statistic:\n5960.1\n\n\nEntities:\n545\nP-value\n0.0000\n\n\nAvg Obs:\n8.0000\nDistribution:\nF(9,4351)\n\n\nMin Obs:\n8.0000\n\n\n\n\nMax Obs:\n8.0000\nF-statistic (robust):\n5960.1\n\n\n\n\nP-value\n0.0000\n\n\nTime periods:\n8\nDistribution:\nF(9,4351)\n\n\nAvg Obs:\n545.00\n\n\n\n\nMin Obs:\n545.00\n\n\n\n\nMax Obs:\n545.00\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\nblack\n-0.1195\n0.0234\n-5.1075\n0.0000\n-0.1653\n-0.0736\n\n\nhisp\n0.0483\n0.0204\n2.3700\n0.0178\n0.0083\n0.0882\n\n\nexper\n0.1097\n0.0097\n11.305\n0.0000\n0.0906\n0.1287\n\n\nhours\n-5.894e-05\n1.28e-05\n-4.6029\n0.0000\n-8.404e-05\n-3.384e-05\n\n\nmarried\n0.1066\n0.0156\n6.8501\n0.0000\n0.0761\n0.1371\n\n\neduc\n0.1093\n0.0028\n38.939\n0.0000\n0.1038\n0.1149\n\n\nunion\n0.2017\n0.0172\n11.714\n0.0000\n0.1680\n0.2355\n\n\nexpersq\n-0.0038\n0.0007\n-5.4359\n0.0000\n-0.0051\n-0.0024\n\n\noccupation\n-0.0253\n0.0029\n-8.5924\n0.0000\n-0.0311\n-0.0196\n\n\n\nid: 0x7f6b2d76f040\n\n\nThe regression is significant: R2 is significantly different from zero (p-value&lt; 1e-5). All coefficients are significant at the 5% level, including the coefficient in front of lwage (log of wages).\nAccording to this coefficient, belonging to a union in a given year, includes salary by 20% approximately. (log(1+0.2)~18%)\nFormulate a model with individual fixed effect. Run a panel regression. Comment the results.\nIn this regression, non-observed heterogeneity between individuals, might explain difference in wage levels, and lead to systematic biases in the estimated coefficient. To control for it, we can add individual fixed effects.\n\nfrom linearmodels import PanelOLS\n\nmod = PanelOLS.from_formula(\n    \"lwage ~ black + hisp + exper + hours + married + educ + union + expersq + occupation + EntityEffects\",\n    data=df)\nprint(mod.fit())\n\nAbsorbingEffectError: \nThe model cannot be estimated. The included effects have fully absorbed\none or more of the variables. This occurs when one or more of the dependent\nvariable is perfectly explained using the effects included in the model.\n\nThe following variables or variable combinations have been fully absorbed\nor have become perfectly collinear after effects are removed:\n\n          black\n          hisp\n          married, educ\n\nSet drop_absorbed=True to automatically drop absorbed variables.\n\n\nWe observe that the fixed effects are “absorbing” some indicator variables (black, hisp, married, educ). For conistency, we have to remove them from the regression.\n\nmod = PanelOLS.from_formula(\"lwage ~ black + hisp + exper + hours + married + educ + union + expersq + occupation + EntityEffects\", data=df, drop_absorbed=True)\n\n\nmod.fit()\n\n/home/pablo/.local/opt/miniconda/lib/python3.8/site-packages/linearmodels/panel/model.py:1743: AbsorbingEffectWarning: \nVariables have been fully absorbed and have removed from the regression:\n\nblack, hisp, educ\n\n  warnings.warn(\n\n\n\nPanelOLS Estimation Summary\n\n\nDep. Variable:\nlwage\nR-squared:\n0.2005\n\n\nEstimator:\nPanelOLS\nR-squared (Between):\n0.3244\n\n\nNo. Observations:\n4360\nR-squared (Within):\n0.2005\n\n\nDate:\nTue, Mar 15 2022\nR-squared (Overall):\n0.3190\n\n\nTime:\n11:52:33\nLog-likelihood\n-1271.2\n\n\nCov. Estimator:\nUnadjusted\n\n\n\n\n\n\nF-statistic:\n159.19\n\n\nEntities:\n545\nP-value\n0.0000\n\n\nAvg Obs:\n8.0000\nDistribution:\nF(6,3809)\n\n\nMin Obs:\n8.0000\n\n\n\n\nMax Obs:\n8.0000\nF-statistic (robust):\n159.19\n\n\n\n\nP-value\n0.0000\n\n\nTime periods:\n8\nDistribution:\nF(6,3809)\n\n\nAvg Obs:\n545.00\n\n\n\n\nMin Obs:\n545.00\n\n\n\n\nMax Obs:\n545.00\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\nexper\n0.1349\n0.0086\n15.710\n0.0000\n0.1181\n0.1518\n\n\nhours\n-0.0001\n1.342e-05\n-10.273\n0.0000\n-0.0002\n-0.0001\n\n\nmarried\n0.0457\n0.0181\n2.5276\n0.0115\n0.0102\n0.0811\n\n\nunion\n0.0786\n0.0191\n4.1134\n0.0000\n0.0411\n0.1160\n\n\nexpersq\n-0.0052\n0.0006\n-8.5424\n0.0000\n-0.0064\n-0.0040\n\n\noccupation\n-0.0080\n0.0035\n-2.2798\n0.0227\n-0.0149\n-0.0011\n\n\n\nF-test for Poolability: 9.1024P-value: 0.0000Distribution: F(544,3809)Included effects: Entityid: 0x7f6b29688c70\n\n\nWe observe that the variables in this regression have a worse predictive power than the former one. This is not a surprise: we have replaced many observed characteristics by fixed effect that are fitted to every single individual. For predictions there is no way to guess the fixed effect, which is why the R2 is low. Note that the effect of belonging to a union, is reduced by half (8% increase in salary explained by unionization), which points to an important effect of unobserved heterogeneity.\nFormulate a model with individual and time fixed effect. Run a panel regression. Comment the results.\nIn the same spirit, the time trend, is not accounted for in the regression, which might explain some of the results. Let’s add time fixed effects.\n\nmod = PanelOLS.from_formula(\n    \"lwage ~ black + hisp + exper + hours + married + educ + union + expersq + occupation \\\n    + EntityEffects + TimeEffects\", data=df, drop_absorbed=True)\n\n\nmod.fit()\n\n/home/pablo/.local/opt/miniconda/lib/python3.8/site-packages/linearmodels/panel/model.py:1743: AbsorbingEffectWarning: \nVariables have been fully absorbed and have removed from the regression:\n\nblack, hisp, exper, educ\n\n  warnings.warn(\n\n\n\nPanelOLS Estimation Summary\n\n\nDep. Variable:\nlwage\nR-squared:\n0.0487\n\n\nEstimator:\nPanelOLS\nR-squared (Between):\n-0.8389\n\n\nNo. Observations:\n4360\nR-squared (Within):\n-0.6698\n\n\nDate:\nTue, Mar 15 2022\nR-squared (Overall):\n-0.8315\n\n\nTime:\n11:55:45\nLog-likelihood\n-1263.6\n\n\nCov. Estimator:\nUnadjusted\n\n\n\n\n\n\nF-statistic:\n38.919\n\n\nEntities:\n545\nP-value\n0.0000\n\n\nAvg Obs:\n8.0000\nDistribution:\nF(5,3803)\n\n\nMin Obs:\n8.0000\n\n\n\n\nMax Obs:\n8.0000\nF-statistic (robust):\n38.919\n\n\n\n\nP-value\n0.0000\n\n\nTime periods:\n8\nDistribution:\nF(5,3803)\n\n\nAvg Obs:\n545.00\n\n\n\n\nMin Obs:\n545.00\n\n\n\n\nMax Obs:\n545.00\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\nhours\n-0.0001\n1.342e-05\n-10.351\n0.0000\n-0.0002\n-0.0001\n\n\nmarried\n0.0470\n0.0181\n2.6047\n0.0092\n0.0116\n0.0825\n\n\nunion\n0.0762\n0.0191\n3.9822\n0.0001\n0.0387\n0.1136\n\n\nexpersq\n-0.0061\n0.0007\n-8.7238\n0.0000\n-0.0075\n-0.0048\n\n\noccupation\n-0.0078\n0.0035\n-2.2274\n0.0260\n-0.0147\n-0.0009\n\n\n\nF-test for Poolability: 9.4090P-value: 0.0000Distribution: F(551,3803)Included effects: Entity, Timeid: 0x7f6b29619790\n\n\nHere the addition of time fixed effects, reduces the explanatory power a bit further, but does not change much the estimate for the effect of unionization (still around 8%)."
  },
  {
    "objectID": "session_never/Panel_data.html#whose-wages-do-union-raise",
    "href": "session_never/Panel_data.html#whose-wages-do-union-raise",
    "title": "Exercise on panel data",
    "section": "",
    "text": "Our goal here, is to redo some of the analysis from:\nF. Vella and M. Verbeek (1998), “Whose Wages Do Unions Raise? A Dynamic Model of Unionism and Wage Rate Determination for Young Men,” Journal of Applied Econometrics 13, 163-183.\nImport the dataset wage_panel from linearmodels.datasets. Describe it and make sure it is in a format suitable for panel data analysis. Show graphically the decrease in unionization over the period.\n\nfrom linearmodels.datasets import wage_panel\n\n\ndf = wage_panel.load()\n\n\ndf\n\n\n\n\n\n\n\n\nnr\nyear\nblack\nexper\nhisp\nhours\nmarried\neduc\nunion\nlwage\nexpersq\noccupation\n\n\n\n\n0\n13\n1980\n0\n1\n0\n2672\n0\n14\n0\n1.197540\n1\n9\n\n\n1\n13\n1981\n0\n2\n0\n2320\n0\n14\n1\n1.853060\n4\n9\n\n\n2\n13\n1982\n0\n3\n0\n2940\n0\n14\n0\n1.344462\n9\n9\n\n\n3\n13\n1983\n0\n4\n0\n2960\n0\n14\n0\n1.433213\n16\n9\n\n\n4\n13\n1984\n0\n5\n0\n3071\n0\n14\n0\n1.568125\n25\n5\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4355\n12548\n1983\n0\n8\n0\n2080\n1\n9\n0\n1.591879\n64\n5\n\n\n4356\n12548\n1984\n0\n9\n0\n2080\n1\n9\n1\n1.212543\n81\n5\n\n\n4357\n12548\n1985\n0\n10\n0\n2080\n1\n9\n0\n1.765962\n100\n5\n\n\n4358\n12548\n1986\n0\n11\n0\n2080\n1\n9\n1\n1.745894\n121\n5\n\n\n4359\n12548\n1987\n0\n12\n0\n3380\n1\n9\n1\n1.466543\n144\n5\n\n\n\n\n4360 rows × 12 columns\n\n\n\nColumn nr corresponds to the individual number. Column year corresponds to observation date. We need two set these two columns as a hierarchical index (so that first index corresponds to nr and the second one corresponds to year)\n\ndf.set_index([\"nr\", \"year\"], inplace=True)\ndf\n\n\n\n\n\n\n\n\n\nblack\nexper\nhisp\nhours\nmarried\neduc\nunion\nlwage\nexpersq\noccupation\n\n\nnr\nyear\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n13\n1980\n0\n1\n0\n2672\n0\n14\n0\n1.197540\n1\n9\n\n\n1981\n0\n2\n0\n2320\n0\n14\n1\n1.853060\n4\n9\n\n\n1982\n0\n3\n0\n2940\n0\n14\n0\n1.344462\n9\n9\n\n\n1983\n0\n4\n0\n2960\n0\n14\n0\n1.433213\n16\n9\n\n\n1984\n0\n5\n0\n3071\n0\n14\n0\n1.568125\n25\n5\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n12548\n1983\n0\n8\n0\n2080\n1\n9\n0\n1.591879\n64\n5\n\n\n1984\n0\n9\n0\n2080\n1\n9\n1\n1.212543\n81\n5\n\n\n1985\n0\n10\n0\n2080\n1\n9\n0\n1.765962\n100\n5\n\n\n1986\n0\n11\n0\n2080\n1\n9\n1\n1.745894\n121\n5\n\n\n1987\n0\n12\n0\n3380\n1\n9\n1\n1.466543\n144\n5\n\n\n\n\n4360 rows × 10 columns\n\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nblack\nexper\nhisp\nhours\nmarried\neduc\nunion\nlwage\nexpersq\noccupation\n\n\n\n\ncount\n4360.000000\n4360.000000\n4360.000000\n4360.000000\n4360.000000\n4360.000000\n4360.000000\n4360.000000\n4360.000000\n4360.000000\n\n\nmean\n0.115596\n6.514679\n0.155963\n2191.257339\n0.438991\n11.766972\n0.244037\n1.649147\n50.424771\n4.988532\n\n\nstd\n0.319777\n2.825873\n0.362862\n566.352301\n0.496321\n1.746181\n0.429564\n0.532609\n40.781991\n2.319978\n\n\nmin\n0.000000\n0.000000\n0.000000\n120.000000\n0.000000\n3.000000\n0.000000\n-3.579079\n0.000000\n1.000000\n\n\n25%\n0.000000\n4.000000\n0.000000\n2040.000000\n0.000000\n11.000000\n0.000000\n1.350717\n16.000000\n4.000000\n\n\n50%\n0.000000\n6.000000\n0.000000\n2080.000000\n0.000000\n12.000000\n0.000000\n1.671143\n36.000000\n5.000000\n\n\n75%\n0.000000\n9.000000\n0.000000\n2414.250000\n1.000000\n12.000000\n0.000000\n1.991086\n81.000000\n6.000000\n\n\nmax\n1.000000\n18.000000\n1.000000\n4992.000000\n1.000000\n16.000000\n1.000000\n4.051860\n324.000000\n9.000000\n\n\n\n\n\n\n\n\n# groupby syntax is very convenient here\ndf.groupby(\"year\")['union'].sum() # number of unionized employees\n\nyear\n1980    137\n1981    136\n1982    140\n1983    134\n1984    137\n1985    122\n1986    115\n1987    143\nName: union, dtype: int64\n\n\n\nN = df.groupby(\"year\")['union'].count()\n\n\nunion_rate =  df.groupby(\"year\")['union'].sum() / N\nunion_rate\n\nyear\n1980    0.251376\n1981    0.249541\n1982    0.256881\n1983    0.245872\n1984    0.251376\n1985    0.223853\n1986    0.211009\n1987    0.262385\nName: union, dtype: float64\n\n\nLet’s plot decline over time of unionization rate\n\nfrom matplotlib import pyplot as plt\nplt.plot(union_rate.index, union_rate)\n\n\n\n\nOur goal is to estimate the wage premium (if any) associated to belonging to a union.\nWe first consider a model \\(y_{i,t} = \\alpha + X_{i,t} \\beta + \\epsilon_{i,t}\\) where \\(y_{i,t}\\) is the wage and \\(X_{i,t}\\) contains a vector of regressors which can explain it.\nEstimate this model using OLS. What is the effect of union. What is the problem of this regression? Should we use individual or time fixed effects?\n\nimport linearmodels\n\n\nfrom linearmodels import PanelOLS\n\nmod = PanelOLS.from_formula(\n    \"lwage ~ black + hisp + exper + hours + married + educ + union + expersq + occupation\",\n    data=df\n)\nmod.fit()\n\n\nPanelOLS Estimation Summary\n\n\nDep. Variable:\nlwage\nR-squared:\n0.9250\n\n\nEstimator:\nPanelOLS\nR-squared (Between):\n0.9592\n\n\nNo. Observations:\n4360\nR-squared (Within):\n0.1763\n\n\nDate:\nTue, Mar 15 2022\nR-squared (Overall):\n0.9250\n\n\nTime:\n11:45:22\nLog-likelihood\n-2938.0\n\n\nCov. Estimator:\nUnadjusted\n\n\n\n\n\n\nF-statistic:\n5960.1\n\n\nEntities:\n545\nP-value\n0.0000\n\n\nAvg Obs:\n8.0000\nDistribution:\nF(9,4351)\n\n\nMin Obs:\n8.0000\n\n\n\n\nMax Obs:\n8.0000\nF-statistic (robust):\n5960.1\n\n\n\n\nP-value\n0.0000\n\n\nTime periods:\n8\nDistribution:\nF(9,4351)\n\n\nAvg Obs:\n545.00\n\n\n\n\nMin Obs:\n545.00\n\n\n\n\nMax Obs:\n545.00\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\nblack\n-0.1195\n0.0234\n-5.1075\n0.0000\n-0.1653\n-0.0736\n\n\nhisp\n0.0483\n0.0204\n2.3700\n0.0178\n0.0083\n0.0882\n\n\nexper\n0.1097\n0.0097\n11.305\n0.0000\n0.0906\n0.1287\n\n\nhours\n-5.894e-05\n1.28e-05\n-4.6029\n0.0000\n-8.404e-05\n-3.384e-05\n\n\nmarried\n0.1066\n0.0156\n6.8501\n0.0000\n0.0761\n0.1371\n\n\neduc\n0.1093\n0.0028\n38.939\n0.0000\n0.1038\n0.1149\n\n\nunion\n0.2017\n0.0172\n11.714\n0.0000\n0.1680\n0.2355\n\n\nexpersq\n-0.0038\n0.0007\n-5.4359\n0.0000\n-0.0051\n-0.0024\n\n\noccupation\n-0.0253\n0.0029\n-8.5924\n0.0000\n-0.0311\n-0.0196\n\n\n\nid: 0x7f6b2d76f040\n\n\nThe regression is significant: R2 is significantly different from zero (p-value&lt; 1e-5). All coefficients are significant at the 5% level, including the coefficient in front of lwage (log of wages).\nAccording to this coefficient, belonging to a union in a given year, includes salary by 20% approximately. (log(1+0.2)~18%)\nFormulate a model with individual fixed effect. Run a panel regression. Comment the results.\nIn this regression, non-observed heterogeneity between individuals, might explain difference in wage levels, and lead to systematic biases in the estimated coefficient. To control for it, we can add individual fixed effects.\n\nfrom linearmodels import PanelOLS\n\nmod = PanelOLS.from_formula(\n    \"lwage ~ black + hisp + exper + hours + married + educ + union + expersq + occupation + EntityEffects\",\n    data=df)\nprint(mod.fit())\n\nAbsorbingEffectError: \nThe model cannot be estimated. The included effects have fully absorbed\none or more of the variables. This occurs when one or more of the dependent\nvariable is perfectly explained using the effects included in the model.\n\nThe following variables or variable combinations have been fully absorbed\nor have become perfectly collinear after effects are removed:\n\n          black\n          hisp\n          married, educ\n\nSet drop_absorbed=True to automatically drop absorbed variables.\n\n\nWe observe that the fixed effects are “absorbing” some indicator variables (black, hisp, married, educ). For conistency, we have to remove them from the regression.\n\nmod = PanelOLS.from_formula(\"lwage ~ black + hisp + exper + hours + married + educ + union + expersq + occupation + EntityEffects\", data=df, drop_absorbed=True)\n\n\nmod.fit()\n\n/home/pablo/.local/opt/miniconda/lib/python3.8/site-packages/linearmodels/panel/model.py:1743: AbsorbingEffectWarning: \nVariables have been fully absorbed and have removed from the regression:\n\nblack, hisp, educ\n\n  warnings.warn(\n\n\n\nPanelOLS Estimation Summary\n\n\nDep. Variable:\nlwage\nR-squared:\n0.2005\n\n\nEstimator:\nPanelOLS\nR-squared (Between):\n0.3244\n\n\nNo. Observations:\n4360\nR-squared (Within):\n0.2005\n\n\nDate:\nTue, Mar 15 2022\nR-squared (Overall):\n0.3190\n\n\nTime:\n11:52:33\nLog-likelihood\n-1271.2\n\n\nCov. Estimator:\nUnadjusted\n\n\n\n\n\n\nF-statistic:\n159.19\n\n\nEntities:\n545\nP-value\n0.0000\n\n\nAvg Obs:\n8.0000\nDistribution:\nF(6,3809)\n\n\nMin Obs:\n8.0000\n\n\n\n\nMax Obs:\n8.0000\nF-statistic (robust):\n159.19\n\n\n\n\nP-value\n0.0000\n\n\nTime periods:\n8\nDistribution:\nF(6,3809)\n\n\nAvg Obs:\n545.00\n\n\n\n\nMin Obs:\n545.00\n\n\n\n\nMax Obs:\n545.00\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\nexper\n0.1349\n0.0086\n15.710\n0.0000\n0.1181\n0.1518\n\n\nhours\n-0.0001\n1.342e-05\n-10.273\n0.0000\n-0.0002\n-0.0001\n\n\nmarried\n0.0457\n0.0181\n2.5276\n0.0115\n0.0102\n0.0811\n\n\nunion\n0.0786\n0.0191\n4.1134\n0.0000\n0.0411\n0.1160\n\n\nexpersq\n-0.0052\n0.0006\n-8.5424\n0.0000\n-0.0064\n-0.0040\n\n\noccupation\n-0.0080\n0.0035\n-2.2798\n0.0227\n-0.0149\n-0.0011\n\n\n\nF-test for Poolability: 9.1024P-value: 0.0000Distribution: F(544,3809)Included effects: Entityid: 0x7f6b29688c70\n\n\nWe observe that the variables in this regression have a worse predictive power than the former one. This is not a surprise: we have replaced many observed characteristics by fixed effect that are fitted to every single individual. For predictions there is no way to guess the fixed effect, which is why the R2 is low. Note that the effect of belonging to a union, is reduced by half (8% increase in salary explained by unionization), which points to an important effect of unobserved heterogeneity.\nFormulate a model with individual and time fixed effect. Run a panel regression. Comment the results.\nIn the same spirit, the time trend, is not accounted for in the regression, which might explain some of the results. Let’s add time fixed effects.\n\nmod = PanelOLS.from_formula(\n    \"lwage ~ black + hisp + exper + hours + married + educ + union + expersq + occupation \\\n    + EntityEffects + TimeEffects\", data=df, drop_absorbed=True)\n\n\nmod.fit()\n\n/home/pablo/.local/opt/miniconda/lib/python3.8/site-packages/linearmodels/panel/model.py:1743: AbsorbingEffectWarning: \nVariables have been fully absorbed and have removed from the regression:\n\nblack, hisp, exper, educ\n\n  warnings.warn(\n\n\n\nPanelOLS Estimation Summary\n\n\nDep. Variable:\nlwage\nR-squared:\n0.0487\n\n\nEstimator:\nPanelOLS\nR-squared (Between):\n-0.8389\n\n\nNo. Observations:\n4360\nR-squared (Within):\n-0.6698\n\n\nDate:\nTue, Mar 15 2022\nR-squared (Overall):\n-0.8315\n\n\nTime:\n11:55:45\nLog-likelihood\n-1263.6\n\n\nCov. Estimator:\nUnadjusted\n\n\n\n\n\n\nF-statistic:\n38.919\n\n\nEntities:\n545\nP-value\n0.0000\n\n\nAvg Obs:\n8.0000\nDistribution:\nF(5,3803)\n\n\nMin Obs:\n8.0000\n\n\n\n\nMax Obs:\n8.0000\nF-statistic (robust):\n38.919\n\n\n\n\nP-value\n0.0000\n\n\nTime periods:\n8\nDistribution:\nF(5,3803)\n\n\nAvg Obs:\n545.00\n\n\n\n\nMin Obs:\n545.00\n\n\n\n\nMax Obs:\n545.00\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\nhours\n-0.0001\n1.342e-05\n-10.351\n0.0000\n-0.0002\n-0.0001\n\n\nmarried\n0.0470\n0.0181\n2.6047\n0.0092\n0.0116\n0.0825\n\n\nunion\n0.0762\n0.0191\n3.9822\n0.0001\n0.0387\n0.1136\n\n\nexpersq\n-0.0061\n0.0007\n-8.7238\n0.0000\n-0.0075\n-0.0048\n\n\noccupation\n-0.0078\n0.0035\n-2.2274\n0.0260\n-0.0147\n-0.0009\n\n\n\nF-test for Poolability: 9.4090P-value: 0.0000Distribution: F(551,3803)Included effects: Entity, Timeid: 0x7f6b29619790\n\n\nHere the addition of time fixed effects, reduces the explanatory power a bit further, but does not change much the estimate for the effect of unionization (still around 8%)."
  },
  {
    "objectID": "Untitled.html",
    "href": "Untitled.html",
    "title": "Data-Based Economics",
    "section": "",
    "text": "from nltk.stem.wordnet import WordNetLemmatizer\nlem = WordNetLemmatizer()\nfrom nltk.stem import PorterStemmer\nps = PorterStemmer()\n\nwords =  [\"flying\", \"flyers\", \"fly\"]\n\nstemmed_words=[ps.stem(w) for w in words]\nlemmatized_words=[lem.lemmatize(w) for w in words]\n\n\nstemmed_words\n\n['fli', 'flyer', 'fli']\n\n\n\nlemmatized_words\n\n['flying', 'flyer', 'fly']"
  }
]